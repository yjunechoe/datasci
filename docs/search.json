[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Data Science for Studying Language & the Mind! The Fall 2023 course information and syllabus are below. Course materials for previous semesters are archived here."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course description",
    "text": "Course description\nData Sci for Lang & Mind is an entry-level course designed to teach basic principles of statistics and data science to students with little or no background in statistics or computer science. Students will learn to identify patterns in data using visualizations and descriptive statistics; make predictions from data using machine learning and optimization; and quantify the certainty of their predictions using statistical models. This course aims to help students build a foundation of critical thinking and computational skills that will allow them to work with data in all fields related to the study of the mind (e.g. linguistics, psychology, philosophy, cognitive science, neuroscience).\nThere are no prerequisites beyond high school algebra. No prior programming or statistics experience is necessary, though you will still enjoy this course if you already have a little. Students who have taken several computer science or statistics classes should look for a more advanced course."
  },
  {
    "objectID": "syllabus.html#people",
    "href": "syllabus.html#people",
    "title": "Syllabus",
    "section": "People",
    "text": "People\n\nInstructor: Dr. Katie Schuler\nTAs: June Choe, Avinash Goss, Ravi Arya"
  },
  {
    "objectID": "syllabus.html#lectures",
    "href": "syllabus.html#lectures",
    "title": "Syllabus",
    "section": "Lectures",
    "text": "Lectures\nTuesdays and Thursdays at 10:15am in COHN 402."
  },
  {
    "objectID": "syllabus.html#labs",
    "href": "syllabus.html#labs",
    "title": "Syllabus",
    "section": "Labs",
    "text": "Labs\nHands-on practice, quiz prep, and problem set work guided by TAs.\n\n402: Thu at 1:45p in WILL 4 with TA\n403: Thu at 3:30p in TOWN 305 with TA\n404: Fri at 10:15a in WILL 24 with Ravi\n405: Fri at 12:00p in TOWN 307 with Avinash"
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office hours",
    "text": "Office hours\nThe linguistics department is on the 3rd floor of the C-wing at 3401 Walnut street, between Franklin’s Table and Modern Eye.\n\nKatie: Tue 12:30-1:30p in 314 ling dept\nJune: TBD in ling dept\nAvinash: TBD\nRavi: TBD"
  },
  {
    "objectID": "syllabus.html#quizzes",
    "href": "syllabus.html#quizzes",
    "title": "Syllabus",
    "section": "Quizzes",
    "text": "Quizzes\nThere are 4 quizzes, taken in class on Tuesdays. Missed quizzes cannot be made up except in cases of genuine conflict or emergency (documentation and a Course Action Notice are required). Instead, you will be invited to submit a missed quiz for half credit (50%)."
  },
  {
    "objectID": "syllabus.html#problem-sets",
    "href": "syllabus.html#problem-sets",
    "title": "Syllabus",
    "section": "Problem sets",
    "text": "Problem sets\nThere are 6 problem sets, due on Sundays to Gradescope by 11:59pm. Students may request an extension of up to 3 days. Extensions beyond this are not permitted, because delaying the release of solutions would negatively impact other students. After solutions are posted, late problem sets can be submitted for half credit (50%)."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\n60% problem sets (equally weighted, lowest dropped)\n40% quizzes (equally weighted)\nLetter grade minimums: 97% A+, 93% A, 90% A-, 87% B+, 84% B, 80% B-, 77% C+, 74% C, 70% C-, 67% D+, 64% D, 61% D-, else F\nAll problems will be graded according to this rubric."
  },
  {
    "objectID": "syllabus.html#collaborations",
    "href": "syllabus.html#collaborations",
    "title": "Syllabus",
    "section": "Collaborations",
    "text": "Collaborations\nCollaboration on problem sets is highly encouraged! If you collaborate, you need to write your own code/solutions, name your collaborators, and cite any outside sources you consulted (you don’t need to cite the course material)."
  },
  {
    "objectID": "syllabus.html#accomodations",
    "href": "syllabus.html#accomodations",
    "title": "Syllabus",
    "section": "Accomodations",
    "text": "Accomodations\nWe will support any accommodations arranged through Disability Services via the Weingarten Center."
  },
  {
    "objectID": "syllabus.html#extra-credit",
    "href": "syllabus.html#extra-credit",
    "title": "Syllabus",
    "section": "Extra credit",
    "text": "Extra credit\nThere is no extra credit in the course. However, students can submit any missed problem set or quiz by the end of the semester for half credit (50%). To ensure fair treatment across all students, all students will receive a 1% “bonus” to their final course grade: 92.54% will become 93.54%."
  },
  {
    "objectID": "syllabus.html#regrade-requests",
    "href": "syllabus.html#regrade-requests",
    "title": "Syllabus",
    "section": "Regrade requests",
    "text": "Regrade requests\nRegrade requests should be submitted through Gradescope within one week of receiving your graded assignment. Please explain why you believe there was a grading mistake, given the posted solutions and rubric."
  },
  {
    "objectID": "syllabus.html#resources",
    "href": "syllabus.html#resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nIn addition to the course website, we will use the following:\n\ngoogle colab (r kernel) - for computing\ncanvas - for posting grades\ngradescope - for submitting problem sets\ned discussion - for announcements and questions\n\nPlease consider using these Penn resources this semester:\n\nWeingarten Center for academic support and tutoring.\nWellness at Penn for health and wellbeing."
  },
  {
    "objectID": "rubric.html",
    "href": "rubric.html",
    "title": "Rubric",
    "section": "",
    "text": "All quiz and problem set questions are graded with the following 4-point rubric. By “understand” we mean you understand the lecture concepts and can apply the technical skills you learned in the course."
  },
  {
    "objectID": "rubric.html#overall-grade",
    "href": "rubric.html#overall-grade",
    "title": "Rubric",
    "section": "Overall grade",
    "text": "Overall grade\nTo compute your overall grade on a problem set or quiz, we take your average score on all questions, add a constant of 6, and divide by 10. For example, given a problem set with 3 questions:\n\nscores of 4-3-3 would result in a 93.3% (A): (4+3+3)/3 + 6 = 9.33/10\nscores of 3-2-2 would result in a 83.3% (B-): (3+2+2)/3 + 6 = 8.33/10\nscores of 1-1-1 would result in a 70% (C-): (1+1+1)/3 + 6 = 7/10\n\nWe chose this rubric because all reasonable attempts receive a passing grade, but A+ is reserved for students with advanced understanding of the material."
  },
  {
    "objectID": "rubric.html#missed-problem-sets-and-quizzes",
    "href": "rubric.html#missed-problem-sets-and-quizzes",
    "title": "Rubric",
    "section": "Missed problem sets and quizzes",
    "text": "Missed problem sets and quizzes\nMissed problem sets or quizzes will receive an overall score of zero. However, students can submit any missed problem set or quiz by the end of the semester for half credit (50%)."
  },
  {
    "objectID": "psets/problem-set-1.html",
    "href": "psets/problem-set-1.html",
    "title": "Problem set 1",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/r-basics.html",
    "href": "notes/r-basics.html",
    "title": "R basics",
    "section": "",
    "text": "Defining some basic concepts:\n\nExpressions are combination of values, variables, operators, and functions that can be evaluated to produce a result. Expressions can be as simple as a single value or more complex involving calculations, comparisons, and function calls. They are the fundamental building blocks of programming.\n\n10 - a simple value expression that evaluates to 10.\nx + 10 - an expression that adds the value of x to 10\na &lt;- x + 10 - an expression that adds the value of x to 10 and assigns the result to the variable a\n\nObjects in R allow us to store various types of data, such as numbers, text, vectors, matrices; and more complex structures like functions and data frames. Objects are created by assigning values to variable names with the assignment operator, &lt;-. For example, in x &lt;- 10, x is an object assigned to the value 10.\nNames that we assign to objects must include only letters, numbers, ., or _. Names must start with a letter (or . if not followed by a number).\nAttributes allow you to attach arbirary metadata to an object. For example, adding a dim (dimension) attribute to a vector allows it to behave like a matrix or n dimensional array.\nFunctions (or commands) are reusable pieces of code that take some input, preform some task or computation, and return an output. Many functions are built-in to base R (see below!), others can be part of packages or even defined by you. Functions are objects!\nEnvironment is the collection of all the objects (functions, variables etc.) we defined in the current R session.\nPackages are collections of functions, data, and documentation bundled together in R. They enhance R’s capabilities by introducing new functions and specialized data structures. Packages need to be installed and loaded before you can use their functions or data.\nComments are notes you leave to yourself (within code blocks in colab) to document your code; comments are not evaluated.\nMessages are notes R leaves for you, after you run your code. Messages can be simply for-your-information, warnings that something unexpected might happen, or erros if R cannot evaluate your code.\n\nWays to get help when coding in R:\n\nRead packages docs - packages usually come with extensive documentation and examples. Reading the docs is one of the best ways to figure things out. Here is an example from the dplyr package.\nRead error messages - read any error messages you receive while coding — they give clues about what is going wrong!\nAsk R - Use R’s built-in functions to get help as you code\nAsk on Ed- ask questions on our class discussion board!\nAsk Google/Stack Overflow - It is a normal and important skill (not cheating) to google things while coding and learning to code! Use keywords and package names to ensure your solutions are course-relevant.\nAsk ChatGPT - You can similarly use ChatGPT or other LLMs as a resource. But keep in mind they may provide a solution that is wrong or not relevant to what we are learning in this course."
  },
  {
    "objectID": "notes/r-basics.html#basics",
    "href": "notes/r-basics.html#basics",
    "title": "R basics",
    "section": "",
    "text": "Defining some basic concepts:\n\nExpressions are combination of values, variables, operators, and functions that can be evaluated to produce a result. Expressions can be as simple as a single value or more complex involving calculations, comparisons, and function calls. They are the fundamental building blocks of programming.\n\n10 - a simple value expression that evaluates to 10.\nx + 10 - an expression that adds the value of x to 10\na &lt;- x + 10 - an expression that adds the value of x to 10 and assigns the result to the variable a\n\nObjects in R allow us to store various types of data, such as numbers, text, vectors, matrices; and more complex structures like functions and data frames. Objects are created by assigning values to variable names with the assignment operator, &lt;-. For example, in x &lt;- 10, x is an object assigned to the value 10.\nNames that we assign to objects must include only letters, numbers, ., or _. Names must start with a letter (or . if not followed by a number).\nAttributes allow you to attach arbirary metadata to an object. For example, adding a dim (dimension) attribute to a vector allows it to behave like a matrix or n dimensional array.\nFunctions (or commands) are reusable pieces of code that take some input, preform some task or computation, and return an output. Many functions are built-in to base R (see below!), others can be part of packages or even defined by you. Functions are objects!\nEnvironment is the collection of all the objects (functions, variables etc.) we defined in the current R session.\nPackages are collections of functions, data, and documentation bundled together in R. They enhance R’s capabilities by introducing new functions and specialized data structures. Packages need to be installed and loaded before you can use their functions or data.\nComments are notes you leave to yourself (within code blocks in colab) to document your code; comments are not evaluated.\nMessages are notes R leaves for you, after you run your code. Messages can be simply for-your-information, warnings that something unexpected might happen, or erros if R cannot evaluate your code.\n\nWays to get help when coding in R:\n\nRead packages docs - packages usually come with extensive documentation and examples. Reading the docs is one of the best ways to figure things out. Here is an example from the dplyr package.\nRead error messages - read any error messages you receive while coding — they give clues about what is going wrong!\nAsk R - Use R’s built-in functions to get help as you code\nAsk on Ed- ask questions on our class discussion board!\nAsk Google/Stack Overflow - It is a normal and important skill (not cheating) to google things while coding and learning to code! Use keywords and package names to ensure your solutions are course-relevant.\nAsk ChatGPT - You can similarly use ChatGPT or other LLMs as a resource. But keep in mind they may provide a solution that is wrong or not relevant to what we are learning in this course."
  },
  {
    "objectID": "notes/r-basics.html#important-functions",
    "href": "notes/r-basics.html#important-functions",
    "title": "R basics",
    "section": "2 Important functions",
    "text": "2 Important functions\nFor objects:\n\nstr(x) - returns summary of object’s structure\ntypeof(x) - returns object’s data type\nlength(x) - returns object’s length\nattributes(x) - returns list of object’s attributes\nis.*(x) - test if object is data type (e.g. is.double(x))\nas.*(x) - coerce object to data type (e.g. as.double(x))\n\nfor environment:\n\nls() - list all variables in environment\nrm(x) - remove x variable from environment\nrm(list = ls()) - remove all variables from environment\n\nFor packages:\n\ninstall.packages() to install packages\nlibrary() to load the package into your current R session.\ndata() to load data from package into environment\nsessionInfo() - version information for current R session and packages\n\nFor help:\n\n?mean - get help with a function\nhelp.search('mean') - search help files for word or phrase\nhelp(package='tidyverse') - find help for a package"
  },
  {
    "objectID": "notes/r-basics.html#vectors",
    "href": "notes/r-basics.html#vectors",
    "title": "R basics",
    "section": "3 Vectors",
    "text": "3 Vectors\nOne of the must fundamental data structures in R is the vector. There are two types:\n\natomic vector - elements of the same data type\nlist - elements refer to any object (even complex objects or other lists)\n\nAtomic vectors can be one of six data types:\n\ndouble - real numbers, written in decimal (0.1234) or scientific notation (1.23e4)\n\nnumbers are double by default (3 is stored as 3.00)\nthree special doubles: Inf, -Inf, and NaN (not a number)\n\ninteger - integers, numbers followed by L (3L or 1e3L)\ncharacter - strings with single or double quotes (‘hello world!’ or “hello world!”)\nlogical - boolean written (TRUE or FALSE) or abbreviated (T or F)\ncomplex - complex numbers where i is imaginary (5 + 3i)\nraw - stores raw bytes\n\nSome more complex data structures are built from atomic vectors by adding attributes:\n\nmatrix - a vector with a dim attribute representing 2 dimensions\narray - a vector with a dim attribute representing n dimensions\nfactor - an integer vector with two attributes: class=\"factor\" and levels, which defines the set of allowed values (useful for categorical data)\ndate-time - a double vector where the value is the number of seconds since Jan 01, 1970 and a tzone attribute representing the time zone\ndata.frame - a named list of vectors (of equal length) with attributes for names (column names), row.names, and class=\"data.frame\" (used to represent datasets)\n\nTo create atomic vectors:\n\nc(2,4,6) - c for combine, returns 2 4 6\n2:4 - vector of integers, returns 2 3 4\nseq(2:6, by=2) - sequence by, returns 2 4 6\n\nTo create more complex structures:\n\nlist(x=c(1,2,3), y=c('a','b')) - create a list\nmatrix(x, nrow=2, ncol=2) - create a matrix from x with nrow and ncol\narray(x, dim=c(2,3,2)) - create an array from x with dimensions\nfactor(x, levels=unique(x)) - turn a vector into a factor\ndata.frame(x=c(1,2,3), y=c('a','b','c')) - create a data frame\n\nMissing elements and empty vectors:\n\nNA- used to represent missing or unknown elements in vectors. Note that NA is contageous: expressions including NA usually return NA\nNULL - used to represent an empty or absent vector of arbitrary type. NULL is its own special type and always has length zero and NULL attributes."
  },
  {
    "objectID": "notes/r-basics.html#subsetting",
    "href": "notes/r-basics.html#subsetting",
    "title": "R basics",
    "section": "4 Subsetting",
    "text": "4 Subsetting\n\nSubsetting is a natural complement to str(). While str() shows you all the pieces of any object (its structure), subsetting allows you to pull out the pieces that you’re interested in. ~ Hadley Wickham, Advanced R\n\nThere are three operators for subsetting objects:\n\n[ - selects multiple elements\n[[ and $ - extracts a single element\n\nThere are six ways to select multiple elements from vectors with [:\n\nx[c(1,2)] - positive integers select elements at specified indexes\nx[-c(1,2)] - negative integers select all but elements at specified indexes\nx[c(\"name\", \"name2\")] select elements by name, if elements are named\nx[] - nothing returns the original object\nx[0] - zero returns a zero-length vector\nx[c(TRUE, TRUE)] - select elements where corresponding logical value is TRUE\n\nThese also apply when selecting multiple elements from higher dimensional objects (matrix, array, data frame), but note that:\n\nindexes for different dimensions are separated by commas [rows, columns, ...]\nomitted dimensions return all values along that dimension\nthe result is simplified to the lowest possible dimensions by default\nthey can also be indexed like a vector (selects columns)\n\nThere are 3 ways to extract a single element from any data structure:\n\n[[2]] - a single positive integer (index)\n[['name']] - a single string\nx$name - the $ operator is a useful shorthand for [['name']]\n\nWhen extracting single elements, note that:\n\n[[ is preferred for atomic vectors for clarity (though[ also works)\n$ does partial matching without warning; use options(warnPartialMatchDollar=TRUE)\nthe behavior for invalid indexes is inconsistent: sometimes you’ll get an error message, and sometimes it will return NULL"
  },
  {
    "objectID": "notes/r-basics.html#operations",
    "href": "notes/r-basics.html#operations",
    "title": "R basics",
    "section": "5 Operations",
    "text": "5 Operations\nArithmetic operators:\n\n+ - add\n- - subtract\n* - multiply\n/ - divide\n^ - exponent\n\nComparison operators return true or false:\n\na == b - equal to\na != b - not equal to\na &gt; b - greater than\na &lt; b - less than\na &gt;= b - greater than or equal to\na &lt;= b - less than or equal to\n\nLogical operators combine multiple true or false statements:\n\n& - and\n| - or\n! - not\nany() - returns true if any element meets condition\nall() - returns true if all elements meet condition\n%in% - returns true if any element is in the following vector\n\nMost math operations (and many functions) are vectorized in R:\n\nthey can work on entire vectors, without the need for explicit loops or iteration.\nthis a powerful feature that allows you to write cleaner, more efficient code\nTo illustrate, suppose x &lt;- c(1, 2, 3):\n\nx + 100 returns [101 102 103]\nx == 1 returns [TRUE FALSE FALSE]"
  },
  {
    "objectID": "notes/r-basics.html#built-in-functions",
    "href": "notes/r-basics.html#built-in-functions",
    "title": "R basics",
    "section": "6 Built-in functions",
    "text": "6 Built-in functions\nNote that you do not need to memorize these built-in functions to be successful on quizzes. Use this as a reference.\nFor basic math:\n\nlog(x) - natural log\nexp(x) - exponential\nsqrt(x) - square root\nabs(x) - absolute value\nmax(x) - largest element\nmin(x) - smallest element\nround(x, n) - round to n decimal places\nsignif(x, n) - round to n significant figures\nsum(x) - add all elements\n\nFor stats:\n\nmean(x) - mean\nmedian(x) - median\nsd(x) - standard deviation\nvar(x) - variance\nquantile(x) - percentage quantiles\nrank(x) - rank of elements\ncor(x, y) - correlation\nlm(x ~ y, data=df) - fit a linear model\nglm(x ~ y, data=df) - fit a generalized linear model\nsummary(x) - get more detailed information from a fitted model\naov(x) - analysis of variance\n\nFor vectors:\n\nsort(x) - return sorted vector\ntable(x) - see counts of values in a vector\nrev(x) - return reversed vector\nunique(x) - return unique values in a vector\ndim(x) - transform vector into n-dimensional array\n\nFor matrices:\n\nt(m) - transpose matrix\nm %+% n - matrix multiplication\nsolve(m, n) - find x in m * x = n\n\nFor data frames:\n\nview(df) - see the full data frame\nhead(df) - see the first 6 rows of data frame\nnrow(df) - number of rows in a data frame\nncol(df) - number of columns in a data frame\ndim(df) - number of columns and rows in a data frame\ncbind(df1, df2) - bind columns\nrbind(df1, df2) - bind rows\n\nFor strings:\n\npaste(x, y, sep=' ') - join multiple vectors together\ntoupper(x) - convert to uppercase\ntolower(x) - convert to lowercase\nnchar(x) - number of characters in a string\n\nFor simple plotting:\n\nplot(x) values of x in order\nplot(x, y) - values of x against y\nhist(x) - histogram of x"
  },
  {
    "objectID": "notes/r-basics.html#programming-in-r",
    "href": "notes/r-basics.html#programming-in-r",
    "title": "R basics",
    "section": "7 Programming in R",
    "text": "7 Programming in R\nWriting functions and handling flow control are important aspects of learning to program in any language. For our purposes, some general conceptual knowledge on these topics is sufficient (see below). Those interested to learn more might enjoy the book Hands-On Programming with R.\n\nFunctions are reusable pieces of code that take some input, perform some task or computation, and return an output.\nfunction(inputs){\n    # do something\n    return(output)\n}\nFlow control refers to managing the order in which expressions are executed in a program:\n\nif…else - if something is true, do this; otherwise do that\nfor loops - repeat code a specific number of times\nwhile loops - repeat code as long as certain conditions are true\nbreak - exit a loop early\nnext - skip to next iteration in a loop"
  },
  {
    "objectID": "notes/r-basics.html#further-reading-and-references",
    "href": "notes/r-basics.html#further-reading-and-references",
    "title": "R basics",
    "section": "8 Further reading and references",
    "text": "8 Further reading and references\nSuggested further reading:\n\nBase R Cheat Sheet\nGetting Started with Data in R in ModernDive textbook\nR Nuts and Bolts in R Programming for Data Science by Roger Peng\n\nOther references:\n\nMatlab vs. Julia vs. Python from blog post by Toby Driscoll\nVectors in Advanced R by Hadley Wickham\nSubsetting in Advanced R by Hadley Wickham\nA field guide to base R in R for Data Science by Hadley Wickham"
  },
  {
    "objectID": "notes/probability-distributions.html",
    "href": "notes/probability-distributions.html",
    "title": "Probablity distributions",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/probability-distributions.html#exploring-a-simple-dataset",
    "href": "notes/probability-distributions.html#exploring-a-simple-dataset",
    "title": "Probablity distributions",
    "section": "1 Exploring a simple dataset",
    "text": "1 Exploring a simple dataset\n\n\nSetup code\n# ------ setup for today's lecture notes ----- # \n\n# suppress startup messages at package load\nsuppressPackageStartupMessages(library(tidyverse))\n\n\nWarning: package 'ggplot2' was built under R version 4.3.1\n\n\nSetup code\n# set the theme for the plots \ntheme_set(theme_classic(base_size=15))\n\n# generate 10000 y values with mean 3 and sd 0.25\ndata &lt;- tibble(\n    y=rnorm(1000, mean=3, sd=0.25)\n) \n\n\nWe begin with the simplest possible dataset: suppose we measure a single quantity y. What can we do with these data?\nWe can create a visual summary of our dataset with a histogram. A histogram plots the distribution of a set of data, which allows us to get a quick visual of the data: formally we have plotted the the frequency distribution (count) of the data, but this also gives a sense of the central tendency and variability in our dataset.\n\n\nCode\nggplot(data=data, aes(x=y)) +\n    geom_histogram(\n        binwidth = 0.25,\n        color=\"black\", fill='lightgray', alpha=0.5\n    )"
  },
  {
    "objectID": "notes/probability-distributions.html#descriptive-statistics",
    "href": "notes/probability-distributions.html#descriptive-statistics",
    "title": "Probablity distributions",
    "section": "2 Descriptive statistics",
    "text": "2 Descriptive statistics\nWe can summarize (or describe) a set of data with descriptive statistics. There are three types of measures:\n\ncentral tendency describes a central or typical value (mean, median, mode)\nvariability describes dispersion or spread of values (variance, standard deviation, IQR)\nfrequency distribution describes how frequently different values occur (count)\n\nR has built-in functions to handle descriptive statistics (we saw these in lecture 1):\n\ndata %&gt;%\n    summarise(\n        n = n(), \n        mean = mean(y),\n        median = median(y),\n        sd = sd(y),\n        iqr_lower = quantile(y, 0.25),\n        iqr_upper = quantile(y, 0.75)\n    ) \n\n\n\n\n\nn\nmean\nmedian\nsd\niqr_lower\niqr_upper\n\n\n\n\n1000\n2.995265\n2.996178\n0.2478109\n2.820712\n3.164067"
  },
  {
    "objectID": "notes/probability-distributions.html#parametric-vs.-nonparametric",
    "href": "notes/probability-distributions.html#parametric-vs.-nonparametric",
    "title": "Probablity distributions",
    "section": "3 Parametric vs. nonparametric",
    "text": "3 Parametric vs. nonparametric\nSome statistics are considered paramteric because they make assumptions about the the distribution of the data (can therefore be computed theoretically from parameters):\n\nThe mean and standard deviation assume the distribution is Gaussian and can therefore be computed via the following equations\nmean \\(\\mu\\)\nsd \\(\\sigma\\)\n\nOther statstics are nonparametric because they make minimal assumptions about the distribution of the data:\n\nmedian is the 50th percentile, the value below which 50% of the data points fall.\ninter-quartile range (IQR) is the difference between the 25th and 75th percentiles (sometimes called the 50% coverage interval because 50% of the data fall in this range).\n\nNote that we can calculate any arbitrary coverage interval. The 95% coverage interval — widely used in the sciences — is the difference between the 2.5 percentile and the 97.5 percentile, including all but 5% of the data."
  },
  {
    "objectID": "notes/probability-distributions.html#probability-distributions",
    "href": "notes/probability-distributions.html#probability-distributions",
    "title": "Probablity distributions",
    "section": "4 Probability distributions",
    "text": "4 Probability distributions\nA probability distribution (aka probability density function) is a mathematical function that describes the probability of observing the different possible values of a variable (or variables). We will focus on univariate distributions in this class — probability distributions of just one random variable — but probability distributions can also be multivariate.\n\nOne of the simplest probability distributions is the uniform distribution, where all possible values of a variable are equally likely. The probability density function for the uniform distribution is given by the following equation with two parameters (the boundaries, min and max):\n\n\\(p(x) = \\frac{1}{max-min}\\)\n\nOne of the most useful probability distributions for our purposes is the Gaussian (or Normal) distribution. The probability density function for the Gaussian distribution is given by the following equation, with the parameters \\(\\mu\\) (mean) and \\(\\sigma\\) (standard deviation):\n\n\\(p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right)\\)\nThe Gaussian distribution assumes that the distribution of a set of data takes a certain form (is unimodal, symmetric, etc).\nWhen values are sampled from a Gaussian distribution, 68% of the values will be within one standard deviation from the mean and 95% within two standard deviations from the mean.\nWhen computing the mean and standard deviation of a set of data, we are fitting a Gaussian distribution to the data."
  },
  {
    "objectID": "notes/probability-distributions.html#probability-distributions-with-r",
    "href": "notes/probability-distributions.html#probability-distributions-with-r",
    "title": "Probablity distributions",
    "section": "5 Probability distributions with R",
    "text": "5 Probability distributions with R\nThe probability distributions we’ve discussed so far are considered “parametric” because they are given by one or more parameters. When we use R’s functions to generate values from these distributions, we provide these parameters as arguments. Base R has four functions we will use to generate values associated with a probability distribution. - dnorm(mean=5, sd=1) returns the height of the probability density function at the given values - pnorm(5, mean=5, sd=1) returns the cumulative density function (the probability that a random number from the distribution will be less than the given values) - qnorm(0.8, mean=5, sd=1) returns the value whose cumulative distribution matches the probability (inverse of p) - rnorm(1000, mean=5, sd=1) returns n random numbers generated from the distribution\nTo use another distribution, change the function’s suffix to the name of the distribution and the parameters to those that define the distribution. For example, to generate n random numbers from a uniform distribution with a min of 1 and a max of 5, run runif(n, min=0, max=1)."
  },
  {
    "objectID": "notes/probability-distributions.html#nonparametric-probability-distributions",
    "href": "notes/probability-distributions.html#nonparametric-probability-distributions",
    "title": "Probablity distributions",
    "section": "6 Nonparametric probability distributions",
    "text": "6 Nonparametric probability distributions\nWhat if the data does not meet the assumptions of the Gaussian distribution? One option is to choose another parametric probability distribution (run help(Distributions) for a full list of available distributions). Another is to use a nonparametric approach, where the probability distribution is not determined by parameters but is instead determined by the data. - A histogram is actually a simple, nonparametric estimate of a probability distribution. To estimate the probability distribution that generated a set of data from a histogram, we modify the scale of the y-axis so that the total area of the bars is equal to 1. - Kernel density estimation (KDE) is another nonparametric method to estimate a probability distribution. KDE is like a smooth histogram, accomplished by placeing a kernel — a tiny Gaussian distribution — at each observed data point and summing across kernels. We can accomplish this in ggplot with the geom_density() geom."
  },
  {
    "objectID": "notes/probability-distributions.html#further-reading-and-references",
    "href": "notes/probability-distributions.html#further-reading-and-references",
    "title": "Probablity distributions",
    "section": "7 Further reading and references",
    "text": "7 Further reading and references\n\nAppendix A: Statistical Background in Modern Dive\nCh 11: Modeling Randomness in Statistical Modeling\n\nhttps://r4ds.hadley.nz/data-visualize#visualizing-distributions"
  },
  {
    "objectID": "notes/model-reliability.html",
    "href": "notes/model-reliability.html",
    "title": "Model reliability",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/model-accuracy.html",
    "href": "notes/model-accuracy.html",
    "title": "Model accuracy",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/model-accuracy.html#model-accuracy-basics",
    "href": "notes/model-accuracy.html#model-accuracy-basics",
    "title": "Model accuracy",
    "section": "1 Model accuracy basics",
    "text": "1 Model accuracy basics\nWe’ve selected a model (model selection) and fit a model to a set of data (model fitting). One question we might want to ask next is how well does this model describe the data (model accuracy)?\n\nWe can visualize our data and the model fit to get a sense of how accurate the model is. But we also want a way to quantify model accuracy – some metric by which to determine whether a model is useful, or how it compares to other models.\nLast week we learned about one metric of model “goodness”, squared error. We could certainly quantify our model accuracy with squared error, but it would be difficult to interpret since it depends on the units of the data.\nToday we’ll learn about another metric, \\(R^2\\) which is easier to interpret and independent of units. \\(R^2\\) quantifies the percentage of variance in our response variable that is explained by our model."
  },
  {
    "objectID": "notes/model-accuracy.html#coefficient-of-determination",
    "href": "notes/model-accuracy.html#coefficient-of-determination",
    "title": "Model accuracy",
    "section": "2 Coefficient of determination",
    "text": "2 Coefficient of determination\nThe coefficient of determination, \\(R^2\\) quantifies the percentage of variance in the response variable that is explained by the model.\nThe equation for variance:\n\n\\(\\frac{\\sum_{i=1}^n (y_i - m_i)^2}{n-1}\\)\nWe take the sum of squares: square the residuals (\\(i^{th}\\) data point minus the \\(i^{th}\\) model value), then divide by the number of cases, \\(n\\), minus 1.\nNotice this is the same equation as standard deviation, we just haven’t done the squaring part, yet.\n\nThe equation for \\(R^2\\) is then:\n\n\\(R^2=100\\times(1-\\frac{unexplained \\; variance}{total \\; variance})\\)\nor \\(R^2=100\\times(1-\\frac{\\sum_{i=1}^n (y_i - m_i)^2}{\\sum_{i=1}^n (y_i - \\overline{y})^2})\\)\n\nLet’s unpack this equation:\n\nThe \\(total \\; variance\\) (denominator) is the sum of squares of the deviations of the data points from their mean: \\(\\sum_{i=1}^n (y_i - \\overline{y})^2\\). In words, take each y value and subtract it from the mean y value, square it, then add them all up.\nThe \\(unexplained \\; variance\\) (numerator) is the sum of squares of the deviations of the model value from the data (residuals): \\(\\sum_{i=1}^n (y_i - m_i)^2\\). In words, take each y value and subtract it from the model value (the model’s prediction) for that data point, square it, then add them all up.\nNote that we do not include the denominator of the variance equation, \\(n-1\\), since the two would cancel each other out in the full \\(R^2\\) equation.\n\nWe subtract the proportion \\(\\frac{unexplained \\; variance}{total \\; variance}\\) from 1 to get the proportion of variance that is explained, and then we multiply by 100 to turn it into the percent of variance explained.\n\nThere is an upper bound of 100%: the situation where the model explains all the variance (it matches the data exactly)\nThere is technically no lower bound, since models can be arbitrarily bad. 0% indicates the model explains none of the variance (it predicts the mean of the data but nothing else)"
  },
  {
    "objectID": "notes/model-accuracy.html#r2-overestimates-model-accuracy",
    "href": "notes/model-accuracy.html#r2-overestimates-model-accuracy",
    "title": "Model accuracy",
    "section": "3 \\(R^2\\) overestimates model accuracy",
    "text": "3 \\(R^2\\) overestimates model accuracy\nOne thing we can ask is how well the model describes our specific sample of data. But the question we actually want to answer is how well does the model we fit describe the population we are interested in.\n\nThe problem is that we usually only have access to the sample we’ve collected and \\(R^2\\) tends to overestimate the accuracy of the model on the population. In other words, the \\(R^2\\) of the model we fit on our sample will be larger than the \\(R^2\\) of the model fit to the population.\nFurther, the population is (usually) unknown to us. To quantify the true accuracy of a fitted model – that is, how well the model describes the population, not the sample we collected – we can use a technique called cross-validation.\n\nBefore we learn about cross-validation, let’s first try to gain further conceptual understanding of why \\(R^2\\) tends to overestimate model accuracy."
  },
  {
    "objectID": "notes/model-accuracy.html#overfitting",
    "href": "notes/model-accuracy.html#overfitting",
    "title": "Model accuracy",
    "section": "4 Overfitting",
    "text": "4 Overfitting\nWhen you fit a model to some sample of data, there is always a risk of overfitting. As the modeler, you have the freedom to fit your sample data better and better (you can add more and more terms, increasing the \\(R^2\\) value). But you need to be careful not to fit the sample data too well.\n\nThis is because any given set of data contains not only the true, underlying patterns we are interested in (the true model or signal), but also random variation (noise). Fitting the sample data too well means we fit not only the signal but also the noise in the data.\nAn overfit model will perform really well on the data it has been trained on (the sample) — we can even fit the sample perfectly if we add enough terms! - but an overfit model will be bad at predicting new, unseen values. Image we collect an additional data point drawn from the population. An overfit model would predict this point poorly!\nOur goal is to find the optimal fitted model – the one that gets as close to the true model as possible without overfitting. But we have no way of knowing which part of the data we sampled is signal and which part is noise. So, we use cross-validation to help identify overfitting."
  },
  {
    "objectID": "notes/model-accuracy.html#model-complexity",
    "href": "notes/model-accuracy.html#model-complexity",
    "title": "Model accuracy",
    "section": "5 Model complexity",
    "text": "5 Model complexity\nIn the lecture on model specification, we briefly mentioned that we would also want to take into consideration the complexity of the model. Simple models are easier to interpret but may not capture all complexities in the data, while complex models can suffer from overfitting the data or be difficult to interpret. Let’s expand on this in the context of model accuracy.\n\nComplex models have the potential to describe many kinds of functions, and the true model — the model that most accurately describes the population we sampled our data from — could be among them. However, complex models have a lot free parameters to estimate (by definition, that’s what makes them complex!), which makes it more difficult to obtain stable parameter estimates with small samples sizes or noisy data.\nSimple models are limited in the types of functions they can describe, so they may not approximate the true model very accurately. However, they have fewer free parameters, which makes it easier to obtain stable parameter estimates with small sample sizes or noisy data.\nWe have no way of knowing a priori whether a simple or complex model will be more accurate for a given dataset. It depends on many things, including the data we have, the underlying relationships, and our research questions. Luckily, we can use cross-validation to find out, trying different models and quantify each model’s accuracy."
  },
  {
    "objectID": "notes/model-accuracy.html#cross-validation",
    "href": "notes/model-accuracy.html#cross-validation",
    "title": "Model accuracy",
    "section": "6 Cross-validation",
    "text": "6 Cross-validation\nRemember from above, the question we actually want to answer with \\(R^2\\) is not how well does the model we fit describe the sample we collected, but how well does the model we fit describe the population we are interested in. But \\(R^2\\) on the sample will tend to overestimate the model’s accuracy on the population. To estimate the accuracy of the model on the population, we need to use a simple but powerful technique called cross-validation. Given a sample of data, there are 3 simple steps to any cross-validation technique:\n\nLeave some data out\nFit a model (to the data kept in)\nEvaluate the model on the left out data (e.g. \\(R^2\\))\n\nThere are many ways to do cross-validation — reflecting that there are many ways we can leave some data out — but they all follow this general 3-step process. We’ll focus on two common approaches in this class:\n\nIn leave-one-out cross-validation, we leave out a single data point and use the fitted model to predict that single point. We repeat this process for every data point, then evaluate each model’s prediction on the left out points (we can use \\(R^2\\)!).\nIn \\(k\\)-fold cross-validation, instead of leaving out a single data point, we randomly divide the dataset into \\(k\\) parts and use the fitted model to predict that part. We repeat this process for every part, then evaluate each model’s prediction on the left out parts (again, we can use \\(R^2\\)!).\n\nHow do we decide which cross-validation approach to use? There are two trade-offs to consider:\n\nHow many iterations do we want to do? The more iterations, the more reliable our accuracy estimate will be. But the more iterations, the more computational resources are required.\nHow much data do we want to use for each part? The more data we use to fit the model, the more accurate the model will be and the more stable the parameter estimates will be. But the more data we use in to estimate reliability, the more reliable our accuracy estimate will be.\n\n\n\nFor example, in leave-one-out cross-validation we use a lot of iterations (one for each data point), so we need a lot of computational resources, but we get to use almost all the data to fit our model (all but one point!) and all the data to calculate \\(R^2\\).\nKeep in mind that the parameter estimates we obtain on each iteration will be different, because they depend on both the model selected (stays the same each iteration) and the data we fit with (changes each iteration). So the \\(R^2\\) we compute via cross-validation really reflects an estimate of our model’s accuracy when fitted to a particular amount of data."
  },
  {
    "objectID": "notes/model-accuracy.html#back-to-model-selection",
    "href": "notes/model-accuracy.html#back-to-model-selection",
    "title": "Model accuracy",
    "section": "7 Back to model selection",
    "text": "7 Back to model selection\nBuilding models is itself an iterative process: we can use model accuracy obtained via cross-validation to determine which model to select (as a way to find the elusive optimal model fit). There are other ways to evaluate models beyond cross-validaiton. You may encounter AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion), for example, which are parametric approaches that attempt to compare different models and find the optimal fit (helping you avoid overfitting and excessively complex models).\n\nIn general AIC considers how well the model fits the data, the number of parameters, and the sample size (there is a penalty for more complex models); BIC is similar but has a stronger penalty for complex models (so will inherently favor simpler models).\nWe’ll focus on cross-validation in this class, because it makes fewer assumptions than metrics like AIC/BIC and is simpler to understand conceptually.\n\nBeyond model accuracy, there are other practical things one might want to consider when selecting a model, such as ease of interpretation and availability of resources (the data you can collect, the computing power you have, etc.)"
  },
  {
    "objectID": "notes/model-accuracy.html#further-reading",
    "href": "notes/model-accuracy.html#further-reading",
    "title": "Model accuracy",
    "section": "8 Further reading",
    "text": "8 Further reading"
  },
  {
    "objectID": "notes/inference.html",
    "href": "notes/inference.html",
    "title": "Inference",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/hello-world.html",
    "href": "notes/hello-world.html",
    "title": "Hello, world!",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/hello-world.html#data-science",
    "href": "notes/hello-world.html#data-science",
    "title": "Hello, world!",
    "section": "1 Data science",
    "text": "1 Data science\n\nData are descriptions of the world around us, collected through observation and stored on computers. Computers enable us to infer properties of the world from these descriptions. Data science is the discipline of drawing conclusions from data using computation. Computational and Inferential Thinking: The Foundations of Data Science\n\n\n\n\nFigure 1: flowchart from R for Data Science"
  },
  {
    "objectID": "notes/hello-world.html#overview-of-the-course",
    "href": "notes/hello-world.html#overview-of-the-course",
    "title": "Hello, world!",
    "section": "2 Overview of the course",
    "text": "2 Overview of the course\nTopics:\n\nLearn R - we will spend the first few weeks getting comfortable programming in R, including some useful skills for data science:\n\nR basics\nData importing\nData visualization\nData wrangling\n\nFoundations - we will spend the next several weeks building a foundation in basic statistics and model building\n\nProbability distributions\nSampling variability\nHypothesis testing\nModel specification\nModel fitting\nModel accuracy\nModel reliability\n\nSpecials - finally we will cover a selection of more advanced topics that are often applied in language and mind fields, with a focus on basic understanding:\n\nInference for regression\nClassification\nFeature engineering (preprocessing)\nMixed-effect models\n\n\nTypical week:\n\nLecture (Tue and Thu) - conceptual overview of core topics and programming tutorials\nLab (Thu or Fri) - ungraded practice problems and review with TAs\n\nAssessments:\n\n6 Problem sets - applying programming skills\n4 Quizzes - testing understanding of concepts (similar to lab practice questions)"
  },
  {
    "objectID": "notes/hello-world.html#why-r",
    "href": "notes/hello-world.html#why-r",
    "title": "Hello, world!",
    "section": "3 Why R?",
    "text": "3 Why R?\nWith many programming languages available for data science (e.g. R, Python, Julia, MATLAB), why use R?\n\nBuilt for stats, specifically\nMakes nice visualizations\nLots of people are doing it, especially in academia\nEasier for beginners to understand\nFree and open source (though so are Python and Julia, MATLAB costs $)\n\nIf you are interested, here is a math professor’s take on the differences between Python, Julia, and MATLAB. Note that although they’re optimized for different things, they’re all great and the technical skills and conceptual knowledge you gain in this course easily transfers to other languages."
  },
  {
    "objectID": "notes/hello-world.html#google-colab",
    "href": "notes/hello-world.html#google-colab",
    "title": "Hello, world!",
    "section": "4 Google Colab",
    "text": "4 Google Colab\nThere are many ways to program with R. Some popular options include:\n\nR Studio\nJupyter\nVS Code\nand even simply the command line/terminal\n\nGoogle Colab is a cloud-based Jupyter notebook that allows you to write, execute, and share code like a google doc. We use Google Colab because it’s simple and accessible to everyone. You can start programming right away, no setup required! Google Colab officially supports Python, but secretly supports R (and Julia, too!)\nNew R notebook:\n\ncolab (r kernel) - use this link to start a new R notebook\nFile &gt; New notebook error, Python! name 'x' is not defined\n\nCell types:\n\n+ Code - write and execute code\n+ Text - write text blocks in markdown\n\nLeft sidebar:\n\nTable of contents - outline from text headings\nFind and replace - find and/or replace\nFiles - upload files to cloud session\n\nFrequently used menu options:\n\nFile &gt; Locate in Drive - where in your Google Drive?\nFile &gt; Save - saves\nFile &gt; Revision history - history of changes you made\nFile &gt; Download &gt; Download .ipynb - used to submit assignments!\nFile &gt; Print - prints\nRuntime &gt; Run all - run all cells\nRuntime &gt; Run before - run all cells before current active cell\nRuntime &gt; Restart and run all - restart runtime, then run all\n\nFrequently used keyboard shortcuts:\n\nCmd/Ctrl+S - save\nCmd/Ctrl+Enter - run focused cell\nCmd/Ctrl+Shift+A - select all cells\nCmd/Ctrl+/ - comment/uncomment selection\nCmd/Ctrl+] - increase indent\nCmd/Ctrl+[ - decrease indent"
  },
  {
    "objectID": "notes/hello-world.html#further-reading-and-references",
    "href": "notes/hello-world.html#further-reading-and-references",
    "title": "Hello, world!",
    "section": "5 Further reading and references",
    "text": "5 Further reading and references\nRecommended reading: - Ch 1 Data Science from Computational and Inferential Thinking: The Foundations of Data Science"
  },
  {
    "objectID": "notes/data-wrangling.html",
    "href": "notes/data-wrangling.html",
    "title": "Data wrangling",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/data-wrangling.html#why-wrangle-and-tidy",
    "href": "notes/data-wrangling.html#why-wrangle-and-tidy",
    "title": "Data wrangling",
    "section": "1 Why wrangle and tidy?",
    "text": "1 Why wrangle and tidy?"
  },
  {
    "objectID": "notes/data-wrangling.html#data-wrangling-with-dplyr",
    "href": "notes/data-wrangling.html#data-wrangling-with-dplyr",
    "title": "Data wrangling",
    "section": "2 Data wrangling with dplyr",
    "text": "2 Data wrangling with dplyr\nAll dplyr functions (verbs) share a common structure:\n\n1st argument is always a data frame\nSubsequent arguments typically describe which columns to operate on (via their names)\nOutput is always a new data frame\n\nWe can group dplyr functions based on what they operate on:\n\nrows - see section 3 Manipulating rows\ncolumns - see section 4 Manipulating columns\ngroups - see section 5 Grouping and summarizing data frames\ntables - see section 6 Joining data frames\n\nWe can easily combine dplyr functions to solve complex problems:\n\nThe pipe operator, |&gt; takes the output from one function and passes it as input (the first argument) to the next function.\nThere is another version of the pipe, %&gt;%. See the reading on data transformation if you are curious about the difference.\n\nIn lecture, we will demonstrate with a few dplyr functions, but you should feel comfortable reading the docs/resources to use others to solve unique problems."
  },
  {
    "objectID": "notes/data-wrangling.html#manipulating-rows",
    "href": "notes/data-wrangling.html#manipulating-rows",
    "title": "Data wrangling",
    "section": "3 Manipulating rows",
    "text": "3 Manipulating rows\nfilter()\narrange()"
  },
  {
    "objectID": "notes/data-wrangling.html#manipulating-columns",
    "href": "notes/data-wrangling.html#manipulating-columns",
    "title": "Data wrangling",
    "section": "4 Manipulating columns",
    "text": "4 Manipulating columns\nselect()\nmutate()"
  },
  {
    "objectID": "notes/data-wrangling.html#grouping-and-summarizing-data-frames",
    "href": "notes/data-wrangling.html#grouping-and-summarizing-data-frames",
    "title": "Data wrangling",
    "section": "5 Grouping and summarizing data frames",
    "text": "5 Grouping and summarizing data frames"
  },
  {
    "objectID": "notes/data-wrangling.html#joining-data-frames",
    "href": "notes/data-wrangling.html#joining-data-frames",
    "title": "Data wrangling",
    "section": "6 Joining data frames",
    "text": "6 Joining data frames\n\ndon’t cover, just show the functions and say we won’t cover it"
  },
  {
    "objectID": "notes/data-wrangling.html#data-tidying-with-tidyr",
    "href": "notes/data-wrangling.html#data-tidying-with-tidyr",
    "title": "Data wrangling",
    "section": "7 Data tidying with tidyr",
    "text": "7 Data tidying with tidyr\n\nsneak in with case study later; for now just show functions and say we won’t cover it.\n\n\nFurther reading and references\nhttps://r4ds.hadley.nz/data-transform\nhttps://r4ds.hadley.nz/joins"
  },
  {
    "objectID": "notes/data-importing.html",
    "href": "notes/data-importing.html",
    "title": "Data importing",
    "section": "",
    "text": "The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. ~ Tidyverse package docs\n\nThe tidyverse collection of packages includes:\n\nggplot2 - for data visualization\ndplyr - for data wrangling\nreadr - for reading data\ntibble - for modern data frames\nstringr: for string manipulation\nforcats: for dealing with factors\ntidyr: for data tidying\npurrr: for functional programming\n\nWe load the tidyverse like any other package, with library(tidyverse). When we do, we will receive a message with (1) a list packages that were loaded and (2) a warning that there are potential conflicts with base R’s stats functions\n\nWe can resolve conflicts with the :: operator, which allows us to specify which package our intended function belongs to as a prefix: stats::filter() or dplyr::filter()"
  },
  {
    "objectID": "notes/data-importing.html#welcome-to-the-tidyverse",
    "href": "notes/data-importing.html#welcome-to-the-tidyverse",
    "title": "Data importing",
    "section": "",
    "text": "The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. ~ Tidyverse package docs\n\nThe tidyverse collection of packages includes:\n\nggplot2 - for data visualization\ndplyr - for data wrangling\nreadr - for reading data\ntibble - for modern data frames\nstringr: for string manipulation\nforcats: for dealing with factors\ntidyr: for data tidying\npurrr: for functional programming\n\nWe load the tidyverse like any other package, with library(tidyverse). When we do, we will receive a message with (1) a list packages that were loaded and (2) a warning that there are potential conflicts with base R’s stats functions\n\nWe can resolve conflicts with the :: operator, which allows us to specify which package our intended function belongs to as a prefix: stats::filter() or dplyr::filter()"
  },
  {
    "objectID": "notes/data-importing.html#what-is-tidy-data",
    "href": "notes/data-importing.html#what-is-tidy-data",
    "title": "Data importing",
    "section": "2 What is tidy data?",
    "text": "2 What is tidy data?\nThe same underlying data can be represented in a table in many different ways; some easier to work with than others. The tidyverse makes use of tidy data principles to make datasets easier to work with in R. Tidy data provides a standard way of structuring datasets:\n\neach variable forms a column; each column forms a variable\neach observation forms a row; each row forms an observation\nvalue is a cell; each cell is a single value\n\nWhy is tidy data easier to work with?\n\nBecause consistency and uniformity are very helpful when programming\nVariables as columns works well for vectorized languages (R!)"
  },
  {
    "objectID": "notes/data-importing.html#functional-programming-with-purrr",
    "href": "notes/data-importing.html#functional-programming-with-purrr",
    "title": "Data importing",
    "section": "3 Functional programming with purrr",
    "text": "3 Functional programming with purrr\n\npurrr enhances R’s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. If you’ve never heard of FP before, the best place to start is the family of map() functions which allow you to replace many for loops with code that is both more succinct and easier to read. ~ purrr docs\n\nLet’s illustrate the joy of the tidyverse with one of its packages: purrr. The docs say that the best place to start is the family of map() functions, so we’ll do that.\nThe map() functions:\n\ntake a vector as input\napply a function to each element\nreturn a new vector\n\nWe say “functions” because there are 5, one for each type of vector:\n\nmap()\nmap_lgl()\nmap_int()\nmap_dbl()\nmap_chr()\n\nTo illustrate, suppose we have a data frame df with 3 columns and we want to compute the mean of each column. We could solve this with copy-and-paste (run mean() 3 different times) or try to use a for loop, but map() can do this with just one line:\nmap_dbl(df, mean)\nNow imagine we have 5 more data frames and we want to compute the mean of each of their columns, too. Again, we could copy and paste the map() function or use it in a for loop. But the map family allows us go up a layer of abstraction. We can use pmap() when we want to apply a function element-wise to corresponding items in multiple lists."
  },
  {
    "objectID": "notes/data-importing.html#modern-data-frames-with-tibble",
    "href": "notes/data-importing.html#modern-data-frames-with-tibble",
    "title": "Data importing",
    "section": "4 Modern data frames with tibble",
    "text": "4 Modern data frames with tibble\n\nA tibble, or tbl_df, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less and complain more ~ tibble docs\n\nTibbles do less than data frames, in a good way:\n\nnever changes type of input (never converts strings to factors!)\nnever changes the name of variables\nonly recycles vectors of length 1\nnever creates row names\n\nYou can read more in vignette(“tibble”) if you are interested, but understanding these differences is not necessary to be successful in the course. The take-away is that data.frame and tibble sometimes behave differently. The behavior of tibble makes more sense for modern data science, so we should us it instead!\nCreate a tibble with one of the following:\n# (1) coerce an existing object with\nas_tibble(x)\n\n# (2) pass a column of vectors \ntibble(x=1:5, y=1)\n\n# (3) define row-by-row, short for traansposed tibble\ntribble(\n    ~x, ~y, ~z,\n    \"a\", 2, 3.6,\n    \"b\", 1, 8.5\n)\nWe will encounter two main ways tibbles and data frames differ:\n\nprinting - by default, tibbles print the first 10 rows and all columns that fit on screen, making it easier to work with large datasets. Tibbles also report the type of each column (e.g. &lt;dbl&gt;, &lt;chr&gt;)\nsubsetting - tibbles are more strict than data frames, which fixes two quirks we encountered last lecture when subsetting with [[ and $: (1) tibbles never do partial matching, and (2) they always generate a warning if the column you are trying to extract does not exist.\n\nTo test if something is a tibble or a data.frame:\n\nis_tibble(x)\nis.data.frame(x)"
  },
  {
    "objectID": "notes/data-importing.html#reading-data-with-readr",
    "href": "notes/data-importing.html#reading-data-with-readr",
    "title": "Data importing",
    "section": "5 Reading data with readr",
    "text": "5 Reading data with readr\n\nThe goal of readr is to provide a fast and friendly way to read rectangular data from delimited files, such as comma-separated values (CSV) and tab-separated values (TSV). It is designed to parse many types of data found in the wild, while providing an informative problem report when parsing leads to unexpected results. ~ readr docs\n\nOften we want to read in some data we’ve generated or collected outside of R. The most basic and common format is plain-text rectangular files. We will “read” these into R with readr’s read_*() functions.\nThe read_*() functions have two important arguments:\n\nfile path - the path to the file (that reader will try to parse)\ncolumn specification - a description of how each column should be converted from a character vector to a specific data type (col_types)\n\nThere are 7 supported file types, each with their own read_*() function:\n\nread_csv(): comma-separated values (CSV)\nread_tsv(): tab-separated values (TSV)\nread_csv2(): semicolon-separated values\nread_delim(): delimited files (CSV and TSV are important special cases)\nread_fwf(): fixed-width files\nread_table(): whitespace-separated files\nread_log(): web log files\n\nTo read .csv files, include a path and (optionally) a column specification:\n# (1) pass only the path; readr guesses col_types \nread_csv(path='path/to/file.csv')\n\n# (2) include a column specification with col_types\nread_csv(\n    path='path/to/file.csv', \n    col_types = list( x = col_string(), y = col_skip() )\n)\nWith no colum specification, readr uses the the first 1000 rows to guess with a simple heuristic:\n\nif column contains only T/F, logical\nif only numbers, double\nif ISO8601 standard, date or date-time\notherwise string\n\nThere are 11 column types that can be specified:\n\ncol_logical() - reads as boolean TRUE FALSE values\ncol_integer() - reads as integer\ncol_double() - reads as double\ncol_number() - numeric parser that can ignore non-numbers\ncol_character() - reads as strings\ncol_factor(levels, ordered = FALSE) - creates factors\ncol_datetime(format = \"\") - creates date-times\ncol_date(format = \"\") - creates dates\ncol_time(format = \"\") - creates times\ncol_skip() - skips a column\ncol_guess() - tries to guess the column\n\nSome useful additional arguments:\n\nif there is no header, include col_names = FALSE\nto provide a header, include col_names = c(\"x\",\"y\",\"z\")\nto skip some lines, include skip = n, where n is number of lines to skip\nto select which columns to import, include col_select(x, y)\nto guess column types with all rows, include guess_max = Inf\n\nSometimes weird things happen. The most common problems are:\n\nmissing values are not NA - your dataset has missing values, but they are not coded as NA as R expects. Solve by adding na argument (e.g. na=c(\"N/A\"))\ncolumn names have spaces - R cannot include spaces in variable names, so it adds backticks (e.g. `brain size`); we can just refer to them with backticks, but if that gets annoying, see janitor::clean_names() to fix them!\n\nReading more complex file types requires functions outside the tidyverse:\n\nexcel with readxl - see Spreadsheets in R for Data Science\ngoogle sheets with googlesheets4 - see Spreadsheets in R for Data Science\ndatabases with DBI - see Databases in R for Data Science\njson data with jsonlite - see Hierarchical data in R for Data Science"
  },
  {
    "objectID": "notes/data-importing.html#further-reading-and-references",
    "href": "notes/data-importing.html#further-reading-and-references",
    "title": "Data importing",
    "section": "6 Further reading and references",
    "text": "6 Further reading and references\nRecommended further reading:\n\nData tidying in R for Data Science\nTibbles in R for Data Science\nData import in R for Data Science:\nreadr cheatsheet"
  },
  {
    "objectID": "labs/1-getting-started-with-r-lab.html",
    "href": "labs/1-getting-started-with-r-lab.html",
    "title": "Lab 1: Getting started with R",
    "section": "",
    "text": "To learn to program in R (or any language), you can read about how to do it, and watch someone else do it; but the only way to really learn is to do it yourself. Create some data structures, try some stuff, and see what happens! Here are some practice quiz questions to guide your learning. We will go over the solutions to these in lab."
  },
  {
    "objectID": "labs/1-getting-started-with-r-lab.html#r-basics",
    "href": "labs/1-getting-started-with-r-lab.html#r-basics",
    "title": "Lab 1: Getting started with R",
    "section": "1 R Basics",
    "text": "1 R Basics\n\nWhich of the following are expressions? Choose all that apply.\n\n10\n5 + 10\nx &lt;- 5 + 10\nx &lt;- y + 10\nmean(x)\n\nWhich of the following are valid variable names in R?\n\nchildAge\nresponse_time\n1stPlaceWinner\n2fast2furious\nfast&furious\n\nSuppose we run the following code block. What will ls() return?\nx &lt;- mean(1, 3, 5)\ny &lt;- median(1, 3, 5)\nSuppose we run library(tidyverse) in Google Colab and receive the following error message? How do we solve this problem?\n\nNameError: name 'library' is not defined"
  },
  {
    "objectID": "labs/1-getting-started-with-r-lab.html#vectors",
    "href": "labs/1-getting-started-with-r-lab.html#vectors",
    "title": "Lab 1: Getting started with R",
    "section": "2 Vectors",
    "text": "2 Vectors\n\nThe expression attributes(x) returns the following result. What will typeof(x) return?\n\n$dim= 2 . 2\n\nSuppose we run the following code block. What will typeof(x) return?\nx &lt;- c()\nExplain in one sentence the difference between an atomic vector and a list.\nSuppose we run the following code block. What will typeof(x) return? What about length(x)? Explain why.\nx &lt;- data.frame(x=c(1,2,3), y=c(\"a\",\"b\",\"c\"))\nSuppose we run the following code. What will y return? What about typeof(y)?\nx &lt;- c(2,4,6)\ny &lt;- x * 2"
  },
  {
    "objectID": "labs/1-getting-started-with-r-lab.html#subsetting",
    "href": "labs/1-getting-started-with-r-lab.html#subsetting",
    "title": "Lab 1: Getting started with R",
    "section": "3 Subsetting",
    "text": "3 Subsetting\n\nSuppose we run the following code. What will each of operations return (a - e)?\nx &lt;- seq(2:8, by=2)\n\nx[c(2,4)]\nx[-c(2,4)]\nx[]\nx[[2]]\nx[2]\n\nSuppose m is a matrix created with matrix(c(1,2,3,4), nrow=2, ncol=2). What will each of the following operations return?\n\nm[c(1), ]\nm[c(2), c(1, 2)]\nm[]\nm[[2]]\n\nSuppose df is a data frame created with data.frame(x=c(1,2,3), y=c(\"a\",\"b\",\"c\")). What will each of the following operations return?\n\ndf[c(1,2)]\ndf[c(1,2), c(2)]\ndf[['x']]\ndf[x]\ndf[[2]]\n\nSuppose df is a dataframe with column names ageChild, ageParent, and dateAdded. What will df$age return? Explain why."
  },
  {
    "objectID": "labs/1-getting-started-with-r-lab.html#programming-in-r",
    "href": "labs/1-getting-started-with-r-lab.html#programming-in-r",
    "title": "Lab 1: Getting started with R",
    "section": "4 Programming in R",
    "text": "4 Programming in R\n\nExplain in one sentence the difference between a for loop and a while loop?"
  },
  {
    "objectID": "drafts.html",
    "href": "drafts.html",
    "title": "Working documents",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 29, 2023\n\n\nHello, world!\n\n\nKatie Schuler\n\n\n\n\nAug 31, 2023\n\n\nR Basics\n\n\nKatie Schuler\n\n\n\n\nAug 31, 2023\n\n\nR basics\n\n\nKatie Schuler\n\n\n\n\nSep 5, 2023\n\n\nData importing\n\n\nKatie Schuler\n\n\n\n\nSep 7, 2023\n\n\nData visualization\n\n\nKatie Schuler\n\n\n\n\nSep 12, 2023\n\n\nData wrangling\n\n\nKatie Schuler\n\n\n\n\nSep 12, 2023\n\n\nProbablity distributions\n\n\nKatie Schuler\n\n\n\n\nSep 20, 2023\n\n\nSampling variability\n\n\nKatie Schuler\n\n\n\n\nSep 26, 2023\n\n\nHypothesis testing\n\n\nKatie Schuler\n\n\n\n\nOct 3, 2023\n\n\nModel specification\n\n\nKatie Schuler\n\n\n\n\nOct 5, 2023\n\n\nModel fitting\n\n\nKatie Schuler\n\n\n\n\nOct 19, 2023\n\n\nModel accuracy\n\n\nKatie Schuler\n\n\n\n\nOct 24, 2023\n\n\nModel reliability\n\n\nKatie Schuler\n\n\n\n\nNov 7, 2023\n\n\nClassification\n\n\nKatie Schuler\n\n\n\n\nNov 14, 2023\n\n\nFeature engineering\n\n\nKatie Schuler\n\n\n\n\nNov 18, 2023\n\n\nInference\n\n\nKatie Schuler\n\n\n\n\nDec 5, 2023\n\n\nMixed-effects models\n\n\nKatie Schuler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts.html#lecture-notes",
    "href": "drafts.html#lecture-notes",
    "title": "Working documents",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 29, 2023\n\n\nHello, world!\n\n\nKatie Schuler\n\n\n\n\nAug 31, 2023\n\n\nR Basics\n\n\nKatie Schuler\n\n\n\n\nAug 31, 2023\n\n\nR basics\n\n\nKatie Schuler\n\n\n\n\nSep 5, 2023\n\n\nData importing\n\n\nKatie Schuler\n\n\n\n\nSep 7, 2023\n\n\nData visualization\n\n\nKatie Schuler\n\n\n\n\nSep 12, 2023\n\n\nData wrangling\n\n\nKatie Schuler\n\n\n\n\nSep 12, 2023\n\n\nProbablity distributions\n\n\nKatie Schuler\n\n\n\n\nSep 20, 2023\n\n\nSampling variability\n\n\nKatie Schuler\n\n\n\n\nSep 26, 2023\n\n\nHypothesis testing\n\n\nKatie Schuler\n\n\n\n\nOct 3, 2023\n\n\nModel specification\n\n\nKatie Schuler\n\n\n\n\nOct 5, 2023\n\n\nModel fitting\n\n\nKatie Schuler\n\n\n\n\nOct 19, 2023\n\n\nModel accuracy\n\n\nKatie Schuler\n\n\n\n\nOct 24, 2023\n\n\nModel reliability\n\n\nKatie Schuler\n\n\n\n\nNov 7, 2023\n\n\nClassification\n\n\nKatie Schuler\n\n\n\n\nNov 14, 2023\n\n\nFeature engineering\n\n\nKatie Schuler\n\n\n\n\nNov 18, 2023\n\n\nInference\n\n\nKatie Schuler\n\n\n\n\nDec 5, 2023\n\n\nMixed-effects models\n\n\nKatie Schuler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts.html#labs",
    "href": "drafts.html#labs",
    "title": "Working documents",
    "section": "Labs",
    "text": "Labs\n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nLab 1: Getting started with R\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts.html#problem-sets",
    "href": "drafts.html#problem-sets",
    "title": "Working documents",
    "section": "Problem sets",
    "text": "Problem sets\n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nProblem set 1\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LING 0700: Datasci for lang & mind",
    "section": "",
    "text": "contact: katie, june, avinash, ravi\nresources: ed, canvas, gradescope, colab\n\n\n\n\nWeek\nDay\nTopic\nResources\n\n\n\n\n1\n08-28\nHello, world!\n\n\n\n1\n08-31\nR-basics\n\n\n\n2\n09-05\nData importing\n\n\n\n2\n09-07\nData visualization\n\n\n\n2\n09-10\nProblem set 1\n\n\n\n3\n09-12\nData wrangling\n\n\n\n3\n09-14\nProbability distributions\n\n\n\n4\n09-19\nQuiz 1\n\n\n\n4\n09-20\nSampling variability\n\n\n\n5\n09-26\nHypothesis testing\n\n\n\n5\n09-28\nR tutorial\n\n\n\n5\n10-01\nProblem set 2\n\n\n\n6\n10-03\nModel specification\n\n\n\n6\n10-05\nModel fitting\n\n\n\n7\n10-10\nQuiz 2\n\n\n\n7\n10-12\nNo class, break\n\n\n\n8\n10-17\nR tutorial\n\n\n\n8\n10-19\nModel accuracy\n\n\n\n8\n10-22\nProblem set 3\n\n\n\n9\n10-24\nModel reliability\n\n\n\n9\n10-26\nR tutorial\n\n\n\n10\n10-31\nQuiz 3\n\n\n\n10\n11-02\nTBD guest\n\n\n\n11\n11-07\nClassification\n\n\n\n11\n11-09\nR tutorial\n\n\n\n11\n11-12\nProblem set 4\n\n\n\n12\n11-14\nInference\n\n\n\n12\n11-16\nR tutorial\n\n\n\n13\n11-21\nQuiz 4\n\n\n\n13\n11-23\nNo class, break\n\n\n\n14\n11-28\nFeature engineering\n\n\n\n14\n11-30\nR tutorial\n\n\n\n14\n12-03\nProblem set 5\n\n\n\n15\n12-05\nMultilevel models\n\n\n\n15\n12-07\nR tutorial\n\n\n\n15\n12-10\nProblem set 6"
  },
  {
    "objectID": "notes/classification.html",
    "href": "notes/classification.html",
    "title": "Classification",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/data-visualization.html",
    "href": "notes/data-visualization.html",
    "title": "Data visualization",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/data-visualization.html#why-visualize-data",
    "href": "notes/data-visualization.html#why-visualize-data",
    "title": "Data visualization",
    "section": "1 Why visualize data?",
    "text": "1 Why visualize data?\n\nVisualization is a fundamentally human activity. A good visualization will show you things you did not expect or raise new questions about the data. A good visualization might also hint that you’re asking the wrong question or that you need to collect different data. ~ R for Data Science"
  },
  {
    "objectID": "notes/data-visualization.html#data-visualization-with-ggplot2",
    "href": "notes/data-visualization.html#data-visualization-with-ggplot2",
    "title": "Data visualization",
    "section": "2 Data visualization with ggplot2",
    "text": "2 Data visualization with ggplot2\nthree main parts:\n\ndata- your (tidy) data\naesthetic mappings (aes) - make data visible\ngeometric objects -\n\nother layers:\n\nfacets\nstatistics - statistical transformations of data\nscales - map data values to visual values of aestetic\ncoordinates\nthemes - overall visuals"
  },
  {
    "objectID": "notes/data-visualization.html#aesthetic-mapping",
    "href": "notes/data-visualization.html#aesthetic-mapping",
    "title": "Data visualization",
    "section": "3 Aesthetic mapping",
    "text": "3 Aesthetic mapping\naesthetic mappings (aes), make data visible\n\nx, y: variable on x and y axis\ncolor: outline color of geom\nfill: fill color of geom\ngroup: group geom belongs to\nshape: shape used to plot point (circle, square, etc)\nlinetype: type of line used (solid, dash, et)\nsize"
  },
  {
    "objectID": "notes/data-visualization.html#geometric-objects",
    "href": "notes/data-visualization.html#geometric-objects",
    "title": "Data visualization",
    "section": "4 Geometric objects",
    "text": "4 Geometric objects\ngeometric objects (geoms) - type of plot"
  },
  {
    "objectID": "notes/data-visualization.html#additional-layers",
    "href": "notes/data-visualization.html#additional-layers",
    "title": "Data visualization",
    "section": "5 Additional layers",
    "text": "5 Additional layers\nWe’ll cover some in the class:\n\nfacets\n\nfacet_grid(.~var1)\nfacet_grid(var2~.)\nfacet_grid(var2~var1)\nfacet_wrap(~var1) - wrap facets\n“scales”?\n\ncoordinates\n\ncoord_cartesian(xlim=c(0,5)) - xlim, ylim\ncoord_flip()\n\nlabels\n\nlabs(title=\"plot title\") - x, y, subtitle\nannotate()\n\nstatistics\nscales\nposition adjustments\nthemes\n\ntheme(legend.position=\"bottom\")"
  },
  {
    "objectID": "notes/data-visualization.html#saving-plots",
    "href": "notes/data-visualization.html#saving-plots",
    "title": "Data visualization",
    "section": "6 Saving plots",
    "text": "6 Saving plots\nHelper functions:\n\nlast_plot() - returns the last plot\nggsave(\"plot.png\", width=5, height=5) - saves last plot\n\n\nFurther reading and references\nUseful resources: - ggplot cheat sheet - introduction to palmerpenguins\nRecommended further reading: - https://moderndive.com/2-viz.html\nOther references:\nhttps://r.qcbs.ca/workshop03/book-en/the-basics-of-visualizing-data.html\nhttps://r4ds.hadley.nz/layers"
  },
  {
    "objectID": "notes/feature-engineering.html",
    "href": "notes/feature-engineering.html",
    "title": "Feature engineering",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/feature-engineering.html#dummy-coding",
    "href": "notes/feature-engineering.html#dummy-coding",
    "title": "Feature engineering",
    "section": "1 Dummy coding",
    "text": "1 Dummy coding\nIncluding categorical variables in a model:\n\nOne approach is to create “dummy variables”, called dummy coding, to represent the different levels of the categorical variable. Dummy variables are coded as 1 or 0 (binary) to indicate the presence or absence of a specific category or level. R does this by default.\nIn our swim records example, R dummy codes gender by creating a dummy variable male, where 1 means true and 0 means false. We do not also need a female variable because when male is 0, the person must be female. Creating an additional variable gives no new information. The “left out” category is called the reference level. Unless you specify otherwise, R uses whatever is alphabetically first as the reference level.\nIf a categorical variable has more than two levels, we can simply add a column. To illustrate, imagine our gender variable has a third level, “non-binary”. We create our male column as before (1 for true, 0 for false), and add a non-binary column (1 for true, 0 for false). If the swimmer is neither male nor non-binary (0 in both columns), they must be female. We can have as many levels as we like in a categorical variable, but we should be aware that this adds as many terms (minus 1) to the model! Here \\(x_2\\) is the vector for the male variable and \\(x_3\\) is the vector for thenon-binary variable:\n\nin R: y ~ 1 + year + gender\nin eq: \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3\\)"
  },
  {
    "objectID": "notes/hypothesis-testing.html",
    "href": "notes/hypothesis-testing.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/hypothesis-testing.html#hypothesis-testing",
    "href": "notes/hypothesis-testing.html#hypothesis-testing",
    "title": "Hypothesis testing",
    "section": "1 Hypothesis testing",
    "text": "1 Hypothesis testing\n\nWe can’t! At least we can’t quantify it. Intead we can use a cool statistical approach to help us: hypothesis test\nHow it works: pos a NULL hypothesis?\n\nwhy null? we need to quantify it\nask if the null were true, how likely is that we’d observe this result?"
  },
  {
    "objectID": "notes/hypothesis-testing.html#p-values",
    "href": "notes/hypothesis-testing.html#p-values",
    "title": "Hypothesis testing",
    "section": "2 p-values",
    "text": "2 p-values\n\nOne way to quantify how likely this is with a ttest.\n\nt-tests are parametric — we figure out the probability theoretically based on an existing distribution\ncomputes a p-value, which quantifies how likely it is we’d observe some result\ndoes the t-test construct the null? I think yes, theoretically, or because properties are known.\n\nInterpretation of p-value\n\nreject the null (p &lt; some threshold)\nfail to reject the null (p &gt; some threshold)"
  },
  {
    "objectID": "notes/hypothesis-testing.html#nonparametric-approaches-to-p-values",
    "href": "notes/hypothesis-testing.html#nonparametric-approaches-to-p-values",
    "title": "Hypothesis testing",
    "section": "3 Nonparametric approaches to p-values",
    "text": "3 Nonparametric approaches to p-values\n\nBut what if we aren’t doing the mean? Or the distribution isn’t normal? We can use the same principle of the sampling distribution. Two approaches to construct the null distribution from the data:\n\nResampling: one appraoch: shuffle around, then get the mean, construct null distribution\nBootstrapping: another approach: sample with replacement, get the mean, construct null distribution\nwhich one should we pick? see k\n\np-value\n\nhow likely is the observed mean? with either of these null distributions we can use the confidence interval\nbeforehand we set an alpha value (usually 0.05) which is the cutoff for what we think is likely/unlikely."
  },
  {
    "objectID": "notes/hypothesis-testing.html#there-is-only-one-test",
    "href": "notes/hypothesis-testing.html#there-is-only-one-test",
    "title": "Hypothesis testing",
    "section": "4 There is only one test",
    "text": "4 There is only one test\n(insert image of there is only one test)\n\nAllows us to appreciate that, though there is a myriad of statistical tests available, there is really only one test:\n\ndescibe the flow chart\n\nWe can use the infer package to perform this kind of hypothesis testing\n\nShow example.\n\n5 Exploring relationships\n\nLast week we explored data in which we measured a single quantity: brain size. We explored this dataset with a histogram and modeled it with a single value (mean). Suppose we have a slightly more complex dataset in which we measure both brain size and body mass (two quantities!). We might want to know whether there is a relationship between brain size and body mass. We can explore the relationship between two quantities visually with a scatter plot.\nIf there is no relationship between the variables, we say they are independent. We can think of independence in the following way: knowing the value of one variable provides no information about the other variable. In our example, knowing an animal’s body size provides no information about their brain size. If there is some relationship between the variables, we can consider two types:\n\nThere may be a linear relationship between the variables. When one goes up the other goes up (positive) or when one goes up the other goes down (negative). In our example, there is a linear relationship between brain size and body mass: as body mass increases, brain size also increases.\nOr a nonlinear relationship. Nonlinear is a very broad category that encompasses all relationships that are not linear (e.g. a U-shaped curve)."
  },
  {
    "objectID": "notes/hypothesis-testing.html#correlation",
    "href": "notes/hypothesis-testing.html#correlation",
    "title": "Hypothesis testing",
    "section": "6 Correlation",
    "text": "6 Correlation\nOne way to quantify linear relationships is with correlation (\\(r\\)). Correlation expresses the linear relationship as a range from -1 to 1, where -1 means the relationship is perfectly negative and 1 means the relationship is perfectly positive.\nCorrelation can be calculated by taking the z-score of each variable (a normalization technique in which we subtract the mean and divide by the standard deviation) and then compute the average product of each variable:\n\nshow equation\nwe can achieve this calculation in R by computing\nadd getting z-score with R?\nCorrelation only describes linear relationships.\n\nIs the correlation we observed significantly different from zero? We can apply the techniques we learned over the past few weeks to find out. Just like the mean — and all other test statistics! — \\(r\\) is subject to sampling variability. We can indicate our uncertainty around the correlation we observe in the same way: construct the sampling distribution of the correlation via bootstrapping, compute a confidence interval, and compute the p-value.\n(demo with infer framework)"
  },
  {
    "objectID": "notes/hypothesis-testing.html#further-reading",
    "href": "notes/hypothesis-testing.html#further-reading",
    "title": "Hypothesis testing",
    "section": "7 Further reading",
    "text": "7 Further reading\n\nThe logic of hypothesis testing - Chapter 13, Statistical Modeling\nHypothesis testing - Chapter 9, Modern Dive"
  },
  {
    "objectID": "notes/mixed-effects-models.html",
    "href": "notes/mixed-effects-models.html",
    "title": "Mixed-effects models",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/model-fitting.html",
    "href": "notes/model-fitting.html",
    "title": "Model fitting",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/model-fitting.html#model-fitting-basics",
    "href": "notes/model-fitting.html#model-fitting-basics",
    "title": "Model fitting",
    "section": "1 Model fitting basics",
    "text": "1 Model fitting basics\n\nin context of model building more broadly\na genear overview of the concept"
  },
  {
    "objectID": "notes/model-fitting.html#mean-squared-error",
    "href": "notes/model-fitting.html#mean-squared-error",
    "title": "Model fitting",
    "section": "2 Mean squared error",
    "text": "2 Mean squared error\nCost function."
  },
  {
    "objectID": "notes/model-fitting.html#error-surface",
    "href": "notes/model-fitting.html#error-surface",
    "title": "Model fitting",
    "section": "3 Error surface",
    "text": "3 Error surface\n\nWe can visualize the error surface for simple example: 2 parameters, \\(\\beta_0\\) and \\(\\beta_1\\), and the cost function (mean square error).\nShow nonlinear model v linear model figs\ngoal is to find the minimum point\nnotice the nonlinear model can have local minimums but lm has only 1. Because lm is a convex function."
  },
  {
    "objectID": "notes/model-fitting.html#gradient-descent",
    "href": "notes/model-fitting.html#gradient-descent",
    "title": "Model fitting",
    "section": "4 Gradient descent",
    "text": "4 Gradient descent\nIF we want to estimate the free parameters in a way that would work broadly, for linear or nonlinear models, we can use gradient descent.\n\nmachine learning / optimization.\nIf we have a lot of data, we could use stochastic gradient descent which is the same except we…"
  },
  {
    "objectID": "notes/model-fitting.html#ordinary-least-squares",
    "href": "notes/model-fitting.html#ordinary-least-squares",
    "title": "Model fitting",
    "section": "5 Ordinary least squares",
    "text": "5 Ordinary least squares\nAs we saw above, linear models have the special property that they have a solution, the OLS. Rather than searching the error surface iteratively via gradient descent (optimization), we can solve for this point directly with linear algebra.\n\nmatrix approach: we write the 3-step function.\nuse lm() in R.\ninfer approach:\n\nspecify(), fit()\n\n\n\nFurther reading\n\nCh. 8 Fitting models to data in Statistical Modeling"
  },
  {
    "objectID": "notes/model-specification.html",
    "href": "notes/model-specification.html",
    "title": "Model specification",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/model-specification.html#correlation-as-model-building",
    "href": "notes/model-specification.html#correlation-as-model-building",
    "title": "Model specification",
    "section": "1 Correlation as model building",
    "text": "1 Correlation as model building\nCorrelation is actually a simple case of model building in which we use one value (\\(x\\)) to predict another (\\(y\\)). Specifically, we are fitting the linear model \\(y = ax + b\\), where \\(a\\) and \\(b\\) are free parameters. Here, \\(y\\) is known as the response variable (the value we are trying to predict) and \\(x\\) is the explanatory variable (the one that we are attempting to explain the response variable with).\n\nAfter z-scoring our variables, the correlation between \\(x\\) and \\(y\\) is equal to the slope of the line that best predicts \\(y\\) from \\(x\\). In our example data set, the correlation between brain size and body mass is NUM and the slope of the line that best describes the relationship between these z-scored variables is also NUM."
  },
  {
    "objectID": "notes/model-specification.html#model-building-overview",
    "href": "notes/model-specification.html#model-building-overview",
    "title": "Model specification",
    "section": "2 Model building overview",
    "text": "2 Model building overview\nIdeally we want to understand statistical modeling beyond the simple case of correlation. What if we have more than one explanatory variable? What if the relationship between variables is not linear? To address model building more broadly, it is helpful to think of building any model as a four-step process. We’ll treat each of these separately over the coming weeks. The goal for today is to get a big picture overview of the model building process and the types of models we might encounter in our research.\n\nModel specification (this week): what is the form?\nModel fitting (week 6); you have the form, how do you guess the free parameters?\nModel accuracy (week 7): you’ve estimated the parameters, how well does that model describe your data?\nModel reliability (week 8): when you estimate the parameters, there is some uncertainty on them"
  },
  {
    "objectID": "notes/model-specification.html#types-of-models",
    "href": "notes/model-specification.html#types-of-models",
    "title": "Model specification",
    "section": "3 Types of models",
    "text": "3 Types of models\nModel specification involves deciding which type of model we’d like to apply. We will mostly apply linear models in this class — for good reasons we will talk about next time! — but it’s useful to first have a conceptual overview of the types of models we could apply.\n\n\nSupervised vs unsupervised\nAs a starting point, we can divide statistical models into two types of learning (so called because we are trying to “learn” something about the data):\n\nIn supervised learning, we want to predict an output (or response) variable based on one or more input (or explanatory) variables. We call this supervised learning because both the input and output variables are known (sometimes this is called “labeled” data), and we are trying to learn the relationship between them. Linear regression is an example of a supervised learning model.\nIn unsupervised learning, there is no specific output variable that we are trying to predict. Instead, the model’s objective is to discover the underlying structure or patterns in the data. We call this unsupervised learning because only the input data is available (sometimes this is called “unlabeled” data); the model is trying to identify relationships in the data without being “supervised” by an outcome variable. PCA and cluster analysis are examples of unsupervised learning.\nThere are other machine learning approaches beyond these, like semi-supervised learning (combining both labeled and unlabeled data) and reinforcement learning (learning through trial and error based on rewards or penalties). But in this course we will focus on supervised learning models.\n\n\n\nRegression v classification\nRegression and classification are both types of supervised learning models — using one or more input variables to predict an output variable. The only difference between them is in type of output variable:\n\nRegression is used when we want to predict a continuous output, meaning it is a number that can take on any value within a range (e.g. height, weight, response time)\nClassification is used when we want to predict a categorical output, meaning it falls into specific classes or categories (e.g. true/false, yes/no, male/female/nonbinary). We cover this during advanced model building.\n\n\n\nLinear v nonlinear regression\nThere are many types of regression models, but we can simplify by dividing them into two main types of models:\n\nIn linear regression, the relationship between the explanatory variable(s) and response variable is represented by a linear equation (a straight line graphed on a two-dimensional plane).\nNonlinear regression is useful when the data does not follow a linear pattern, and the relationship between the variables is better captured by more complex functions (e.g. a curve or any other nonlinear shape). Nonlinear regression models can be further divided into two types:\n\nWe can linearize a nonlinear model by applying a mathematical transformation to make it look like a linear equation (e.g. log, square root, etc). We can fit a linearized model just like a linear model, but the prediction of the model is not linear with respect to x.\nSometimes, no matter what transformation you apply, you cannot acheive a linear form. These types of models are referred to as nonlinearizable nonlinear models and are beyond the scope of this class!"
  },
  {
    "objectID": "notes/model-specification.html#model-specification",
    "href": "notes/model-specification.html#model-specification",
    "title": "Model specification",
    "section": "4 Model specification",
    "text": "4 Model specification\nRecall from last week that model specification is one aspect of the model building process. It involves selecting the functional form of the model (the type of model) and choosing which variables to include. When specifying a model, you’ll need to make the following decisions:\n\nResponse variable (\\(y\\)): Choose the variable you want to predict or explain (output).\nExplanatory variables(\\(x_n\\)): Choose the variables that may explain the variation in the response variable (inputs).\nFunctional form: Specify the functional relationship between the response and explanatory variables. For linear models, the relationship is linear, and we use the linear model equation as our functional form!\nModel terms: Choose which model terms to include, which is another way of saying that you need to decide how to include your explanatory variables in the model (since they can be included in more than one way).\n\nIn addition to the decisions above, the following issues can also be considered part of the model specification process. But we will consider these in future weeks.\n\nModel assumptions: Check any assumptions underlying the model you selected (e.g. does the model assume the relationship is linear?).\nModel complexity: Simple models are easier to interpret but may not capture all complexities in the data. Complex models may suffer from overfitting the data or being difficult to interpret.\n\nA well-specified model should be based on a clear understanding of the data, the underlying relationships, and the research question."
  },
  {
    "objectID": "notes/model-specification.html#response-variable-y",
    "href": "notes/model-specification.html#response-variable-y",
    "title": "Model specification",
    "section": "5 Response variable, \\(y\\)",
    "text": "5 Response variable, \\(y\\)\nChoosing the response variable is usually straightforward once you’ve clearly defined your research question: what is the thing you are trying to understand? You also need to make sure whatever you’ve selected is something you can measure (or has already been measured!)\n\nIn the swim records example, we are trying to explain variation in record times, so we choose record time as our \\(y\\). In the brain size example we are trying to explain variation in brain sizes, so we choose brain size as our \\(y\\).\nRemember from last time that we can use regression for continuous response variables (numbers) but we need to use classification if the response variable is categorical (categories or levels)."
  },
  {
    "objectID": "notes/model-specification.html#explanatory-variables-x_n",
    "href": "notes/model-specification.html#explanatory-variables-x_n",
    "title": "Model specification",
    "section": "6 Explanatory variables, \\(x_n\\)",
    "text": "6 Explanatory variables, \\(x_n\\)\nChoosing which explanatory variables to include requires a bit more careful consideration. It’s one part using your knowledge about the domain you are studying and one part exploratory data analysis!\n\nOne extreme would be to include just one explanatory variable: the obvious one based on your research question. In the swim records example, we want to understand how swim records change over time, so we should definitely include time as an explanatory variable. But this model is underspecified. We need to consider other variables that have the potential to explain variation in our response variable, even if they are not of direct interest. For example, some variation in swim record times can likely be explained by the swimmer’s gender, so we should include gender as an explanatory variable in our model.\nImportantly, we do not include every explanatory variable we can think of! We want to explain the variation in our response variable without building too complex a model or overfitting the data (overspecifying). We’ll go into more detail about this in future lectures. For now, just remember you’re Goldilocks: you want the explanatory variables in the model to explain just the right amount of variation."
  },
  {
    "objectID": "notes/model-specification.html#functional-form",
    "href": "notes/model-specification.html#functional-form",
    "title": "Model specification",
    "section": "7 Functional form",
    "text": "7 Functional form\nLast lecture we introduced the types of models that we could select when specifying a model. We also mentioned that we will focus on linear models in this class (which are a type of regression model, which are themselves types of supervised learning models!)\nWhen specifying the functional form of a model, we’re literally specifying the mathematical formula we’re going to use to represent the relationship between our response and explanatory variables. Linear models are models in which the response variable (output) is a weighted sum of the explanatory variables (inputs). In other words, there is a linear relationship between the response variable and the explanatory variables. The linear model equation can be expressed in many ways (which can be confusing!). Here are four different ways of representing the formula for the linear model, to emphasize that they are all the same thing.\n\nIn high school algebra, the linear model equation is represented as the equation of a straight line. \\(y\\) is the response variable, \\(x\\) is the explanatory variable, \\(a\\) is the slope of the line (the relationship between \\(x\\) and \\(y\\)) and \\(b\\) is the y-intercept (the value of \\(y\\) when \\(x\\) is zero). You have (hopefully!) already encountered this equation.\n\n\n\\(y=ax+b\\).\n\n\n\nIn machine learning, the linear model equation is usually represented as a weighted sum of input variables. Note that the only changes are that we refer to the free parameters as weights (\\(w_n\\)) instead of \\(a\\) and \\(b\\) (to emphasize these are the weights the model learns) and the ability to add more than one input (\\(x_1, x_2, ...x_n\\) instead of just \\(x\\)):\n\n\n\\(y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\\)\n\n\nIn statistics, the linear model equation is also represented as a weighted sum of input variables, except we call the weights “regression coefficients” (\\(\\beta_n\\)) and we add an error term to account for unexplained variability (\\(\\varepsilon\\)):\n\n\n\\(y = β_0 + β_1x_1 + β_2x_2 + ... + β_nx_n + ε\\)\n\n\nIn matrix notation, the linear model equation is represented as a dot product of vectors. This is just a more compact representation of the statistics (or machine learning) way, often used in linear algebra and statistics. \\(X\\) is the matrix containing the values of the explanatory variables, \\(β\\) is the vector of regression coefficients, and \\(\\varepsilon\\) is the vector of error terms.\n\n\n\\(y = Xβ + ε\\)"
  },
  {
    "objectID": "notes/model-specification.html#model-terms",
    "href": "notes/model-specification.html#model-terms",
    "title": "Model specification",
    "section": "8 Model terms",
    "text": "8 Model terms\nWe’ve specified our response and explanatory variables and the functional form of our model. Now we need to specify the model terms. Model terms describe how to include the explanatory variables in our model (they can be included in more than one way!) There are four kinds of terms: (1) intercept, (2) main, (3) interaction, and (4) transformation.\n\nThe intercept term, \\(\\beta_0\\), is a constant (not variable) capturing the typical value of the response variable when all explanatory variables are zero. It allows the model to have an offset from the origin, so it is also called the “offset” or “gain” parameter in some fields. Unless it makes sense for our response variable to be zero when all other variables are zero (it rarely does!) we should include the intercept term.\n\n\nin R: y ~ 1\nin eq: \\(y=\\beta_0 + \\varepsilon\\)\n\n\nMain terms (AKA main effects) represent the effect of each explanatory variable on the response variable directly. In other words, how does the response variable change as a result of changes in a given explanatory variable, when all other explanatory variables are zero? Each main term corresponds to one explanatory variable and is included in the model as a single term (\\(\\beta_nx_n\\)). We can add as many explanatory variables as we like to the model:\n\n\nin R: y ~ 1 + year + gender\nin eq: \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\n\nNote that can include categorical explanatory variables like gender, we just need to find a way to represent the same information numerically, since linear models require numerical inputs.\nInteraction terms allow us to express that the effect of one explanatory variable on the response variable is different at different values of another explanatory variable. For example, in the swim records data, the effect of gender on record times changes over year (or said another way, the effect of year on record times is different for men and women). We are still describing how variation in the response variable is explained by one or more explanatory variables, we’re just describing how two (or more) variables combine to influence the response. In the linear model equation, we add a term to the model in which we multiply the values of the interacting variables.\n\nin R: y ~ 1 + year + gender + year:gender\n\nor the short way: y ~ 1 + year * gender\n\nin eq: \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2\\)\n\nTransformation terms allow us to modify the explanatory variables to accommodate nonlinear relationships with the response variable. Some of the most common transformations are \\(x^2\\), \\(\\sqrt{x}\\), and \\(log(x)\\). Note that \\(x\\) must be a quantitative variable: we can’t transform categorical variables. In the swim records example, squaring the year term (\\(x_1^2\\)) allowed our model to have a curve shape. But notice that this makes it seem like record times are slowing down after 1990. This is obviously not the case — records inherently only get faster! — but models lack common sense, and there is no easy math way to tell our model to “be curvy, but also never slope upward”. - in R: y ~ sq(year) * gender - eq: \\(y = \\beta_0 + \\beta_1x_1^2 + \\beta_2x_2 + \\beta_3x_1^2x_2\\)\n\nFurther reading\n\nCh 6: Language of models in Statistical Modeling"
  },
  {
    "objectID": "notes/r-basics-webr.html",
    "href": "notes/r-basics-webr.html",
    "title": "R Basics",
    "section": "",
    "text": "Defining some basic concepts:\n\nExpressions are combination of values, variables, operators, and functions that can be evaluated to produce a result. Expressions can be as simple as a single value or more complex involving calculations, comparisons, and function calls. They are the fundamental building blocks of programming.\n\n10 - a simple value expression that evaluates to 10.\nx + 10 - an expression that adds the value of x to 10\na &lt;- x + 10 - an expression that adds the value of x to 10 and assigns the result to the variable a\n\nLoading\n  webR...\n\n\n  \n\n\nObjects in R allow us to store various types of data, such as numbers, text, vectors, matrices; and more complex structures like functions and data frames. Objects are created by assigning values to variable names with the assignment operator, &lt;-. For example, in x &lt;- 10, x is an object assigned to the value 10.\nNames that we assign to objects must include only letters, numbers, ., or _. Names must start with a letter (or . if not followed by a number).\nAttributes allow you to attach arbirary metadata to an object. For example, adding a dim (dimension) attribute to a vector allows it to behave like a matrix or n dimensional array.\nFunctions (or commands) are reusable pieces of code that take some input, preform some task or computation, and return an output. Many functions are built-in to base R (see below!), others can be part of packages or even defined by you. Functions are objects!\nEnvironment is the collection of all the objects (functions, variables etc.) we defined in the current R session.\nPackages are collections of functions, data, and documentation bundled together in R. They enhance R’s capabilities by introducing new functions and specialized data structures. Packages need to be installed and loaded before you can use their functions or data.\nComments are notes you leave to yourself (within code blocks in colab) to document your code; comments are not evaluated.\nMessages are notes R leaves for you, after you run your code. Messages can be simply for-your-information, warnings that something unexpected might happen, or erros if R cannot evaluate your code.\n\nWays to get help when coding in R:\n\nRead packages docs - packages usually come with extensive documentation and examples. Reading the docs is one of the best ways to figure things out. Here is an example from the dplyr package.\nRead error messages - read any error messages you receive while coding — they give clues about what is going wrong!\nAsk R - Use R’s built-in functions to get help as you code\nAsk on Ed- ask questions on our class discussion board!\nAsk Google/Stack Overflow - It is a normal and important skill (not cheating) to google things while coding and learning to code! Use keywords and package names to ensure your solutions are course-relevant.\nAsk ChatGPT - You can similarly use ChatGPT or other LLMs as a resource. But keep in mind they may provide a solution that is wrong or not relevant to what we are learning in this course."
  },
  {
    "objectID": "notes/r-basics-webr.html#basics",
    "href": "notes/r-basics-webr.html#basics",
    "title": "R Basics",
    "section": "",
    "text": "Defining some basic concepts:\n\nExpressions are combination of values, variables, operators, and functions that can be evaluated to produce a result. Expressions can be as simple as a single value or more complex involving calculations, comparisons, and function calls. They are the fundamental building blocks of programming.\n\n10 - a simple value expression that evaluates to 10.\nx + 10 - an expression that adds the value of x to 10\na &lt;- x + 10 - an expression that adds the value of x to 10 and assigns the result to the variable a\n\nLoading\n  webR...\n\n\n  \n\n\nObjects in R allow us to store various types of data, such as numbers, text, vectors, matrices; and more complex structures like functions and data frames. Objects are created by assigning values to variable names with the assignment operator, &lt;-. For example, in x &lt;- 10, x is an object assigned to the value 10.\nNames that we assign to objects must include only letters, numbers, ., or _. Names must start with a letter (or . if not followed by a number).\nAttributes allow you to attach arbirary metadata to an object. For example, adding a dim (dimension) attribute to a vector allows it to behave like a matrix or n dimensional array.\nFunctions (or commands) are reusable pieces of code that take some input, preform some task or computation, and return an output. Many functions are built-in to base R (see below!), others can be part of packages or even defined by you. Functions are objects!\nEnvironment is the collection of all the objects (functions, variables etc.) we defined in the current R session.\nPackages are collections of functions, data, and documentation bundled together in R. They enhance R’s capabilities by introducing new functions and specialized data structures. Packages need to be installed and loaded before you can use their functions or data.\nComments are notes you leave to yourself (within code blocks in colab) to document your code; comments are not evaluated.\nMessages are notes R leaves for you, after you run your code. Messages can be simply for-your-information, warnings that something unexpected might happen, or erros if R cannot evaluate your code.\n\nWays to get help when coding in R:\n\nRead packages docs - packages usually come with extensive documentation and examples. Reading the docs is one of the best ways to figure things out. Here is an example from the dplyr package.\nRead error messages - read any error messages you receive while coding — they give clues about what is going wrong!\nAsk R - Use R’s built-in functions to get help as you code\nAsk on Ed- ask questions on our class discussion board!\nAsk Google/Stack Overflow - It is a normal and important skill (not cheating) to google things while coding and learning to code! Use keywords and package names to ensure your solutions are course-relevant.\nAsk ChatGPT - You can similarly use ChatGPT or other LLMs as a resource. But keep in mind they may provide a solution that is wrong or not relevant to what we are learning in this course."
  },
  {
    "objectID": "notes/r-basics-webr.html#important-functions",
    "href": "notes/r-basics-webr.html#important-functions",
    "title": "R Basics",
    "section": "2 Important functions",
    "text": "2 Important functions\nFor objects:\n\nstr(x) - returns summary of object’s structure\ntypeof(x) - returns object’s data type\nlength(x) - returns object’s length\nattributes(x) - returns list of object’s attributes\nis.*(x) - test if object is data type (e.g. is.double(x))\nas.*(x) - coerce object to data type (e.g. as.double(x))\n\nLoading\n  webR...\n\n\n  \n\n\nfor environment:\n\nls() - list all variables in environment\nrm(x) - remove x variable from environment\nrm(list = ls()) - remove all variables from environment\n\nLoading\n  webR...\n\n\n  \n\n\nFor packages:\n\ninstall.packages() to install packages\nlibrary() to load the package into your current R session.\ndata() to load data from package into environment\nsessionInfo() - version information for current R session and packages\n\nLoading\n  webR...\n\n\n  \n\n\nFor help:\n\n?mean - get help with a function\nhelp.search('mean') - search help files for word or phrase\nhelp(package='tidyverse') - find help for a package"
  },
  {
    "objectID": "notes/r-basics-webr.html#vectors",
    "href": "notes/r-basics-webr.html#vectors",
    "title": "R Basics",
    "section": "3 Vectors",
    "text": "3 Vectors\nOne of the must fundamental data structures in R is the vector. There are two types:\n\natomic vector - elements of the same data type\nlist - elements refer to any object (even complex objects or other lists)\n\nAtomic vectors can be one of six data types:\n\ndouble - real numbers, written in decimal (0.1234) or scientific notation (1.23e4)\n\nnumbers are double by default (3 is stored as 3.00)\nthree special doubles: Inf, -Inf, and NaN (not a number)\n\ninteger - integers, numbers followed by L (3L or 1e3L)\ncharacter - strings with single or double quotes (‘hello world!’ or “hello world!”)\nlogical - boolean written (TRUE or FALSE) or abbreviated (T or F)\ncomplex - complex numbers where i is imaginary (5 + 3i)\nraw - stores raw bytes\n\nSome more complex data structures are built from atomic vectors by adding attributes:\n\nmatrix - a vector with a dim attribute representing 2 dimensions\narray - a vector with a dim attribute representing n dimensions\nfactor - an integer vector with two attributes: class=\"factor\" and levels, which defines the set of allowed values (useful for categorical data)\ndate-time - a double vector where the value is the number of seconds since Jan 01, 1970 and a tzone attribute representing the time zone\ndata.frame - a named list of vectors (of equal length) with attributes for names (column names), row.names, and class=\"data.frame\" (used to represent datasets)\n\nTo create atomic vectors:\n\nc(2,4,6) - c for combine, returns 2 4 6\n2:4 - vector of integers, returns 2 3 4\nseq(2, 6, by = 2) - sequence by, returns 2 4 6\n\nLoading\n  webR...\n\n\n  \n\n\nTo create more complex structures:\n\nlist(x=c(1,2,3), y=c('a','b')) - create a list\nmatrix(x, nrow=2, ncol=2) - create a matrix from x with nrow and ncol\narray(x, dim=c(2,3,2)) - create an array from x with dimensions\nfactor(x, levels=unique(x)) - turn a vector into a factor\ndata.frame(x=c(1,2,3), y=c('a','b','c')) - create a data frame\n\nLoading\n  webR...\n\n\n  \n\n\nMissing elements and empty vectors:\n\nNA- used to represent missing or unknown elements in vectors. Note that NA is contageous: expressions including NA usually return NA\nNULL - used to represent an empty or absent vector of arbitrary type. NULL is its own special type and always has length zero and NULL attributes."
  },
  {
    "objectID": "notes/r-basics-webr.html#subsetting",
    "href": "notes/r-basics-webr.html#subsetting",
    "title": "R Basics",
    "section": "4 Subsetting",
    "text": "4 Subsetting\n\nSubsetting is a natural complement to str(). While str() shows you all the pieces of any object (its structure), subsetting allows you to pull out the pieces that you’re interested in. ~ Hadley Wickham, Advanced R\n\nThere are three operators for subsetting objects:\n\n[ - selects multiple elements\n[[ and $ - extracts a single element\n\nThere are six ways to select multiple elements from vectors with [:\n\nx[c(1,2)] - positive integers select elements at specified indexes\nx[-c(1,2)] - negative integers select all but elements at specified indexes\nx[c(\"name\", \"name2\")] select elements by name, if elements are named\nx[] - nothing returns the original object\nx[0] - zero returns a zero-length vector\nx[c(TRUE, TRUE)] - select elements where corresponding logical value is TRUE\n\nThese also apply when selecting multiple elements from higher dimensional objects (matrix, array, data frame), but note that:\n\nindexes for different dimensions are separated by commas [rows, columns, ...]\nomitted dimensions return all values along that dimension\nthe result is simplified to the lowest possible dimensions by default\nthey can also be indexed like a vector (selects columns)\n\nThere are 3 ways to extract a single element from any data structure:\n\n[[2]] - a single positive integer (index)\n[['name']] - a single string\nx$name - the $ operator is a useful shorthand for [['name']]\n\nWhen extracting single elements, note that:\n\n[[ is preferred for atomic vectors for clarity (though[ also works)\n$ does partial matching without warning; use options(warnPartialMatchDollar=TRUE)\nthe behavior for invalid indexes is inconsistent: sometimes you’ll get an error message, and sometimes it will return NULL"
  },
  {
    "objectID": "notes/r-basics-webr.html#operations",
    "href": "notes/r-basics-webr.html#operations",
    "title": "R Basics",
    "section": "5 Operations",
    "text": "5 Operations\nArithmetic operators:\n\n+ - add\n- - subtract\n* - multiply\n/ - divide\n^ - exponent\n\nComparison operators return true or false:\n\na == b - equal to\na != b - not equal to\na &gt; b - greater than\na &lt; b - less than\na &gt;= b - greater than or equal to\na &lt;= b - less than or equal to\n\nLogical operators combine multiple true or false statements:\n\n& - and\n| - or\n! - not\nany() - returns true if any element meets condition\nall() - returns true if all elements meet condition\n%in% - returns true if any element is in the following vector\n\nMost math operations (and many functions) are vectorized in R:\n\nthey can work on entire vectors, without the need for explicit loops or iteration.\nthis a powerful feature that allows you to write cleaner, more efficient code\nTo illustrate, suppose x &lt;- c(1, 2, 3):\n\nx + 100 returns [101 102 103]\nx == 1 returns [TRUE FALSE FALSE]"
  },
  {
    "objectID": "notes/r-basics-webr.html#built-in-functions",
    "href": "notes/r-basics-webr.html#built-in-functions",
    "title": "R Basics",
    "section": "6 Built-in functions",
    "text": "6 Built-in functions\nNote that you do not need to memorize these built-in functions to be successful on quizzes. Use this as a reference.\nFor basic math:\n\nlog(x) - natural log\nexp(x) - exponential\nsqrt(x) - square root\nabs(x) - absolute value\nmax(x) - largest element\nmin(x) - smallest element\nround(x, n) - round to n decimal places\nsignif(x, n) - round to n significant figures\nsum(x) - add all elements\n\nFor stats:\n\nmean(x) - mean\nmedian(x) - median\nsd(x) - standard deviation\nvar(x) - variance\nquantile(x) - percentage quantiles\nrank(x) - rank of elements\ncor(x, y) - correlation\nlm(x ~ y, data=df) - fit a linear model\nglm(x ~ y, data=df) - fit a generalized linear model\nsummary(x) - get more detailed information from a fitted model\naov(x) - analysis of variance\n\nFor vectors:\n\nsort(x) - return sorted vector\ntable(x) - see counts of values in a vector\nrev(x) - return reversed vector\nunique(x) - return unique values in a vector\ndim(x) - transform vector into n-dimensional array\n\nFor matrices:\n\nt(m) - transpose matrix\nm %+% n - matrix multiplication\nsolve(m, n) - find x in m * x = n\n\nFor data frames:\n\nview(df) - see the full data frame\nhead(df) - see the first 6 rows of data frame\nnrow(df) - number of rows in a data frame\nncol(df) - number of columns in a data frame\ndim(df) - number of columns and rows in a data frame\ncbind(df1, df2) - bind columns\nrbind(df1, df2) - bind rows\n\nFor strings:\n\npaste(x, y, sep=' ') - join multiple vectors together\ntoupper(x) - convert to uppercase\ntolower(x) - convert to lowercase\nnchar(x) - number of characters in a string\n\nFor simple plotting:\n\nplot(x) values of x in order\nplot(x, y) - values of x against y\nhist(x) - histogram of x\n\n\nLoading\n  webR..."
  },
  {
    "objectID": "notes/r-basics-webr.html#programming-in-r",
    "href": "notes/r-basics-webr.html#programming-in-r",
    "title": "R Basics",
    "section": "7 Programming in R",
    "text": "7 Programming in R\nWriting functions and handling flow control are important aspects of learning to program in any language. For our purposes, some general conceptual knowledge on these topics is sufficient (see below). Those interested to learn more might enjoy the book Hands-On Programming with R.\n\nFunctions are reusable pieces of code that take some input, perform some task or computation, and return an output.\nfunction(inputs){\n    # do something\n    return(output)\n}\nFlow control refers to managing the order in which expressions are executed in a program:\n\nif…else - if something is true, do this; otherwise do that\nfor loops - repeat code a specific number of times\nwhile loops - repeat code as long as certain conditions are true\nbreak - exit a loop early\nnext - skip to next iteration in a loop"
  },
  {
    "objectID": "notes/r-basics-webr.html#further-reading-and-references",
    "href": "notes/r-basics-webr.html#further-reading-and-references",
    "title": "R Basics",
    "section": "8 Further reading and references",
    "text": "8 Further reading and references\nSuggested further reading:\n\nBase R Cheat Sheet\nGetting Started with Data in R in ModernDive textbook\nR Nuts and Bolts in R Programming for Data Science by Roger Peng\n\nOther references:\n\nMatlab vs. Julia vs. Python from blog post by Toby Driscoll\nVectors in Advanced R by Hadley Wickham\nSubsetting in Advanced R by Hadley Wickham\nA field guide to base R in R for Data Science by Hadley Wickham"
  },
  {
    "objectID": "notes/sampling-variability.html",
    "href": "notes/sampling-variability.html",
    "title": "Sampling variability",
    "section": "",
    "text": "Suppose we measure a single quantity for a single condition: the height of human adults. Let’s get a visual summary of our data with a histogram.\nWe can see that our data follow a roughly Gaussian distribution (unimodal, symmetric), so we can use the mean to describe the central tendency and standard deviation to describe the spread of the values. They are given by the following equations:\n\nmean(\\(x\\)) = \\(\\overline{x} = \\frac{\\sum_{i=i}^{n} x_{i}}{n}\\)\nsd(\\(x\\)) = \\(\\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n-1}}\\)"
  },
  {
    "objectID": "notes/sampling-variability.html#exploring-a-simple-dataset",
    "href": "notes/sampling-variability.html#exploring-a-simple-dataset",
    "title": "Sampling variability",
    "section": "",
    "text": "Suppose we measure a single quantity for a single condition: the height of human adults. Let’s get a visual summary of our data with a histogram.\nWe can see that our data follow a roughly Gaussian distribution (unimodal, symmetric), so we can use the mean to describe the central tendency and standard deviation to describe the spread of the values. They are given by the following equations:\n\nmean(\\(x\\)) = \\(\\overline{x} = \\frac{\\sum_{i=i}^{n} x_{i}}{n}\\)\nsd(\\(x\\)) = \\(\\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n-1}}\\)"
  },
  {
    "objectID": "notes/sampling-variability.html#standard-deviation",
    "href": "notes/sampling-variability.html#standard-deviation",
    "title": "Sampling variability",
    "section": "2 Standard deviation",
    "text": "2 Standard deviation\nMost of us have an intuitive understanding of how the mean is computed: add up all the values and divide by the number of values. But what about the standard deviation? To understand how the standard deviation is computed, it helps to think the mean as a very simple model of our data: one where all cases are the same. Each individual case can be expressed as the model value plus how much the case deviates from the model value (residual, or leftover).\n\nJust as the mean describes a typical value in our data set, we can use the mean square of the residuals to describe the typical variation in our data set. To compute the mean square, we square the residuals (a neat trick that makes them all positive!) and then add them up — known as the sum of squares. Then, we divide by the number of cases, \\(n\\), minus 1. (we’ll see why we use \\(n-1\\) instead of just \\(n\\) later in the lecture).\nLater in the class, we’ll compute the mean square for all sorts of models. Here, when it’s used to describe how far a set of data are from the mean, it goes by the nickname variance. To fix the annoying squared units, we take the square root of the variance and voilà! We have the standard deviation."
  },
  {
    "objectID": "notes/sampling-variability.html#sampling-distribution",
    "href": "notes/sampling-variability.html#sampling-distribution",
    "title": "Sampling variability",
    "section": "3 Sampling distribution",
    "text": "3 Sampling distribution\nWhen measuring some quantity we are usually interested in knowing something about the population (the height of human adults, for example). But in practice we can only observe a small sample of the entire population.\n\nAny statistic we compute from a random sample we’ve collected (technically known as the parameter estimate) will be subject to sampling variability and will differ from that statistic computed on the entire population (technically known as the parameter). In other words, our measurements are noisy, and we need a way to express our uncertainty on the statistic we’ve computed. Quantifying this sampling variability is an important component of statistical inference.\nThe sampling distribution is the probability distribution of the values our parameter estimate can take on. We can construct the sampling distribution by taking a random sample, computing the statistic of interest, and repeating this process many times. The spread of these results indicates how the parameter estimate will vary from different random samples.\nWe can quantify the spread of our results (AKA express our uncertainty on our parameter estimate) using a parametric approach, by computing the standard deviation of our sampling distribution (called standard error!), or using a nonparametric approach, by constructing a confidence interval."
  },
  {
    "objectID": "notes/sampling-variability.html#standard-error-and-confidence-intervals",
    "href": "notes/sampling-variability.html#standard-error-and-confidence-intervals",
    "title": "Sampling variability",
    "section": "4 Standard error and confidence intervals",
    "text": "4 Standard error and confidence intervals\nThe standard deviation of the sampling distribution is known as the standard error. When the statistic of interest is the mean, the standard error is given by the following equation, where \\(\\sigma\\) is the standard deviation of the population and \\(n\\) is the sample size: \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\nIn practice, the standard deviation of the population is unknown, so we use the standard deviation of the sample as an estimate. That is why we use \\(n-1\\) in our mean square calculation. We assume our sample standard deviation is probably underestimating the population, so we “correct” this by dividing by \\(n-1\\) instead of \\(n\\).\nStandard error is considered parametric because we assume a parametric probability distribution (Gaussian) and compute the standard error based on what happens theoretically when we sample that distribution.\nclt? sample size relationship.\n\nWe can also quantify the sampling variability with a confidence interval, which expresses our uncertainty on our parameter estimate via a coverage interval. We can construct any confidence interval, but in ✨science✨ the convention is to choose the 95% coverage interval.\n\nRecall from last lecture that a coverage interval is a nonparametric statistic. The 95% coverage interval are the values between which 95% of the data points fall (the difference between the 2.5 percentile and the 97.5 percentile in our sampling distribution).\nConfidence intervals are closely related to standard error: assuming the sampling distribution is Gaussian (the parametric approach), the 68% confidence interval is +/- 1 standard error and the 95% confidence interval is +/- 2 standard error."
  },
  {
    "objectID": "notes/sampling-variability.html#bootstrapping",
    "href": "notes/sampling-variability.html#bootstrapping",
    "title": "Sampling variability",
    "section": "5 Bootstrapping",
    "text": "5 Bootstrapping\nIdeally, we would construct the sampling distribution by repeating our experiment many times, drawing new random samples from the population each time. But in practice, this is impossible. We are usually constrained — by time, money, access, etc. — such that we can only take one sample.\n\nThis is no problem if we can assume the underlying population distribution is Gaussian: we can just compute the standard error, which relies on the mean and standard deviation of the sample to approximate what would happen if we had sampled from a Gaussian probability distribution (see above!).\nWhat if the underlying distribution is not Gaussian, or we want to drop these parametric assumptions? We can use a technique called bootstrapping.\n\nWith bootstrapping, instead of assuming a parametric probability distribution, we can use the data themselves to approximate the underlying probability distribution. In other words, instead of sampling from the population, we can sample our sample! We’re “pulling ourselves up by our bootstraps”: constructing the sampling distribution from our own data.\n\nThe procedure is very simple. To illustrate, suppose we have a set a data with 100 data points. We generate the bootstrap sampling distribution by drawing the same number of data points (100) with replacement from our data set and compute the parameter estimate — mean, median, whatever — on those points, then we repeat the process many times.\n\nThere are many ways to generate a bootstrap sampling distribution in R. We will use the infer package in this class, which was developed by Hadley Wickham (the tidyverse guy!) and others to simplify aspects of statistical inference in R.\n\nspecify(response=x): choose which variable is the focus of our inference\ngenerate(reps=n, type='bootstrap'): generate n replicates of the data\ncalculate(stat=\"mean\"): statistic to calculate on each sample; what parameter are you trying to estimate?\n\nWe can further use infer to visualize the bootstrap sampling distribution and get a confidence interval around the parameter we estimated.\n\nvisualize(): quick visualization of the distribution\nget_confidence_interval(level=0.95, type=\"percentile\"): computes the confidence interval\nshade_ci(endpoints=c(min, max)): shades the visualization with the computed confidence interval"
  },
  {
    "objectID": "notes/sampling-variability.html#further-reading",
    "href": "notes/sampling-variability.html#further-reading",
    "title": "Sampling variability",
    "section": "6 Further Reading",
    "text": "6 Further Reading"
  },
  {
    "objectID": "rbasics.html",
    "href": "rbasics.html",
    "title": "LING 0700",
    "section": "",
    "text": "2 + 3\n\n5\n\n\n\nlibrary(tidyverse)\n\nERROR: Error in library(tidyverse): there is no package called 'tidyverse'"
  },
  {
    "objectID": "slides/demo.html#data-science",
    "href": "slides/demo.html#data-science",
    "title": "Hello, world!",
    "section": "Data science",
    "text": "Data science\n\nData are descriptions of the world around us, collected through observation and stored on computers. Computers enable us to infer properties of the world from these descriptions. Data science is the discipline of drawing conclusions from data using computation.\n\nComputational and Inferential Thinking: The Foundations of Data Science"
  },
  {
    "objectID": "slides/demo.html#why-r",
    "href": "slides/demo.html#why-r",
    "title": "Hello, world!",
    "section": "Why R?",
    "text": "Why R?"
  },
  {
    "objectID": "slides/demo.html#many-ways-to-use-r",
    "href": "slides/demo.html#many-ways-to-use-r",
    "title": "Hello, world!",
    "section": "Many ways to use R",
    "text": "Many ways to use R\n\nR Studio\nJupyter\nVS Code\nand even simply the command line/terminal\n\n\nWe pick Google colab"
  }
]