---
title: "Sampling variability"
author: "Katie Schuler"
date: 2023-09-20
---

## Exploring a simple dataset

Suppose we measure a single quantity for a single condition: the height of human adults. Let's get a visual summary of our data with a histogram. 


We can see that our data follow a roughly Gaussian distribution (unimodal, symmetric), so we can use the mean to describe the central tendency and standard deviation to describe the spread of the values. They are given by the following equations:

- mean($x$) = $\overline{x} = \frac{\sum_{i=i}^{n} x_{i}}{n}$

- sd($x$) = $\sqrt{\frac{\sum_{i=1}^n (x_i - \overline{x})^2}{n-1}}$

## Standard deviation

Most of us have an intuitive understanding of how the mean is computed: add up all the values and divide by the number of values.  But what about the standard deviation? To understand how the standard deviation is computed, it helps to think the mean as a very simple model of our data: one where all cases are the same. Each individual case can be expressed as the **model value** plus how much the case deviates from the model value (**residual**, or leftover).

- Just as the mean describes a typical value in our data set, we can use the mean square of the residuals to describe the typical variation in our data set. To compute the **mean square**, we square the residuals (a neat trick that makes them all positive!) and then add them up — known as the **sum of squares**. Then, we divide by the number of cases, $n$, minus 1. (we'll see why we use $n-1$ instead of just $n$ later in the lecture).
- Later in the class, we'll compute the *mean square* for all sorts of models. Here, when it's used to describe how far a set of data are from the mean, it goes by the nickname **variance**. To fix the annoying squared units, we take the square root of the variance and voilà! We have the **standard deviation**.


## Sampling distribution

When measuring some quantity we are usually interested in knowing something about the **population** (the height of human adults, for example). But in practice we can only observe a small **sample** of the entire population. 

- Any statistic we compute from a random sample we've collected (technically known as the **parameter estimate**) will be subject to sampling variability and will differ from that statistic computed on the entire population (technically known as the **parameter**). In other words, our measurements are noisy, and we need a way to express our uncertainty on the statistic we've computed. Quantifying this sampling variability is an important component of statistical inference. 
- The **sampling distribution** is the probability distribution of the values our parameter estimate can take on. We can construct the sampling distribution by taking a random sample, computing the statistic of interest, and repeating this process many times.  The spread of these results indicates how the parameter estimate will vary from different random samples.
- We can quantify the spread of our results (AKA express our uncertainty on our parameter estimate) using a parametric approach, by computing the standard deviation of our sampling distribution (called standard error!), or using a nonparametric approach, by constructing a confidence interval.


## Standard error and confidence intervals

The standard deviation of the sampling distribution is known as the **standard error**. When the statistic of interest is the mean, the standard error is given by the following equation, where $\sigma$ is the standard deviation of the population and $n$ is the sample size: $\frac{\sigma}{\sqrt{n}}$

- In practice, the standard deviation of the population is unknown, so we use the standard deviation of the *sample* as an estimate. *That* is why we use $n-1$ in our mean square calculation. We assume our sample standard deviation is probably underestimating the population, so we "correct" this by dividing by $n-1$ instead of $n$.
- Standard error is considered parametric because we assume a parametric probability distribution (Gaussian) and compute the standard error based on what happens theoretically when we sample that distribution.
- clt? sample size relationship. 

We can also quantify the sampling variability with a **confidence interval**, which expresses our uncertainty on our parameter estimate via a coverage interval. We can construct any confidence interval, but in ✨science✨ the convention is to choose the 95% coverage interval.

- Recall from last lecture that a coverage interval is a nonparametric statistic. The 95% coverage interval are the values between which 95% of the data points fall (the difference between the 2.5 percentile and the 97.5 percentile in our sampling distribution).
- Confidence intervals are closely related to standard error: assuming the sampling distribution is Gaussian (the parametric approach), the 68% confidence interval is +/- 1 standard error and the 95% confidence interval is +/- 2 standard error.


## Bootstrapping

Ideally, we would construct the sampling distribution by repeating our experiment many times, drawing new random samples from the population each time. But in practice, this is impossible. We are usually constrained — by time, money, access, etc. — such that we can only take *one* sample. 

- This is no problem if we can assume the underlying population distribution is Gaussian: we can just compute the standard error, which relies on the mean and standard deviation of the sample to approximate what would happen if we *had* sampled from a Gaussian probability distribution (see above!). 
- What if the underlying distribution is not Gaussian, or we want to drop these parametric assumptions? We can use a technique called bootstrapping. 

With **bootstrapping**, instead of assuming a parametric probability distribution, we can use the data themselves to approximate the underlying probability distribution. In other words, instead of sampling from the population, we can sample our sample! We're "pulling ourselves up by our bootstraps": constructing the sampling distribution from our own data.

- The procedure is very simple. To illustrate, suppose we have a set a data with 100 data points. We generate the bootstrap sampling distribution by drawing the same number of data points (100) *with replacement* from our data set and compute the parameter estimate — mean, median, whatever — on those points, then we repeat the process many times. 


There are many ways to generate a bootstrap sampling distribution in R. We will use the [`infer`](https://infer.netlify.app/) package in this class, which was developed by Hadley Wickham (the `tidyverse` guy!) and others to simplify aspects of statistical inference in R. 

  - `specify(response=x)`: choose which variable is the focus of our inference 
  - `generate(reps=n, type='bootstrap')`: generate n replicates of the data 
  - `calculate(stat="mean")`: statistic to calculate on each sample; what parameter are you trying to estimate? 

We can further use `infer` to visualize the bootstrap sampling distribution and get a confidence interval around the parameter we estimated. 

  - `visualize()`: quick visualization of the distribution
  - `get_confidence_interval(level=0.95, type="percentile")`: computes the confidence interval
  - `shade_ci(endpoints=c(min, max))`: shades the visualization with the computed confidence interval


 
## Further Reading
