[
  {
    "objectID": "labs/lab-01.html",
    "href": "labs/lab-01.html",
    "title": "Lab 1: Getting started with R",
    "section": "",
    "text": "To learn to program in R (or any language), you can read about how to do it, and watch someone else do it; but the only way to really learn is to do it yourself. Create some data structures, try some stuff, and see what happens! Here are some practice quiz questions to guide your learning. We will go over the solutions to these in lab."
  },
  {
    "objectID": "labs/lab-01.html#google-colab",
    "href": "labs/lab-01.html#google-colab",
    "title": "Lab 1: Getting started with R",
    "section": "1 Google Colab",
    "text": "1 Google Colab\nTrue or false? We start a new R notebook in Google Colab with File &gt; New notebook\n\n True False\n\nFor problem sets, how will you submit your colab notebook for grading?\n\n File &gt; Download &gt; Download .ipynb, then upload to Gradescope File &gt; Download &gt; Download .ipynb, then upload to Canvas File &gt; Download &gt; Download .py, then upload to Gradescope File &gt; Download &gt; Download .py, then upload to Canvas\n\nWhat version of R is Google Colab running? Hint: use sessionInfo().\n\nWhat is the relationship between R and Google Colab?\n\n R is a programming language. Google Colab is a smaller, specific version of R. R is a programming language. Google Colab is a development environment where you can run R R and Google Colab are both programming languages. R is a paid (proprietary) programming language. Google Colab is a free service to run R."
  },
  {
    "objectID": "labs/lab-01.html#r-basics",
    "href": "labs/lab-01.html#r-basics",
    "title": "Lab 1: Getting started with R",
    "section": "2 R Basics",
    "text": "2 R Basics\nWhich of the following are expressions?\n\n 10 5 + 10 x &lt;- 5 + 10 x &lt;- y + 10 mean(x)\n\nWhich of the following are valid variable names in R?\n\n childAge response_time 1stPlaceWinner 2fast2furious pi\n\nSuppose we open a new colab notebook and run the following code block. What will be returned?\nx &lt;- 1 + 2\ny &lt;- 0 + 3\nls()\n\n 3 x=3 • y=3 'x' • 'y'  mean(c(1,3,5)) • median(c(1,3,5))\n\nWhich of the following will load the emo package into the current environment?\n\n install.packages('emo') library(emo) data(emo) attributes(emo)\n\nWhich of the following occur in the code block below?\n# compute the mean of x and y\nmean(c(x,y))\n\n a message a function a comment an expression"
  },
  {
    "objectID": "labs/lab-01.html#vectors",
    "href": "labs/lab-01.html#vectors",
    "title": "Lab 1: Getting started with R",
    "section": "3 Vectors",
    "text": "3 Vectors\nSuppose we construct a vector with c(1, \"two\", 3, 4, 5, 6) and assign it to x. What will the following code block return?\ntypeof(x)\n\nWhat is the previous question an example of?\n\n attribute addition explicit coercion implicit coercion none of the above\n\nWhat will the following code block return?\nx &lt;- 1:6\ny &lt;- matrix(x, ncols=2, nrows=2)\ntypeof(y)\n\nWhat will the following code block return?\nx &lt;- c()\nlength(x)\n\nGiven the following output from str(x), what will as.logical(x) return?\n\n\n num [1:4] 1 0 1 0\n\n\n\n an error TRUE • FALSE • TRUE • FALSE FALSE FALSE • TRUE • FALSE • TRUE\n\nGiven the following vector, what will as.double(x) return?\nx &lt;- c(\"one\", \"two\", \"three\")\n\n an error 1 • 2 • 3 2 • 4 • 6 'one' • 'two' • 'three'\n\nWhat happens if you add a vector of numbers to a single number in the following expression?\nc(1, 3, 5) + 1\n\n 2 • 3 • 5 Error: length mismatch 1 • 3 • 5 • 1 2 • 4 • 6\n\nWhat happens if you multiply a vector times another vector?\nc(1, 3, 5) * c(10, 100, 1000)\n\n 10 • 300 • 5000 Error: length mismatch 10 • 30 • 50 • 100 • 300 • 500 • 1000 • 3000 • 5000 a 2 x 3 matrix Error: cannot multiply vectors\n\nSuppose we run the following code. What will any(x) return?\nx &lt;- c(1, 5, 11) &gt; 10\n\n TRUE FALSE Error: vector is double but requires logical"
  },
  {
    "objectID": "labs/lab-01.html#subsetting",
    "href": "labs/lab-01.html#subsetting",
    "title": "Lab 1: Getting started with R",
    "section": "4 Subsetting",
    "text": "4 Subsetting\nWhich of the following code subsets the vector x &lt;- c(\"blue\", \"pink\", \"red\") to return just the first element?\n\n x[1] x[[1]] x[0] x[-c(2, 3)] x[“blue”]\n\nSuppose we run the following code. What will x[[2]] return?\nx &lt;- seq(from = 2, to =8, by=2)\n\nSuppose we run the following code. What will m[1, 2] return?\nm &lt;- matrix(c(10,20,30,40), nrow=2, ncol=2)"
  },
  {
    "objectID": "psets/problem-set-01.html",
    "href": "psets/problem-set-01.html",
    "title": "Problem set 1",
    "section": "",
    "text": "Under Construction\n\n\n\nAdditional problems will be added; you can begin work on problems 0-4."
  },
  {
    "objectID": "psets/problem-set-01.html#problem-0",
    "href": "psets/problem-set-01.html#problem-0",
    "title": "Problem set 1",
    "section": "Problem 0",
    "text": "Problem 0\nCreate a new colab R notebook. Add a text block that includes the title “Problem set 1”, your name, and the date. Add a text block with a heading for each problem in the set. As you work through the problem set, include your code/solutions below the appropriate heading."
  },
  {
    "objectID": "psets/problem-set-01.html#problem-1",
    "href": "psets/problem-set-01.html#problem-1",
    "title": "Problem set 1",
    "section": "Problem 1",
    "text": "Problem 1\nSuppose you record how many words your quiet roommate says to you each day for a week. Convert the vector given below from strings to doubles using coercion and store it as words_spoken. Use R’s built-in functions to compute the total number of words your roommate said that week, the average number of words spoken per day, and the maximum number of words spoken on a single day. Store the results in a tibble. Perform a comparison operation on the words_spoken vector to determine whether each day’s words were above the average. Finally, use subsetting to extract the days where the number of words were in the single digits.\nwords_spoken_stringy &lt;- c(\"5\", \"3\", \"20\",\" 0\", \"4\", \"35\", \"1\")"
  },
  {
    "objectID": "psets/problem-set-01.html#problem-2",
    "href": "psets/problem-set-01.html#problem-2",
    "title": "Problem set 1",
    "section": "Problem 2",
    "text": "Problem 2\nCreate the matrix given below. Use one of R’s built-in functions to append a new row to the matrix made up of all 1s. Multiply every number in the new matrix by 5. Then use subsetting to return the second and fourth row of values in the final matrix.\n\n\n     [,1] [,2] [,3]\n[1,]    3    6    9\n[2,]    4    7   10\n[3,]    5    8   11"
  },
  {
    "objectID": "psets/problem-set-01.html#problem-3",
    "href": "psets/problem-set-01.html#problem-3",
    "title": "Problem set 1",
    "section": "Problem 3",
    "text": "Problem 3\nCreate a data frame that looks like the one below. Return the structure of the dataframe with str(). Use subsetting to select all columns that were indicated as doubles. Compute the mean of each column in the subset dataframe using map_*() and convert the output to a tibble (use the pipe!).\n\n\n  age height  major score firstgen\n1  30     65 cogsci   100     TRUE\n2  45     66   ling    75    FALSE\n3  81     72  psych    88     TRUE\n4  27     59   ling    97    FALSE"
  },
  {
    "objectID": "psets/problem-set-01.html#problem-4",
    "href": "psets/problem-set-01.html#problem-4",
    "title": "Problem set 1",
    "section": "Problem 4",
    "text": "Problem 4\nRead the documentation for the emo package. Install and load the package. Then create a character vector including 8 of your favorite emojis by their keyword (see the docs for a list). Finally, use the package’s emo::ji() function in conjunction with map_*() to print all 8 emojis with one line of code. Combine this output with your keyword vector in a tibble."
  },
  {
    "objectID": "psets/problem-set-01.html#problem-5",
    "href": "psets/problem-set-01.html#problem-5",
    "title": "Problem set 1",
    "section": "Problem 5",
    "text": "Problem 5\n\n\n\n\n\n\nUnder Construction\n\n\n\nWill be added on 2023-09-03"
  },
  {
    "objectID": "psets/problem-set-01.html#problem-6",
    "href": "psets/problem-set-01.html#problem-6",
    "title": "Problem set 1",
    "section": "Problem 6",
    "text": "Problem 6\n\n\n\n\n\n\nUnder Construction\n\n\n\nWill be added on 2023-09-03"
  },
  {
    "objectID": "webexercises.html",
    "href": "webexercises.html",
    "title": "Webexercises",
    "section": "",
    "text": "This is a Web Exercise template created by the psychology teaching team at the University of Glasgow, based on ideas from Software Carpentry. This template shows how instructors can easily create interactive web documents that students can use in self-guided learning.\nThe {webexercises} package provides a number of functions that you use in inline R code or through code chunk options to create HTML widgets (text boxes, pull down menus, buttons that reveal hidden content). Examples are given below. Render this file to HTML to see how it works.\nNOTE: To use the widgets in the compiled HTML file, you need to have a JavaScript-enabled browser."
  },
  {
    "objectID": "webexercises.html#example-questions",
    "href": "webexercises.html#example-questions",
    "title": "Webexercises",
    "section": "1 Example Questions",
    "text": "1 Example Questions\n\nFill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 36 is: \n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\n\nMultiple Choice (mcq())\n\n“Never gonna give you up, never gonna: let you goturn you downrun awaylet you down”\n“I bless the rainsguess it rainssense the rain down in Africa” -Toto\n\n\n\nTrue or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). TRUEFALSE\n\n\n\nLonger MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\n 95% of the data fall within this range there is a 95% probability that the true mean lies within this range if you repeated the process many times, 95% of intervals calculated in this way contain the true mean"
  },
  {
    "objectID": "webexercises.html#checked-sections",
    "href": "webexercises.html#checked-sections",
    "title": "Webexercises",
    "section": "2 Checked sections",
    "text": "2 Checked sections\nCreate sections with the class webex-check to add a button that hides feedback until it is pressed. Add the class webex-box to draw a box around the section (or use your own styles).\n\nI am going to learn a lot: TRUEFALSE\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion"
  },
  {
    "objectID": "webexercises.html#hidden-solutions-and-hints",
    "href": "webexercises.html#hidden-solutions-and-hints",
    "title": "Webexercises",
    "section": "3 Hidden solutions and hints",
    "text": "3 Hidden solutions and hints\nYou can fence off a solution area that will be hidden behind a button using hide() before the solution and unhide() after, each as inline R code. Pass the text you want to appear on the button to the hide() function.\nIf the solution is a code chunk, instead of using hide() and unhide(), simply set the webex.hide chunk option to TRUE, or set it to the string you wish to display on the button.\nRecreate the scatterplot below, using the built-in cars dataset.\n\n\n\n\n\n\n\nI need a hint\n\nSee the documentation for plot() (?plot)\n\n\n\n\n\nClick here to see the solution\n\nplot(cars$speed, cars$dist)"
  },
  {
    "objectID": "drafts.html",
    "href": "drafts.html",
    "title": "All drafts",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 29, 2023\n\n\nHello, world!\n\n\nKatie Schuler\n\n\n\n\nAug 31, 2023\n\n\nR basics\n\n\nKatie Schuler, June Choe\n\n\n\n\nSep 5, 2023\n\n\nData importing\n\n\nKatie Schuler\n\n\n\n\nSep 7, 2023\n\n\nData visualization\n\n\nKatie Schuler\n\n\n\n\nSep 12, 2023\n\n\nData wrangling\n\n\nKatie Schuler\n\n\n\n\nSep 12, 2023\n\n\nProbablity distributions\n\n\nKatie Schuler\n\n\n\n\nSep 20, 2023\n\n\nSampling variability\n\n\nKatie Schuler\n\n\n\n\nSep 26, 2023\n\n\nHypothesis testing\n\n\nKatie Schuler\n\n\n\n\nOct 3, 2023\n\n\nModel specification\n\n\nKatie Schuler\n\n\n\n\nOct 5, 2023\n\n\nModel fitting\n\n\nKatie Schuler\n\n\n\n\nOct 19, 2023\n\n\nModel accuracy\n\n\nKatie Schuler\n\n\n\n\nOct 24, 2023\n\n\nModel reliability\n\n\nKatie Schuler\n\n\n\n\nNov 7, 2023\n\n\nClassification\n\n\nKatie Schuler\n\n\n\n\nNov 14, 2023\n\n\nFeature engineering\n\n\nKatie Schuler\n\n\n\n\nNov 18, 2023\n\n\nInference\n\n\nKatie Schuler\n\n\n\n\nDec 5, 2023\n\n\nMixed-effects models\n\n\nKatie Schuler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts.html#lecture-notes",
    "href": "drafts.html#lecture-notes",
    "title": "All drafts",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 29, 2023\n\n\nHello, world!\n\n\nKatie Schuler\n\n\n\n\nAug 31, 2023\n\n\nR basics\n\n\nKatie Schuler, June Choe\n\n\n\n\nSep 5, 2023\n\n\nData importing\n\n\nKatie Schuler\n\n\n\n\nSep 7, 2023\n\n\nData visualization\n\n\nKatie Schuler\n\n\n\n\nSep 12, 2023\n\n\nData wrangling\n\n\nKatie Schuler\n\n\n\n\nSep 12, 2023\n\n\nProbablity distributions\n\n\nKatie Schuler\n\n\n\n\nSep 20, 2023\n\n\nSampling variability\n\n\nKatie Schuler\n\n\n\n\nSep 26, 2023\n\n\nHypothesis testing\n\n\nKatie Schuler\n\n\n\n\nOct 3, 2023\n\n\nModel specification\n\n\nKatie Schuler\n\n\n\n\nOct 5, 2023\n\n\nModel fitting\n\n\nKatie Schuler\n\n\n\n\nOct 19, 2023\n\n\nModel accuracy\n\n\nKatie Schuler\n\n\n\n\nOct 24, 2023\n\n\nModel reliability\n\n\nKatie Schuler\n\n\n\n\nNov 7, 2023\n\n\nClassification\n\n\nKatie Schuler\n\n\n\n\nNov 14, 2023\n\n\nFeature engineering\n\n\nKatie Schuler\n\n\n\n\nNov 18, 2023\n\n\nInference\n\n\nKatie Schuler\n\n\n\n\nDec 5, 2023\n\n\nMixed-effects models\n\n\nKatie Schuler\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts.html#labs",
    "href": "drafts.html#labs",
    "title": "All drafts",
    "section": "Labs",
    "text": "Labs\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 7, 2023\n\n\nLab 2: Import and visualize data\n\n\nKatie Schuler\n\n\n\n\nundefined\n\n\nLab 1: Getting started with R\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts.html#problem-sets",
    "href": "drafts.html#problem-sets",
    "title": "All drafts",
    "section": "Problem sets",
    "text": "Problem sets\n\n\n\n\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nProblem set 1\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rbasics.html",
    "href": "rbasics.html",
    "title": "LING 0700 | PSYC 2314",
    "section": "",
    "text": "2 + 3\n\n5\n\n\n\nlibrary(tidyverse)\n\nERROR: Error in library(tidyverse): there is no package called 'tidyverse'"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Datasci for lang & mind",
    "section": "",
    "text": "contact: katie, june, avinash, ravi\nresources: ed, canvas, gradescope, colab\n\n\n\n\nWeek\nDay\nTopic\nResources\n\n\n\n\n1\n08-28\nHello, world!\nslides\n\n\n1\n08-31\nR-basics\nslides, demo, lab\n\n\n2\n09-05\nData importing\n\n\n\n2\n09-07\nData visualization\n\n\n\n2\n09-10\nProblem set 1\n\n\n\n3\n09-12\nData wrangling\n\n\n\n3\n09-14\nProbability distributions\n\n\n\n4\n09-19\nQuiz 1\n\n\n\n4\n09-20\nSampling variability\n\n\n\n5\n09-26\nHypothesis testing\n\n\n\n5\n09-28\nR tutorial\n\n\n\n5\n10-01\nProblem set 2\n\n\n\n6\n10-03\nModel specification\n\n\n\n6\n10-05\nModel fitting\n\n\n\n7\n10-10\nQuiz 2\n\n\n\n7\n10-12\nNo class, break\n\n\n\n8\n10-17\nR tutorial\n\n\n\n8\n10-19\nModel accuracy\n\n\n\n8\n10-22\nProblem set 3\n\n\n\n9\n10-24\nModel reliability\n\n\n\n9\n10-26\nR tutorial\n\n\n\n10\n10-31\nQuiz 3\n\n\n\n10\n11-02\nTBD guest\n\n\n\n11\n11-07\nClassification\n\n\n\n11\n11-09\nR tutorial\n\n\n\n11\n11-12\nProblem set 4\n\n\n\n12\n11-14\nInference\n\n\n\n12\n11-16\nR tutorial\n\n\n\n13\n11-21\nQuiz 4\n\n\n\n13\n11-23\nNo class, break\n\n\n\n14\n11-28\nFeature engineering\n\n\n\n14\n11-30\nR tutorial\n\n\n\n14\n12-03\nProblem set 5\n\n\n\n15\n12-05\nMultilevel models\n\n\n\n15\n12-07\nR tutorial\n\n\n\n15\n12-10\nProblem set 6"
  },
  {
    "objectID": "slides/hello-world.html#data-science",
    "href": "slides/hello-world.html#data-science",
    "title": "Hello, world!",
    "section": "Data science",
    "text": "Data science\nData science is about making decisions based on incomplete information.\n\n\n\n\nFigure 1: from Kok & de Lange (2014)\n\n\nThis concept is not new. Brains were built for doing this!"
  },
  {
    "objectID": "slides/hello-world.html#but-we-have-new-tools-and-lots-more-data",
    "href": "slides/hello-world.html#but-we-have-new-tools-and-lots-more-data",
    "title": "Hello, world!",
    "section": "But we have new tools and lots more data!",
    "text": "But we have new tools and lots more data!\n\n\n\n\nFigure 2: from https://www.domo.com/data-never-sleeps."
  },
  {
    "objectID": "slides/hello-world.html#data-science-workflow",
    "href": "slides/hello-world.html#data-science-workflow",
    "title": "Hello, world!",
    "section": "Data science workflow",
    "text": "Data science workflow\nThe folks who wrote R for Data Science proposed the following data science workflow:\n\nFigure 3: from R for Data Science"
  },
  {
    "objectID": "slides/hello-world.html#overview-of-course",
    "href": "slides/hello-world.html#overview-of-course",
    "title": "Hello, world!",
    "section": "Overview of course",
    "text": "Overview of course\nWe will spend the first few weeks getting comfortable programming in R, including some useful skills for data science:\n\nR basics\nData importing\nData visualization\nData wrangling"
  },
  {
    "objectID": "slides/hello-world.html#overview-of-course-1",
    "href": "slides/hello-world.html#overview-of-course-1",
    "title": "Hello, world!",
    "section": "Overview of course",
    "text": "Overview of course\nThen, we will spend the next several weeks building a foundation in basic statistics and model building:\n\nProbability distributions\nSampling variability\nHypothesis testing\nModel specification\nModel fitting\nModel accuracy\nModel reliability"
  },
  {
    "objectID": "slides/hello-world.html#overview-of-course-2",
    "href": "slides/hello-world.html#overview-of-course-2",
    "title": "Hello, world!",
    "section": "Overview of course",
    "text": "Overview of course\nFinally we will cover a selection of more advanced topics that are often applied in language and mind fields, with a focus on basic understanding:\n\nClassification\nFeature engineering (preprocessing)\nInference for regression\nMixed-effect models"
  },
  {
    "objectID": "slides/hello-world.html#syllabus-briefly",
    "href": "slides/hello-world.html#syllabus-briefly",
    "title": "Hello, world!",
    "section": "Syllabus, briefly",
    "text": "Syllabus, briefly\nEach week will include two lectures and a lab:\n\nLectures are on Tuesdays and Thursdays at 10:15am and will be a mix of conceptual overviews and R tutorials. It is a good idea to bring your laptop so you can follow along and try stuff in R!\nLab is on Thursday or Friday and will consist of (ungraded) practice problems and concept review with TAs. You may attend any lab section that works for your schedule."
  },
  {
    "objectID": "slides/hello-world.html#syllabus-briefly-1",
    "href": "slides/hello-world.html#syllabus-briefly-1",
    "title": "Hello, world!",
    "section": "Syllabus, briefly",
    "text": "Syllabus, briefly\nThere are 10 graded assessments:\n\n6 Problem sets in which you will be asked to apply your newly aquired R programming skills.\n4 Quizzes in which you will be tested on your understanding of lecture concepts."
  },
  {
    "objectID": "slides/hello-world.html#syllabus-briefly-2",
    "href": "slides/hello-world.html#syllabus-briefly-2",
    "title": "Hello, world!",
    "section": "Syllabus, briefly",
    "text": "Syllabus, briefly\nThere are a few policies to take note of:\n\nMissed quizzes cannot be made up except in cases of genuine conflict or emergency (documentation and course action notice required)\nYou may request an extension on any problem set of up to 3 days. But extensions beyond 3 days will not be granted (because delying solutions will negative impact other students).\nYou may submit any missed quiz or problem set by the end of the semester for half-credit (50%)."
  },
  {
    "objectID": "slides/hello-world.html#why-r",
    "href": "slides/hello-world.html#why-r",
    "title": "Hello, world!",
    "section": "Why R?",
    "text": "Why R?\nWith many programming languages available for data science (e.g. R, Python, Julia, MATLAB), why use R?\n\nBuilt for stats, specifically\nMakes nice visualizations\nLots of people are doing it, especially in academia\nEasier for beginners to understand\nFree and open source (though so are Python and Julia, MATLAB costs $)"
  },
  {
    "objectID": "slides/hello-world.html#many-ways-to-use-r",
    "href": "slides/hello-world.html#many-ways-to-use-r",
    "title": "Hello, world!",
    "section": "Many ways to use R",
    "text": "Many ways to use R\n\nR Studio\nJupyter\nVS Code\nand even simply the command line/terminal"
  },
  {
    "objectID": "slides/hello-world.html#google-colab",
    "href": "slides/hello-world.html#google-colab",
    "title": "Hello, world!",
    "section": "Google Colab",
    "text": "Google Colab\n\nGoogle Colab is a cloud-based Jupyter notebook that allows you to write, execute, and share code like a google doc.\nWe use Google Colab because it’s simple and accessible to everyone. You can start programming right away, no setup required!"
  },
  {
    "objectID": "slides/hello-world.html#secretly-r",
    "href": "slides/hello-world.html#secretly-r",
    "title": "Hello, world!",
    "section": "Secretly, R!",
    "text": "Secretly, R!\nGoogle Colab officially supports Python, but secretly supports R (and Julia, too!)\n\ncolab (r kernel)\n\n\n\nhttps://kathrynschuler.com/datasci"
  },
  {
    "objectID": "slides/r-basics.html#you-are-here",
    "href": "slides/r-basics.html#you-are-here",
    "title": "R basics",
    "section": "You are here",
    "text": "You are here\n\n\nData science with R\n\n\nHello, world!\nR basics\nData importing\nData visualization\nData wrangling\n\n\n\nStats & Model buidling\n\n\nProbability distributions\nSampling variability\nHypothesis testing\nModel specification\nModel fitting\nModel accuracy\nModel reliability\n\n\n\nMore advanced\n\n\nClassification\nFeature engineering (preprocessing)\nInference for regression\nMixed-effect models"
  },
  {
    "objectID": "slides/r-basics.html#learning-resources",
    "href": "slides/r-basics.html#learning-resources",
    "title": "R basics",
    "section": "Learning resources",
    "text": "Learning resources\n\ncolab notebook (R kernel)\nLecture notes\nLabs"
  },
  {
    "objectID": "slides/r-basics.html#basic-concepts-review",
    "href": "slides/r-basics.html#basic-concepts-review",
    "title": "R basics",
    "section": "Basic concepts (review)",
    "text": "Basic concepts (review)\n\nExpressions: fundamental building blocks of programming\nObjects: allow us to store stuff, created with assignment operator\nNames: names w give objects must be letters, numbers, ., or _\nAttributes: allow us to attach arbitrary metadata to objects\nFunctions: take some input, perform some computation, and return some output\nEnvironment: collection of all objects we defined in current R session\nPackages: collections of functions, data, and documentation bundled together in R\nComments: notes you leave for yourself, not evaluated\nMessages: notes R leaves for you (FYI, warning, error)"
  },
  {
    "objectID": "slides/r-basics.html#important-functions",
    "href": "slides/r-basics.html#important-functions",
    "title": "R basics",
    "section": "Important functions",
    "text": "Important functions\nObjects\n\nstr(x) - returns summary of object’s structure\ntypeof(x) - returns object’s data type\nlength(x) - returns object’s length\nattributes(x) - returns list of object’s attributes"
  },
  {
    "objectID": "slides/r-basics.html#important-functions-1",
    "href": "slides/r-basics.html#important-functions-1",
    "title": "R basics",
    "section": "Important functions",
    "text": "Important functions\nEnvironment\n\nls() - list all variables in environment\nrm(x) - remove x variable from environment\nrm(list = ls()) - remove all variables from environment"
  },
  {
    "objectID": "slides/r-basics.html#important-function",
    "href": "slides/r-basics.html#important-function",
    "title": "R basics",
    "section": "Important function",
    "text": "Important function\nPackages\n\ninstall.packages() to install packages\nlibrary() to load package into current R session.\ndata() to load data from package into environment\nsessionInfo() - version info, packages for current R session"
  },
  {
    "objectID": "slides/r-basics.html#important-functions-2",
    "href": "slides/r-basics.html#important-functions-2",
    "title": "R basics",
    "section": "Important functions",
    "text": "Important functions\nHelp\n\n?mean - get help with a function\nhelp('mean') - search help files for word or phrase\nhelp(package='tidyverse') - find help for a package"
  },
  {
    "objectID": "slides/r-basics.html#vectors-1",
    "href": "slides/r-basics.html#vectors-1",
    "title": "R basics",
    "section": "Vectors",
    "text": "Vectors\nare fundamental data structures in R. There are two types:\n\natomic vectors - elements of the same data type\nlists - elements refer to any object"
  },
  {
    "objectID": "slides/r-basics.html#atomic-vectors",
    "href": "slides/r-basics.html#atomic-vectors",
    "title": "R basics",
    "section": "Atomic vectors",
    "text": "Atomic vectors\nAtomic vectors can be one of six data types:\n\n\n\ntypeof(x)\nexamples\n\n\n\n\ndouble\n3, 3.32\n\n\ninteger\n1L, 144L\n\n\ncharacter\n“hello”, ‘hello, world!’\n\n\nlogical\nTRUE, F\n\n\n\n\natomic because they must contain only one type"
  },
  {
    "objectID": "slides/r-basics.html#atomic-vectors-1",
    "href": "slides/r-basics.html#atomic-vectors-1",
    "title": "R basics",
    "section": "Atomic vectors",
    "text": "Atomic vectors\n\n\ndouble\n\ntypeof(3.34)\n\n[1] \"double\"\n\n\ninteger\n\ntypeof(3L)\n\n[1] \"integer\"\n\n\n\ncharacter\n\ntypeof('hello, world!')\n\n[1] \"character\"\n\n\nlogical\n\ntypeof(TRUE)\n\n[1] \"logical\""
  },
  {
    "objectID": "slides/r-basics.html#create-a-vector",
    "href": "slides/r-basics.html#create-a-vector",
    "title": "R basics",
    "section": "Create a vector",
    "text": "Create a vector\nwith c() for concatenate\n\nc(2,4,6)\n\n[1] 2 4 6\n\nc(\"hello\", \"world\", \"!\")\n\n[1] \"hello\" \"world\" \"!\"    \n\nc(T, F, T)\n\n[1]  TRUE FALSE  TRUE\n\nc(\"hello\", c(2, 3))\n\n[1] \"hello\" \"2\"     \"3\""
  },
  {
    "objectID": "slides/r-basics.html#create-a-vector-1",
    "href": "slides/r-basics.html#create-a-vector-1",
    "title": "R basics",
    "section": "Create a vector",
    "text": "Create a vector\nwith sequences seq() or repetitions rep()\n\n# sequence of integers have a special shorthand\n6:10\n\n[1]  6  7  8  9 10\n\n\n\n\n# sequence from, to, by \nseq(from=3, to=5, by=0.5)\n\n[1] 3.0 3.5 4.0 4.5 5.0\n\n\n\n\n\n# rep(x, times = 1, each = 1)\nrep(c(1,0), times = 4)\n\n[1] 1 0 1 0 1 0 1 0\n\n\n\n\n\n# rep(x, times = 1, each = 1)\nrep(c(1,0), each = 4)\n\n[1] 1 1 1 1 0 0 0 0"
  },
  {
    "objectID": "slides/r-basics.html#check-data-type",
    "href": "slides/r-basics.html#check-data-type",
    "title": "R basics",
    "section": "Check data type",
    "text": "Check data type\nwith typeof(x) - returns the type of vector x\n\ntypeof(3)\n\n[1] \"double\"\n\ntypeof(3L)\n\n[1] \"integer\"\n\ntypeof(\"three\")\n\n[1] \"character\"\n\ntypeof(TRUE)\n\n[1] \"logical\""
  },
  {
    "objectID": "slides/r-basics.html#check-data-type-1",
    "href": "slides/r-basics.html#check-data-type-1",
    "title": "R basics",
    "section": "Check data type",
    "text": "Check data type\nwith is.*(x) - returns TRUE if x has type *\n\nis.double(3)\n\n[1] TRUE\n\nis.integer(3L)\n\n[1] TRUE\n\nis.character(\"three\")\n\n[1] TRUE\n\nis.logical(TRUE)\n\n[1] TRUE"
  },
  {
    "objectID": "slides/r-basics.html#coercion-implicit",
    "href": "slides/r-basics.html#coercion-implicit",
    "title": "R basics",
    "section": "Coercion, implicit",
    "text": "Coercion, implicit\nIf you try to include elements of different types, R will coerce them into the same type without warning (implicit coercion)\n\nx &lt;- c(1, 2, \"three\", 4, 5 )\nx\n\n[1] \"1\"     \"2\"     \"three\" \"4\"     \"5\"    \n\ntypeof(x)\n\n[1] \"character\""
  },
  {
    "objectID": "slides/r-basics.html#coercion-explicit",
    "href": "slides/r-basics.html#coercion-explicit",
    "title": "R basics",
    "section": "Coercion, explicit",
    "text": "Coercion, explicit\nYou can also use explict coercion to change a vector to another data type with as.*()\n\nx &lt;- c(1, 0 , 1, 0)\nas.logical(x)\n\n[1]  TRUE FALSE  TRUE FALSE\n\n\n\n\n\n\nhierarchy: character &gt; double &gt; integer &gt; logical"
  },
  {
    "objectID": "slides/r-basics.html#more-complex-structures-1",
    "href": "slides/r-basics.html#more-complex-structures-1",
    "title": "R basics",
    "section": "More complex structures",
    "text": "More complex structures\nSome more complex data structures are built from atomic vectors by adding attributes:\n\n\n\n\n\n\n\nStructure\nDescription\n\n\n\n\nmatrix\nvector with dim attribute representing 2 dimensions\n\n\narray\nvector with dim attribute representing n dimensions\n\n\ndata.frame\na named list of vectors (of equal length) with attributes for names (column names), row.names, and class=\"data.frame\""
  },
  {
    "objectID": "slides/r-basics.html#create-more-complex-structures",
    "href": "slides/r-basics.html#create-more-complex-structures",
    "title": "R basics",
    "section": "Create more complex structures",
    "text": "Create more complex structures\n\n\nmatrix\n\nmatrix(0, nrow=2, ncol=3)\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n\n\ndata.frame\n\ndata.frame(x=c(1,2,3), y=c('a','b','c'))\n\n  x y\n1 1 a\n2 2 b\n3 3 c\n\n\n\narray\n\narray(0, dim=c(2,3,2))\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0"
  },
  {
    "objectID": "slides/r-basics.html#basic-math-operators",
    "href": "slides/r-basics.html#basic-math-operators",
    "title": "R basics",
    "section": "Basic math operators",
    "text": "Basic math operators\n\n\n\nOperator\nOperation\n\n\n\n\n()\nParentheses\n\n\n^\nExponent\n\n\n*\nMultiply\n\n\n/\nDivide\n\n\n+\nAdd\n\n\n-\nSubtract"
  },
  {
    "objectID": "slides/r-basics.html#basic-math-operations",
    "href": "slides/r-basics.html#basic-math-operations",
    "title": "R basics",
    "section": "Basic math operations",
    "text": "Basic math operations\nfollow the order of operations you expect (PEMDAS)\n\n# multiplication takes precedence\n2 + 3 * 10\n\n[1] 32\n\n# we can use paratheses to be explicit\n(2 + 3) * 10 \n\n[1] 50"
  },
  {
    "objectID": "slides/r-basics.html#comparison-operators",
    "href": "slides/r-basics.html#comparison-operators",
    "title": "R basics",
    "section": "Comparison operators",
    "text": "Comparison operators\n\n\n\nOperator\nComparison\n\n\n\n\nx &lt; y\nless than\n\n\nx &gt; y\ngreater than\n\n\nx &lt;= y\nless than or equal to\n\n\nx &gt;= y\ngreater than or equal to\n\n\nx != y\nnot equal to\n\n\nx == y\nequal to"
  },
  {
    "objectID": "slides/r-basics.html#comparison-operators-1",
    "href": "slides/r-basics.html#comparison-operators-1",
    "title": "R basics",
    "section": "Comparison operators",
    "text": "Comparison operators\n\nx &lt;- 2\ny &lt;- 3\n\n\n\n\nx &lt; y\n\n[1] TRUE\n\nx &gt; y \n\n[1] FALSE\n\nx != y\n\n[1] TRUE\n\nx == y\n\n[1] FALSE"
  },
  {
    "objectID": "slides/r-basics.html#logical-operators",
    "href": "slides/r-basics.html#logical-operators",
    "title": "R basics",
    "section": "Logical operators",
    "text": "Logical operators\n\n\n\nOperator\nOperation\n\n\n\n\nx | y\nor\n\n\nx & y\nand\n\n\n!x\nnot\n\n\nany()\ntrue if any element meets condition\n\n\nall()\ntrue if all elements meet condition\n\n\n%in%\ntrue if any element is in following vector"
  },
  {
    "objectID": "slides/r-basics.html#logical-operators-1",
    "href": "slides/r-basics.html#logical-operators-1",
    "title": "R basics",
    "section": "Logical operators",
    "text": "Logical operators\n\nx &lt;- TRUE\ny &lt;- FALSE\n\n\n\n\nx | y\n\n[1] TRUE\n\nx & y \n\n[1] FALSE\n\n!x \n\n[1] FALSE\n\nany(c(x,y))\n\n[1] TRUE\n\nall(c(x,y))\n\n[1] FALSE"
  },
  {
    "objectID": "slides/r-basics.html#operations-are-vectorized",
    "href": "slides/r-basics.html#operations-are-vectorized",
    "title": "R basics",
    "section": "Operations are vectorized",
    "text": "Operations are vectorized\nAlmost all operations (and many functions) are vectorized\n\n\n\nmath\n\nc(1, 2, 3) + c(4, 5, 6)\n\n[1] 5 7 9\n\nc(1, 2, 3) / c(4, 5, 6)\n\n[1] 0.25 0.40 0.50\n\nc(1, 2, 3) * 10 \n\n[1] 10 20 30\n\nc(1, 2, 30) &gt; 10\n\n[1] FALSE FALSE  TRUE\n\n\n\nlogical\n\nx &lt;- c(TRUE, FALSE, FALSE)\ny &lt;- c(TRUE, TRUE, FALSE)\nz &lt;- TRUE\n\n\nx | y\n\n[1]  TRUE  TRUE FALSE\n\nx & y \n\n[1]  TRUE FALSE FALSE\n\nx | z \n\n[1] TRUE TRUE TRUE\n\nx & z \n\n[1]  TRUE FALSE FALSE"
  },
  {
    "objectID": "slides/r-basics.html#operator-coercion",
    "href": "slides/r-basics.html#operator-coercion",
    "title": "R basics",
    "section": "Operator coercion",
    "text": "Operator coercion\nOperators and functions will also coerce values when needed (and without warning)\n\n5.6 + 2L\n\n[1] 7.6\n\n10 + FALSE \n\n[1] 10\n\nlog(1)\n\n[1] 0\n\nlog(TRUE)\n\n[1] 0"
  },
  {
    "objectID": "slides/r-basics.html#subsetting-1",
    "href": "slides/r-basics.html#subsetting-1",
    "title": "R basics",
    "section": "Subsetting",
    "text": "Subsetting\n\nSubsetting is a natural complement to str(). While str() shows you all the pieces of any object (its structure), subsetting allows you to pull out the pieces that you’re interested in. ~ Hadley Wickham, Advanced R\n\nstr()\n\nx &lt;- c(\"hello\", \"world\", \"!\")\nstr(x)\n\n chr [1:3] \"hello\" \"world\" \"!\"\n\ny &lt;- c(1, 2, 3, 4, 5)\nstr(y)\n\n num [1:5] 1 2 3 4 5"
  },
  {
    "objectID": "slides/r-basics.html#subsetting-2",
    "href": "slides/r-basics.html#subsetting-2",
    "title": "R basics",
    "section": "Subsetting",
    "text": "Subsetting\nThere are three operators for subsetting objects:\n\n[ - subsets (one or more) elements\n[[ and $ - extracts a single element"
  },
  {
    "objectID": "slides/r-basics.html#subset-multiple-elements-with",
    "href": "slides/r-basics.html#subset-multiple-elements-with",
    "title": "R basics",
    "section": "Subset multiple elements with [",
    "text": "Subset multiple elements with [\n\n\n\n\n\n\n\nCode\nReturns\n\n\n\n\nx[c(1,2)]\npositive integers select elements at specified indexes\n\n\nx[-c(1,2)]\nnegative integers select all but elements at specified indexes\n\n\nx[c(\"x\", \"y\")]\nselect elements by name, if elements are named\n\n\nx[]\nnothing returns the original object\n\n\nx[0]\nzero returns a zero-length vector\n\n\nx[c(TRUE, TRUE)]\nselect elements where corresponding logical value is TRUE"
  },
  {
    "objectID": "slides/r-basics.html#subset-multiple-elements-with-1",
    "href": "slides/r-basics.html#subset-multiple-elements-with-1",
    "title": "R basics",
    "section": "Subset multiple elements with [",
    "text": "Subset multiple elements with [\n\n\natomic vector\n\nx &lt;- c(\"hello\", \"world\", \"1\")\n\n\nx[c(1,2)]\n\n[1] \"hello\" \"world\"\n\nx[-c(1,2)]\n\n[1] \"1\"\n\nx[]\n\n[1] \"hello\" \"world\" \"1\"    \n\n\n\ndata.frame\n\ny &lt;- data.frame(\n        this = c(1, 2,3), \n        that = c(\"a\", \"b\", \"c\"),\n        theother = c(4, 5, 6)\n        )\n\n\ny[c(1,2)]\n\n  this that\n1    1    a\n2    2    b\n3    3    c\n\ny[-c(1,2)]\n\n  theother\n1        4\n2        5\n3        6\n\ny[c(\"this\")]\n\n  this\n1    1\n2    2\n3    3"
  },
  {
    "objectID": "slides/r-basics.html#ways-to-extract-a-single-element",
    "href": "slides/r-basics.html#ways-to-extract-a-single-element",
    "title": "R basics",
    "section": "3 ways to extract a single element",
    "text": "3 ways to extract a single element\n\n\n\nCode\nReturns\n\n\n\n\n[[2]]\na single positive integer (index)\n\n\n[['name']]\na single string\n\n\nx$name\nthe $ operator is a useful shorthand for [['name']]"
  },
  {
    "objectID": "slides/r-basics.html#ways-to-extract-a-single-element-1",
    "href": "slides/r-basics.html#ways-to-extract-a-single-element-1",
    "title": "R basics",
    "section": "3 ways to extract a single element",
    "text": "3 ways to extract a single element\n\n\natomic vector\n\nx &lt;- c(\"hello\", \"world\", \"1\")\n\n\nx[[1]]\n\n[1] \"hello\"\n\nx[[2]]\n\n[1] \"world\"\n\nx[[3]]\n\n[1] \"1\"\n\n\n\ndata.frame\n\ny &lt;- data.frame(\n        this = c(1, 2,3), \n        that = c(\"a\", \"b\", \"c\"),\n        theother = c(4, 5, 6)\n        )\n\n\ny[[1]]\n\n[1] 1 2 3\n\ny[[\"that\"]]\n\n[1] \"a\" \"b\" \"c\"\n\ny$that\n\n[1] \"a\" \"b\" \"c\""
  },
  {
    "objectID": "slides/r-basics.html#r-has-many-built-in-functions",
    "href": "slides/r-basics.html#r-has-many-built-in-functions",
    "title": "R basics",
    "section": "R has many built-in functions",
    "text": "R has many built-in functions\n\nx &lt;- c(1, -2, 3)\n\n\n\n\nSome are vectorized\n\nlog(x)\n\n[1] 0.000000      NaN 1.098612\n\nabs(x)\n\n[1] 1 2 3\n\nround(x, 2)\n\n[1]  1 -2  3\n\n\n\nSome are not\n\nmean(x)\n\n[1] 0.6666667\n\nmax(x)\n\n[1] 3\n\nmin(x)\n\n[1] -2"
  },
  {
    "objectID": "slides/r-basics.html#missing-values",
    "href": "slides/r-basics.html#missing-values",
    "title": "R basics",
    "section": "Missing values",
    "text": "Missing values\n\n\nNA\n\nused to represent missing or unknown elements in vectors\nNote that NA is contageous: expressions including NA usually return NA\nCheck for NA values with is.na()\n\n\nx &lt;- c(1, NA, 3)\nis.na(x)\n\n[1] FALSE  TRUE FALSE\n\nlength(x)\n\n[1] 3\n\nmean(x)\n\n[1] NA\n\n\n\nNULL\n\nused to represent an empty or absent vector of arbitrary type\nNULL is its own special type and always has length zero and NULL attributes\nCheck for NULL values with is.null()\n\n\nx &lt;- c()\nis.null(x)\n\n[1] TRUE\n\nlength(x)\n\n[1] 0\n\nmean(x)\n\n[1] NA"
  },
  {
    "objectID": "slides/r-basics.html#programming",
    "href": "slides/r-basics.html#programming",
    "title": "R basics",
    "section": "Programming",
    "text": "Programming\n\n\nfunctions\nare reusable pieces of code that take some input, perform some task or computation, and return an output\nfunction(inputs){\n    # do something\n    return(output)\n}\n\ncontrol flow\nrefers to managing the order in which expressions are executed in a program\n\nif…else - if something is true, do this; otherwise do that\nfor loops - repeat code a specific number of times\nwhile loops - repeat code as long as certain conditions are true\nbreak - exit a loop early\nnext - skip to next iteration in a loop"
  },
  {
    "objectID": "slides/r-basics.html#notes-on-with-higher-dim-objects",
    "href": "slides/r-basics.html#notes-on-with-higher-dim-objects",
    "title": "R basics",
    "section": "Notes on [ with higher dim objects",
    "text": "Notes on [ with higher dim objects\n\nm &lt;- matrix(1:6, nrow=2, ncol=3)\nm\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n\n\n# separate dimensions by comma \nm[1, 2]\n\n[1] 3\n\n# omitted dim return all from that dim \nm[2, ]\n\n[1] 2 4 6\n\nm[ , 2]\n\n[1] 3 4"
  },
  {
    "objectID": "slides/r-basics.html#notes-on-and",
    "href": "slides/r-basics.html#notes-on-and",
    "title": "R basics",
    "section": "Notes on [[ and $:",
    "text": "Notes on [[ and $:\nboth [[ and [ work for vectors; use [[\n\nx &lt;- c(1, -2, 3)\nx[[1]]\n\n[1] 1\n\nx[1]\n\n[1] 1\n\n\n$ does partial matching without warning\n\ndf &lt;- data.frame(\n        this = c(1, 2,3), \n        that = c(\"a\", \"b\", \"c\"),\n        theother = c(4, 5, 6)\n        )\n\n\ndf[['theo']]\n\nNULL\n\ndf$theo\n\n[1] 4 5 6"
  },
  {
    "objectID": "notes/feature-engineering.html",
    "href": "notes/feature-engineering.html",
    "title": "Feature engineering",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/feature-engineering.html#dummy-coding",
    "href": "notes/feature-engineering.html#dummy-coding",
    "title": "Feature engineering",
    "section": "1 Dummy coding",
    "text": "1 Dummy coding\nIncluding categorical variables in a model:\n\nOne approach is to create “dummy variables”, called dummy coding, to represent the different levels of the categorical variable. Dummy variables are coded as 1 or 0 (binary) to indicate the presence or absence of a specific category or level. R does this by default.\nIn our swim records example, R dummy codes gender by creating a dummy variable male, where 1 means true and 0 means false. We do not also need a female variable because when male is 0, the person must be female. Creating an additional variable gives no new information. The “left out” category is called the reference level. Unless you specify otherwise, R uses whatever is alphabetically first as the reference level.\nIf a categorical variable has more than two levels, we can simply add a column. To illustrate, imagine our gender variable has a third level, “non-binary”. We create our male column as before (1 for true, 0 for false), and add a non-binary column (1 for true, 0 for false). If the swimmer is neither male nor non-binary (0 in both columns), they must be female. We can have as many levels as we like in a categorical variable, but we should be aware that this adds as many terms (minus 1) to the model! Here \\(x_2\\) is the vector for the male variable and \\(x_3\\) is the vector for thenon-binary variable:\n\nin R: y ~ 1 + year + gender\nin eq: \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3\\)"
  },
  {
    "objectID": "notes/data-wrangling.html",
    "href": "notes/data-wrangling.html",
    "title": "Data wrangling",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/data-wrangling.html#why-wrangle-and-tidy",
    "href": "notes/data-wrangling.html#why-wrangle-and-tidy",
    "title": "Data wrangling",
    "section": "1 Why wrangle and tidy?",
    "text": "1 Why wrangle and tidy?"
  },
  {
    "objectID": "notes/data-wrangling.html#data-wrangling-with-dplyr",
    "href": "notes/data-wrangling.html#data-wrangling-with-dplyr",
    "title": "Data wrangling",
    "section": "2 Data wrangling with dplyr",
    "text": "2 Data wrangling with dplyr\nAll dplyr functions (verbs) share a common structure:\n\n1st argument is always a data frame\nSubsequent arguments typically describe which columns to operate on (via their names)\nOutput is always a new data frame\n\nWe can group dplyr functions based on what they operate on:\n\nrows - see section 3 Manipulating rows\ncolumns - see section 4 Manipulating columns\ngroups - see section 5 Grouping and summarizing data frames\ntables - see section 6 Joining data frames\n\nWe can easily combine dplyr functions to solve complex problems:\n\nThe pipe operator, |&gt; takes the output from one function and passes it as input (the first argument) to the next function.\nThere is another version of the pipe, %&gt;%. See the reading on data transformation if you are curious about the difference.\n\nIn lecture, we will demonstrate with a few dplyr functions, but you should feel comfortable reading the docs/resources to use others to solve unique problems."
  },
  {
    "objectID": "notes/data-wrangling.html#manipulating-rows",
    "href": "notes/data-wrangling.html#manipulating-rows",
    "title": "Data wrangling",
    "section": "3 Manipulating rows",
    "text": "3 Manipulating rows\nfilter()\narrange()"
  },
  {
    "objectID": "notes/data-wrangling.html#manipulating-columns",
    "href": "notes/data-wrangling.html#manipulating-columns",
    "title": "Data wrangling",
    "section": "4 Manipulating columns",
    "text": "4 Manipulating columns\nselect()\nmutate()"
  },
  {
    "objectID": "notes/data-wrangling.html#grouping-and-summarizing-data-frames",
    "href": "notes/data-wrangling.html#grouping-and-summarizing-data-frames",
    "title": "Data wrangling",
    "section": "5 Grouping and summarizing data frames",
    "text": "5 Grouping and summarizing data frames"
  },
  {
    "objectID": "notes/data-wrangling.html#joining-data-frames",
    "href": "notes/data-wrangling.html#joining-data-frames",
    "title": "Data wrangling",
    "section": "6 Joining data frames",
    "text": "6 Joining data frames\n\ndon’t cover, just show the functions and say we won’t cover it"
  },
  {
    "objectID": "notes/data-wrangling.html#data-tidying-with-tidyr",
    "href": "notes/data-wrangling.html#data-tidying-with-tidyr",
    "title": "Data wrangling",
    "section": "7 Data tidying with tidyr",
    "text": "7 Data tidying with tidyr\n\nsneak in with case study later; for now just show functions and say we won’t cover it.\n\n\nFurther reading and references\nhttps://r4ds.hadley.nz/data-transform\nhttps://r4ds.hadley.nz/joins"
  },
  {
    "objectID": "notes/hello-world.html",
    "href": "notes/hello-world.html",
    "title": "Hello, world!",
    "section": "",
    "text": "Data are descriptions of the world around us, collected through observation and stored on computers. Computers enable us to infer properties of the world from these descriptions. Data science is the discipline of drawing conclusions from data using computation.\n– Computational and Inferential Thinking: The Foundations of Data Science\n\nData science is about making decisions based on incomplete information. This concept is not new – brains (especially human brains!) love doing this. To illustrate, what do you see in the following image?\n\n\n\nFigure 1: from Kok & de Lange (2014)1\n\n\nDid you see a gray triangle on top of three circles? Most people do! But it turns out that your brain is filling in the gaps, inferring the presence of a triangle from the partial information available in the black circles 2.\nWhile the underlying concept is not new, the computational tools we use are relatively new. And we have a lot more data!\n\n\n\nFigure 2: from https://www.domo.com/data-never-sleeps."
  },
  {
    "objectID": "notes/hello-world.html#what-is-data-science",
    "href": "notes/hello-world.html#what-is-data-science",
    "title": "Hello, world!",
    "section": "",
    "text": "Data are descriptions of the world around us, collected through observation and stored on computers. Computers enable us to infer properties of the world from these descriptions. Data science is the discipline of drawing conclusions from data using computation.\n– Computational and Inferential Thinking: The Foundations of Data Science\n\nData science is about making decisions based on incomplete information. This concept is not new – brains (especially human brains!) love doing this. To illustrate, what do you see in the following image?\n\n\n\nFigure 1: from Kok & de Lange (2014)1\n\n\nDid you see a gray triangle on top of three circles? Most people do! But it turns out that your brain is filling in the gaps, inferring the presence of a triangle from the partial information available in the black circles 2.\nWhile the underlying concept is not new, the computational tools we use are relatively new. And we have a lot more data!\n\n\n\nFigure 2: from https://www.domo.com/data-never-sleeps."
  },
  {
    "objectID": "notes/hello-world.html#data-science-workflow",
    "href": "notes/hello-world.html#data-science-workflow",
    "title": "Hello, world!",
    "section": "2 Data science workflow",
    "text": "2 Data science workflow\nThe folks who wrote R for Data Science proposed the following data science workflow:\n\n\n\nFigure 3: from R for Data Science\n\n\nLet’s unpack what these mean briefly:\n\nImport: gather data from a variety of sources, which can include structured data (like databases and spreadsheets) and unstructured data (like text, images, and videos).\nTidy and Transform: The raw data we import is often messy. Data scientists clean and preprocess the data, which involves removing errors, handling missing values, and transforming data into a suitable format for analysis.\nVisualize (exploratory data analysis) visualize and summarize data to identify patterns, form hypotheses, select appropriate models, and guide further analysis.\nModel: Using statistical methods, machine learning algorithms, and other computational techniques, data scientists build models to understand underlying patterns in the data. Models are tested using validation techniques to ensure their accuracy and reliability. Then data scientists use them to draw meaningful conclusions, like predictions about the future or inferences about populuation.\nCommunicate: Finally, a crucial part of data science is communicating findings clearly and effectively, whatever your purpose (academic, industry, or the public!)\nProgram: Surrounding all these is programming, since the computational tools are what make these possible!\n\nData science has applications in many fields far beyond language and the mind. It allows us to make data-driven decisions, solve complex problems, and uncover hidden insights that might not be apparent through other methods."
  },
  {
    "objectID": "notes/hello-world.html#overview-of-the-course",
    "href": "notes/hello-world.html#overview-of-the-course",
    "title": "Hello, world!",
    "section": "3 Overview of the course",
    "text": "3 Overview of the course\nWe will spend the first few weeks getting comfortable programming in R, including some useful skills for data science:\n\nR basics\nData importing\nData visualization\nData wrangling\n\nThen, we will spend the next several weeks building a foundation in basic statistics and model building:\n\nProbability distributions\nSampling variability\nHypothesis testing\nModel specification\nModel fitting\nModel accuracy\nModel reliability\n\nFinally we will cover a selection of more advanced topics that are often applied in language and mind fields, with a focus on basic understanding:\n\nClassification\nFeature engineering (preprocessing)\nInference for regression\nMixed-effect models"
  },
  {
    "objectID": "notes/hello-world.html#syllabus-briefly",
    "href": "notes/hello-world.html#syllabus-briefly",
    "title": "Hello, world!",
    "section": "4 Syllabus, briefly",
    "text": "4 Syllabus, briefly\nEach week will include two lectures and a lab:\n\nLectures are on Tuesdays and Thursdays at 10:15am and will be a mix of conceptual overviews and R tutorials. It is a good idea to bring your laptop so you can follow along and try stuff in R!\nLab is on Thursday or Friday and will consist of (ungraded) practice problems and concept review with TAs. You may attend any lab section that works for your schedule.\n\nThere are 10 graded assessments:\n\n6 Problem sets in which you will be asked to apply your newly aquired R programming skills.\n4 Quizzes in which you will be tested on your understanding of lecture concepts.\n\nThere are a few policies to take note of:\n\nMissed quizzes cannot be made up except in cases of genuine conflict or emergency (documentation and course action notice required)\nYou may request an extension on any problem set of up to 3 days. But extensions beyond 3 days will not be granted (because delying solutions will negative impact other students).\nYou may submit any missed quiz or problem set by the end of the semester for half-credit (50%)."
  },
  {
    "objectID": "notes/hello-world.html#why-r",
    "href": "notes/hello-world.html#why-r",
    "title": "Hello, world!",
    "section": "5 Why R?",
    "text": "5 Why R?\nWith many programming languages available for data science (e.g. R, Python, Julia, MATLAB), why use R?\n\nBuilt for stats, specifically\nMakes nice visualizations\nLots of people are doing it, especially in academia\nEasier for beginners to understand\nFree and open source (though so are Python and Julia, MATLAB costs $)\n\nIf you are interested, here is a math professor’s take on the differences between Python, Julia, and MATLAB. Note that although they’re optimized for different things, they are all great and the technical skills and conceptual knowledge you gain in this course will transfer to other languages."
  },
  {
    "objectID": "notes/hello-world.html#google-colab",
    "href": "notes/hello-world.html#google-colab",
    "title": "Hello, world!",
    "section": "6 Google Colab",
    "text": "6 Google Colab\nThere are many ways to program with R. Some popular options include:\n\nR Studio\nJupyter\nVS Code\nand even simply the command line/terminal\n\nGoogle Colab is a cloud-based Jupyter notebook that allows you to write, execute, and share code like a google doc. We use Google Colab because it’s simple and accessible to everyone. You can start programming right away, no setup required! Google Colab officially supports Python, but secretly supports R (and Julia, too!)\nNew R notebook:\n\ncolab (r kernel) - use this link to start a new R notebook\nFile &gt; New notebook error, Python! name 'x' is not defined\n\nCell types:\n\n+ Code - write and execute code\n+ Text - write text blocks in markdown\n\nLeft sidebar:\n\nTable of contents - outline from text headings\nFind and replace - find and/or replace\nFiles - upload files to cloud session\n\nFrequently used menu options:\n\nFile &gt; Locate in Drive - where in your Google Drive?\nFile &gt; Save - saves\nFile &gt; Revision history - history of changes you made\nFile &gt; Download &gt; Download .ipynb - used to submit assignments!\nFile &gt; Print - prints\nRuntime &gt; Run all - run all cells\nRuntime &gt; Run before - run all cells before current active cell\nRuntime &gt; Restart and run all - restart runtime, then run all\n\nFrequently used keyboard shortcuts:\n\nCmd/Ctrl+S - save\nCmd/Ctrl+Enter - run focused cell\nCmd/Ctrl+Shift+A - select all cells\nCmd/Ctrl+/ - comment/uncomment selection\nCmd/Ctrl+] - increase indent\nCmd/Ctrl+[ - decrease indent"
  },
  {
    "objectID": "notes/hello-world.html#further-reading-and-references",
    "href": "notes/hello-world.html#further-reading-and-references",
    "title": "Hello, world!",
    "section": "7 Further reading and references",
    "text": "7 Further reading and references\nRecommended reading:\n\nCh 1 Data Science from Computational and Inferential Thinking: The Foundations of Data Science (just skip the bits about using Python, since we are using R)\nCh 1 Introduction in R for Data Science\nCh 1 Software for Modeling in Tidy Modeling with R\n\nOther references:\n\nMatlab vs. Julia vs. Python from blog post by Toby Driscoll"
  },
  {
    "objectID": "notes/hello-world.html#footnotes",
    "href": "notes/hello-world.html#footnotes",
    "title": "Hello, world!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKok, P., & de Lange, F. P. (2014). Shape perception simultaneously up-and downregulates neural activity in the primary visual cortex. Current Biology, 24(13), 1531-1535.↩︎\nhttps://neurosciencenews.com/neuroimaging-visual-processing-fmri-1150/↩︎"
  },
  {
    "objectID": "notes/classification.html",
    "href": "notes/classification.html",
    "title": "Classification",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/probability-distributions.html",
    "href": "notes/probability-distributions.html",
    "title": "Probablity distributions",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/probability-distributions.html#exploring-a-simple-dataset",
    "href": "notes/probability-distributions.html#exploring-a-simple-dataset",
    "title": "Probablity distributions",
    "section": "1 Exploring a simple dataset",
    "text": "1 Exploring a simple dataset\n\n\nSetup code\n# ------ setup for today's lecture notes ----- # \n\n# suppress startup messages at package load\nsuppressPackageStartupMessages(library(tidyverse))\n\n# set the theme for the plots \ntheme_set(theme_classic(base_size=15))\n\n# generate 10000 y values with mean 3 and sd 0.25\ndata &lt;- tibble(\n    y=rnorm(1000, mean=3, sd=0.25)\n) \n\n\nWe begin with the simplest possible dataset: suppose we measure a single quantity y. What can we do with these data?\nWe can create a visual summary of our dataset with a histogram. A histogram plots the distribution of a set of data, which allows us to get a quick visual of the data: formally we have plotted the the frequency distribution (count) of the data, but this also gives a sense of the central tendency and variability in our dataset.\n\n\nCode\nggplot(data=data, aes(x=y)) +\n    geom_histogram(\n        binwidth = 0.25,\n        color=\"black\", fill='lightgray', alpha=0.5\n    )"
  },
  {
    "objectID": "notes/probability-distributions.html#descriptive-statistics",
    "href": "notes/probability-distributions.html#descriptive-statistics",
    "title": "Probablity distributions",
    "section": "2 Descriptive statistics",
    "text": "2 Descriptive statistics\nWe can summarize (or describe) a set of data with descriptive statistics. There are three types of measures:\n\ncentral tendency describes a central or typical value (mean, median, mode)\nvariability describes dispersion or spread of values (variance, standard deviation, IQR)\nfrequency distribution describes how frequently different values occur (count)\n\nR has built-in functions to handle descriptive statistics (we saw these in lecture 1):\n\ndata %&gt;%\n    summarise(\n        n = n(), \n        mean = mean(y),\n        median = median(y),\n        sd = sd(y),\n        iqr_lower = quantile(y, 0.25),\n        iqr_upper = quantile(y, 0.75)\n    ) \n\n# A tibble: 1 × 6\n      n  mean median    sd iqr_lower iqr_upper\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1  1000  2.99   2.98 0.254      2.81      3.17"
  },
  {
    "objectID": "notes/probability-distributions.html#parametric-vs.-nonparametric",
    "href": "notes/probability-distributions.html#parametric-vs.-nonparametric",
    "title": "Probablity distributions",
    "section": "3 Parametric vs. nonparametric",
    "text": "3 Parametric vs. nonparametric\nSome statistics are considered paramteric because they make assumptions about the the distribution of the data (can therefore be computed theoretically from parameters):\n\nThe mean and standard deviation assume the distribution is Gaussian and can therefore be computed via the following equations\nmean \\(\\mu\\)\nsd \\(\\sigma\\)\n\nOther statstics are nonparametric because they make minimal assumptions about the distribution of the data:\n\nmedian is the 50th percentile, the value below which 50% of the data points fall.\ninter-quartile range (IQR) is the difference between the 25th and 75th percentiles (sometimes called the 50% coverage interval because 50% of the data fall in this range).\n\nNote that we can calculate any arbitrary coverage interval. The 95% coverage interval — widely used in the sciences — is the difference between the 2.5 percentile and the 97.5 percentile, including all but 5% of the data."
  },
  {
    "objectID": "notes/probability-distributions.html#probability-distributions",
    "href": "notes/probability-distributions.html#probability-distributions",
    "title": "Probablity distributions",
    "section": "4 Probability distributions",
    "text": "4 Probability distributions\nA probability distribution (aka probability density function) is a mathematical function that describes the probability of observing the different possible values of a variable (or variables). We will focus on univariate distributions in this class — probability distributions of just one random variable — but probability distributions can also be multivariate.\n\nOne of the simplest probability distributions is the uniform distribution, where all possible values of a variable are equally likely. The probability density function for the uniform distribution is given by the following equation with two parameters (the boundaries, min and max):\n\n\\(p(x) = \\frac{1}{max-min}\\)\n\nOne of the most useful probability distributions for our purposes is the Gaussian (or Normal) distribution. The probability density function for the Gaussian distribution is given by the following equation, with the parameters \\(\\mu\\) (mean) and \\(\\sigma\\) (standard deviation):\n\n\\(p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}\\right)\\)\nThe Gaussian distribution assumes that the distribution of a set of data takes a certain form (is unimodal, symmetric, etc).\nWhen values are sampled from a Gaussian distribution, 68% of the values will be within one standard deviation from the mean and 95% within two standard deviations from the mean.\nWhen computing the mean and standard deviation of a set of data, we are fitting a Gaussian distribution to the data."
  },
  {
    "objectID": "notes/probability-distributions.html#probability-distributions-with-r",
    "href": "notes/probability-distributions.html#probability-distributions-with-r",
    "title": "Probablity distributions",
    "section": "5 Probability distributions with R",
    "text": "5 Probability distributions with R\nThe probability distributions we’ve discussed so far are considered “parametric” because they are given by one or more parameters. When we use R’s functions to generate values from these distributions, we provide these parameters as arguments. Base R has four functions we will use to generate values associated with a probability distribution. - dnorm(mean=5, sd=1) returns the height of the probability density function at the given values - pnorm(5, mean=5, sd=1) returns the cumulative density function (the probability that a random number from the distribution will be less than the given values) - qnorm(0.8, mean=5, sd=1) returns the value whose cumulative distribution matches the probability (inverse of p) - rnorm(1000, mean=5, sd=1) returns n random numbers generated from the distribution\nTo use another distribution, change the function’s suffix to the name of the distribution and the parameters to those that define the distribution. For example, to generate n random numbers from a uniform distribution with a min of 1 and a max of 5, run runif(n, min=0, max=1)."
  },
  {
    "objectID": "notes/probability-distributions.html#nonparametric-probability-distributions",
    "href": "notes/probability-distributions.html#nonparametric-probability-distributions",
    "title": "Probablity distributions",
    "section": "6 Nonparametric probability distributions",
    "text": "6 Nonparametric probability distributions\nWhat if the data does not meet the assumptions of the Gaussian distribution? One option is to choose another parametric probability distribution (run help(Distributions) for a full list of available distributions). Another is to use a nonparametric approach, where the probability distribution is not determined by parameters but is instead determined by the data. - A histogram is actually a simple, nonparametric estimate of a probability distribution. To estimate the probability distribution that generated a set of data from a histogram, we modify the scale of the y-axis so that the total area of the bars is equal to 1. - Kernel density estimation (KDE) is another nonparametric method to estimate a probability distribution. KDE is like a smooth histogram, accomplished by placeing a kernel — a tiny Gaussian distribution — at each observed data point and summing across kernels. We can accomplish this in ggplot with the geom_density() geom."
  },
  {
    "objectID": "notes/probability-distributions.html#further-reading-and-references",
    "href": "notes/probability-distributions.html#further-reading-and-references",
    "title": "Probablity distributions",
    "section": "7 Further reading and references",
    "text": "7 Further reading and references\n\nAppendix A: Statistical Background in Modern Dive\nCh 11: Modeling Randomness in Statistical Modeling\n\nhttps://r4ds.hadley.nz/data-visualize#visualizing-distributions"
  },
  {
    "objectID": "notes/model-accuracy.html",
    "href": "notes/model-accuracy.html",
    "title": "Model accuracy",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/model-accuracy.html#model-accuracy-basics",
    "href": "notes/model-accuracy.html#model-accuracy-basics",
    "title": "Model accuracy",
    "section": "1 Model accuracy basics",
    "text": "1 Model accuracy basics\nWe’ve selected a model (model selection) and fit a model to a set of data (model fitting). One question we might want to ask next is how well does this model describe the data (model accuracy)?\n\nWe can visualize our data and the model fit to get a sense of how accurate the model is. But we also want a way to quantify model accuracy – some metric by which to determine whether a model is useful, or how it compares to other models.\nLast week we learned about one metric of model “goodness”, squared error. We could certainly quantify our model accuracy with squared error, but it would be difficult to interpret since it depends on the units of the data.\nToday we’ll learn about another metric, \\(R^2\\) which is easier to interpret and independent of units. \\(R^2\\) quantifies the percentage of variance in our response variable that is explained by our model."
  },
  {
    "objectID": "notes/model-accuracy.html#coefficient-of-determination",
    "href": "notes/model-accuracy.html#coefficient-of-determination",
    "title": "Model accuracy",
    "section": "2 Coefficient of determination",
    "text": "2 Coefficient of determination\nThe coefficient of determination, \\(R^2\\) quantifies the percentage of variance in the response variable that is explained by the model.\nThe equation for variance:\n\n\\(\\frac{\\sum_{i=1}^n (y_i - m_i)^2}{n-1}\\)\nWe take the sum of squares: square the residuals (\\(i^{th}\\) data point minus the \\(i^{th}\\) model value), then divide by the number of cases, \\(n\\), minus 1.\nNotice this is the same equation as standard deviation, we just haven’t done the squaring part, yet.\n\nThe equation for \\(R^2\\) is then:\n\n\\(R^2=100\\times(1-\\frac{unexplained \\; variance}{total \\; variance})\\)\nor \\(R^2=100\\times(1-\\frac{\\sum_{i=1}^n (y_i - m_i)^2}{\\sum_{i=1}^n (y_i - \\overline{y})^2})\\)\n\nLet’s unpack this equation:\n\nThe \\(total \\; variance\\) (denominator) is the sum of squares of the deviations of the data points from their mean: \\(\\sum_{i=1}^n (y_i - \\overline{y})^2\\). In words, take each y value and subtract it from the mean y value, square it, then add them all up.\nThe \\(unexplained \\; variance\\) (numerator) is the sum of squares of the deviations of the model value from the data (residuals): \\(\\sum_{i=1}^n (y_i - m_i)^2\\). In words, take each y value and subtract it from the model value (the model’s prediction) for that data point, square it, then add them all up.\nNote that we do not include the denominator of the variance equation, \\(n-1\\), since the two would cancel each other out in the full \\(R^2\\) equation.\n\nWe subtract the proportion \\(\\frac{unexplained \\; variance}{total \\; variance}\\) from 1 to get the proportion of variance that is explained, and then we multiply by 100 to turn it into the percent of variance explained.\n\nThere is an upper bound of 100%: the situation where the model explains all the variance (it matches the data exactly)\nThere is technically no lower bound, since models can be arbitrarily bad. 0% indicates the model explains none of the variance (it predicts the mean of the data but nothing else)"
  },
  {
    "objectID": "notes/model-accuracy.html#r2-overestimates-model-accuracy",
    "href": "notes/model-accuracy.html#r2-overestimates-model-accuracy",
    "title": "Model accuracy",
    "section": "3 \\(R^2\\) overestimates model accuracy",
    "text": "3 \\(R^2\\) overestimates model accuracy\nOne thing we can ask is how well the model describes our specific sample of data. But the question we actually want to answer is how well does the model we fit describe the population we are interested in.\n\nThe problem is that we usually only have access to the sample we’ve collected and \\(R^2\\) tends to overestimate the accuracy of the model on the population. In other words, the \\(R^2\\) of the model we fit on our sample will be larger than the \\(R^2\\) of the model fit to the population.\nFurther, the population is (usually) unknown to us. To quantify the true accuracy of a fitted model – that is, how well the model describes the population, not the sample we collected – we can use a technique called cross-validation.\n\nBefore we learn about cross-validation, let’s first try to gain further conceptual understanding of why \\(R^2\\) tends to overestimate model accuracy."
  },
  {
    "objectID": "notes/model-accuracy.html#overfitting",
    "href": "notes/model-accuracy.html#overfitting",
    "title": "Model accuracy",
    "section": "4 Overfitting",
    "text": "4 Overfitting\nWhen you fit a model to some sample of data, there is always a risk of overfitting. As the modeler, you have the freedom to fit your sample data better and better (you can add more and more terms, increasing the \\(R^2\\) value). But you need to be careful not to fit the sample data too well.\n\nThis is because any given set of data contains not only the true, underlying patterns we are interested in (the true model or signal), but also random variation (noise). Fitting the sample data too well means we fit not only the signal but also the noise in the data.\nAn overfit model will perform really well on the data it has been trained on (the sample) — we can even fit the sample perfectly if we add enough terms! - but an overfit model will be bad at predicting new, unseen values. Image we collect an additional data point drawn from the population. An overfit model would predict this point poorly!\nOur goal is to find the optimal fitted model – the one that gets as close to the true model as possible without overfitting. But we have no way of knowing which part of the data we sampled is signal and which part is noise. So, we use cross-validation to help identify overfitting."
  },
  {
    "objectID": "notes/model-accuracy.html#model-complexity",
    "href": "notes/model-accuracy.html#model-complexity",
    "title": "Model accuracy",
    "section": "5 Model complexity",
    "text": "5 Model complexity\nIn the lecture on model specification, we briefly mentioned that we would also want to take into consideration the complexity of the model. Simple models are easier to interpret but may not capture all complexities in the data, while complex models can suffer from overfitting the data or be difficult to interpret. Let’s expand on this in the context of model accuracy.\n\nComplex models have the potential to describe many kinds of functions, and the true model — the model that most accurately describes the population we sampled our data from — could be among them. However, complex models have a lot free parameters to estimate (by definition, that’s what makes them complex!), which makes it more difficult to obtain stable parameter estimates with small samples sizes or noisy data.\nSimple models are limited in the types of functions they can describe, so they may not approximate the true model very accurately. However, they have fewer free parameters, which makes it easier to obtain stable parameter estimates with small sample sizes or noisy data.\nWe have no way of knowing a priori whether a simple or complex model will be more accurate for a given dataset. It depends on many things, including the data we have, the underlying relationships, and our research questions. Luckily, we can use cross-validation to find out, trying different models and quantify each model’s accuracy."
  },
  {
    "objectID": "notes/model-accuracy.html#cross-validation",
    "href": "notes/model-accuracy.html#cross-validation",
    "title": "Model accuracy",
    "section": "6 Cross-validation",
    "text": "6 Cross-validation\nRemember from above, the question we actually want to answer with \\(R^2\\) is not how well does the model we fit describe the sample we collected, but how well does the model we fit describe the population we are interested in. But \\(R^2\\) on the sample will tend to overestimate the model’s accuracy on the population. To estimate the accuracy of the model on the population, we need to use a simple but powerful technique called cross-validation. Given a sample of data, there are 3 simple steps to any cross-validation technique:\n\nLeave some data out\nFit a model (to the data kept in)\nEvaluate the model on the left out data (e.g. \\(R^2\\))\n\nThere are many ways to do cross-validation — reflecting that there are many ways we can leave some data out — but they all follow this general 3-step process. We’ll focus on two common approaches in this class:\n\nIn leave-one-out cross-validation, we leave out a single data point and use the fitted model to predict that single point. We repeat this process for every data point, then evaluate each model’s prediction on the left out points (we can use \\(R^2\\)!).\nIn \\(k\\)-fold cross-validation, instead of leaving out a single data point, we randomly divide the dataset into \\(k\\) parts and use the fitted model to predict that part. We repeat this process for every part, then evaluate each model’s prediction on the left out parts (again, we can use \\(R^2\\)!).\n\nHow do we decide which cross-validation approach to use? There are two trade-offs to consider:\n\nHow many iterations do we want to do? The more iterations, the more reliable our accuracy estimate will be. But the more iterations, the more computational resources are required.\nHow much data do we want to use for each part? The more data we use to fit the model, the more accurate the model will be and the more stable the parameter estimates will be. But the more data we use in to estimate reliability, the more reliable our accuracy estimate will be.\n\n\n\nFor example, in leave-one-out cross-validation we use a lot of iterations (one for each data point), so we need a lot of computational resources, but we get to use almost all the data to fit our model (all but one point!) and all the data to calculate \\(R^2\\).\nKeep in mind that the parameter estimates we obtain on each iteration will be different, because they depend on both the model selected (stays the same each iteration) and the data we fit with (changes each iteration). So the \\(R^2\\) we compute via cross-validation really reflects an estimate of our model’s accuracy when fitted to a particular amount of data."
  },
  {
    "objectID": "notes/model-accuracy.html#back-to-model-selection",
    "href": "notes/model-accuracy.html#back-to-model-selection",
    "title": "Model accuracy",
    "section": "7 Back to model selection",
    "text": "7 Back to model selection\nBuilding models is itself an iterative process: we can use model accuracy obtained via cross-validation to determine which model to select (as a way to find the elusive optimal model fit). There are other ways to evaluate models beyond cross-validaiton. You may encounter AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion), for example, which are parametric approaches that attempt to compare different models and find the optimal fit (helping you avoid overfitting and excessively complex models).\n\nIn general AIC considers how well the model fits the data, the number of parameters, and the sample size (there is a penalty for more complex models); BIC is similar but has a stronger penalty for complex models (so will inherently favor simpler models).\nWe’ll focus on cross-validation in this class, because it makes fewer assumptions than metrics like AIC/BIC and is simpler to understand conceptually.\n\nBeyond model accuracy, there are other practical things one might want to consider when selecting a model, such as ease of interpretation and availability of resources (the data you can collect, the computing power you have, etc.)"
  },
  {
    "objectID": "notes/model-accuracy.html#further-reading",
    "href": "notes/model-accuracy.html#further-reading",
    "title": "Model accuracy",
    "section": "8 Further reading",
    "text": "8 Further reading"
  },
  {
    "objectID": "notes/inference.html",
    "href": "notes/inference.html",
    "title": "Inference",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/mixed-effects-models.html",
    "href": "notes/mixed-effects-models.html",
    "title": "Mixed-effects models",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/model-fitting.html",
    "href": "notes/model-fitting.html",
    "title": "Model fitting",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/model-fitting.html#model-fitting-basics",
    "href": "notes/model-fitting.html#model-fitting-basics",
    "title": "Model fitting",
    "section": "1 Model fitting basics",
    "text": "1 Model fitting basics\n\nin context of model building more broadly\na genear overview of the concept"
  },
  {
    "objectID": "notes/model-fitting.html#mean-squared-error",
    "href": "notes/model-fitting.html#mean-squared-error",
    "title": "Model fitting",
    "section": "2 Mean squared error",
    "text": "2 Mean squared error\nCost function."
  },
  {
    "objectID": "notes/model-fitting.html#error-surface",
    "href": "notes/model-fitting.html#error-surface",
    "title": "Model fitting",
    "section": "3 Error surface",
    "text": "3 Error surface\n\nWe can visualize the error surface for simple example: 2 parameters, \\(\\beta_0\\) and \\(\\beta_1\\), and the cost function (mean square error).\nShow nonlinear model v linear model figs\ngoal is to find the minimum point\nnotice the nonlinear model can have local minimums but lm has only 1. Because lm is a convex function."
  },
  {
    "objectID": "notes/model-fitting.html#gradient-descent",
    "href": "notes/model-fitting.html#gradient-descent",
    "title": "Model fitting",
    "section": "4 Gradient descent",
    "text": "4 Gradient descent\nIF we want to estimate the free parameters in a way that would work broadly, for linear or nonlinear models, we can use gradient descent.\n\nmachine learning / optimization.\nIf we have a lot of data, we could use stochastic gradient descent which is the same except we…"
  },
  {
    "objectID": "notes/model-fitting.html#ordinary-least-squares",
    "href": "notes/model-fitting.html#ordinary-least-squares",
    "title": "Model fitting",
    "section": "5 Ordinary least squares",
    "text": "5 Ordinary least squares\nAs we saw above, linear models have the special property that they have a solution, the OLS. Rather than searching the error surface iteratively via gradient descent (optimization), we can solve for this point directly with linear algebra.\n\nmatrix approach: we write the 3-step function.\nuse lm() in R.\ninfer approach:\n\nspecify(), fit()\n\n\n\nFurther reading\n\nCh. 8 Fitting models to data in Statistical Modeling"
  },
  {
    "objectID": "notes/sampling-variability.html",
    "href": "notes/sampling-variability.html",
    "title": "Sampling variability",
    "section": "",
    "text": "Suppose we measure a single quantity for a single condition: the height of human adults. Let’s get a visual summary of our data with a histogram.\nWe can see that our data follow a roughly Gaussian distribution (unimodal, symmetric), so we can use the mean to describe the central tendency and standard deviation to describe the spread of the values. They are given by the following equations:\n\nmean(\\(x\\)) = \\(\\overline{x} = \\frac{\\sum_{i=i}^{n} x_{i}}{n}\\)\nsd(\\(x\\)) = \\(\\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n-1}}\\)"
  },
  {
    "objectID": "notes/sampling-variability.html#exploring-a-simple-dataset",
    "href": "notes/sampling-variability.html#exploring-a-simple-dataset",
    "title": "Sampling variability",
    "section": "",
    "text": "Suppose we measure a single quantity for a single condition: the height of human adults. Let’s get a visual summary of our data with a histogram.\nWe can see that our data follow a roughly Gaussian distribution (unimodal, symmetric), so we can use the mean to describe the central tendency and standard deviation to describe the spread of the values. They are given by the following equations:\n\nmean(\\(x\\)) = \\(\\overline{x} = \\frac{\\sum_{i=i}^{n} x_{i}}{n}\\)\nsd(\\(x\\)) = \\(\\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\overline{x})^2}{n-1}}\\)"
  },
  {
    "objectID": "notes/sampling-variability.html#standard-deviation",
    "href": "notes/sampling-variability.html#standard-deviation",
    "title": "Sampling variability",
    "section": "2 Standard deviation",
    "text": "2 Standard deviation\nMost of us have an intuitive understanding of how the mean is computed: add up all the values and divide by the number of values. But what about the standard deviation? To understand how the standard deviation is computed, it helps to think the mean as a very simple model of our data: one where all cases are the same. Each individual case can be expressed as the model value plus how much the case deviates from the model value (residual, or leftover).\n\nJust as the mean describes a typical value in our data set, we can use the mean square of the residuals to describe the typical variation in our data set. To compute the mean square, we square the residuals (a neat trick that makes them all positive!) and then add them up — known as the sum of squares. Then, we divide by the number of cases, \\(n\\), minus 1. (we’ll see why we use \\(n-1\\) instead of just \\(n\\) later in the lecture).\nLater in the class, we’ll compute the mean square for all sorts of models. Here, when it’s used to describe how far a set of data are from the mean, it goes by the nickname variance. To fix the annoying squared units, we take the square root of the variance and voilà! We have the standard deviation."
  },
  {
    "objectID": "notes/sampling-variability.html#sampling-distribution",
    "href": "notes/sampling-variability.html#sampling-distribution",
    "title": "Sampling variability",
    "section": "3 Sampling distribution",
    "text": "3 Sampling distribution\nWhen measuring some quantity we are usually interested in knowing something about the population (the height of human adults, for example). But in practice we can only observe a small sample of the entire population.\n\nAny statistic we compute from a random sample we’ve collected (technically known as the parameter estimate) will be subject to sampling variability and will differ from that statistic computed on the entire population (technically known as the parameter). In other words, our measurements are noisy, and we need a way to express our uncertainty on the statistic we’ve computed. Quantifying this sampling variability is an important component of statistical inference.\nThe sampling distribution is the probability distribution of the values our parameter estimate can take on. We can construct the sampling distribution by taking a random sample, computing the statistic of interest, and repeating this process many times. The spread of these results indicates how the parameter estimate will vary from different random samples.\nWe can quantify the spread of our results (AKA express our uncertainty on our parameter estimate) using a parametric approach, by computing the standard deviation of our sampling distribution (called standard error!), or using a nonparametric approach, by constructing a confidence interval."
  },
  {
    "objectID": "notes/sampling-variability.html#standard-error-and-confidence-intervals",
    "href": "notes/sampling-variability.html#standard-error-and-confidence-intervals",
    "title": "Sampling variability",
    "section": "4 Standard error and confidence intervals",
    "text": "4 Standard error and confidence intervals\nThe standard deviation of the sampling distribution is known as the standard error. When the statistic of interest is the mean, the standard error is given by the following equation, where \\(\\sigma\\) is the standard deviation of the population and \\(n\\) is the sample size: \\(\\frac{\\sigma}{\\sqrt{n}}\\)\n\nIn practice, the standard deviation of the population is unknown, so we use the standard deviation of the sample as an estimate. That is why we use \\(n-1\\) in our mean square calculation. We assume our sample standard deviation is probably underestimating the population, so we “correct” this by dividing by \\(n-1\\) instead of \\(n\\).\nStandard error is considered parametric because we assume a parametric probability distribution (Gaussian) and compute the standard error based on what happens theoretically when we sample that distribution.\nclt? sample size relationship.\n\nWe can also quantify the sampling variability with a confidence interval, which expresses our uncertainty on our parameter estimate via a coverage interval. We can construct any confidence interval, but in ✨science✨ the convention is to choose the 95% coverage interval.\n\nRecall from last lecture that a coverage interval is a nonparametric statistic. The 95% coverage interval are the values between which 95% of the data points fall (the difference between the 2.5 percentile and the 97.5 percentile in our sampling distribution).\nConfidence intervals are closely related to standard error: assuming the sampling distribution is Gaussian (the parametric approach), the 68% confidence interval is +/- 1 standard error and the 95% confidence interval is +/- 2 standard error."
  },
  {
    "objectID": "notes/sampling-variability.html#bootstrapping",
    "href": "notes/sampling-variability.html#bootstrapping",
    "title": "Sampling variability",
    "section": "5 Bootstrapping",
    "text": "5 Bootstrapping\nIdeally, we would construct the sampling distribution by repeating our experiment many times, drawing new random samples from the population each time. But in practice, this is impossible. We are usually constrained — by time, money, access, etc. — such that we can only take one sample.\n\nThis is no problem if we can assume the underlying population distribution is Gaussian: we can just compute the standard error, which relies on the mean and standard deviation of the sample to approximate what would happen if we had sampled from a Gaussian probability distribution (see above!).\nWhat if the underlying distribution is not Gaussian, or we want to drop these parametric assumptions? We can use a technique called bootstrapping.\n\nWith bootstrapping, instead of assuming a parametric probability distribution, we can use the data themselves to approximate the underlying probability distribution. In other words, instead of sampling from the population, we can sample our sample! We’re “pulling ourselves up by our bootstraps”: constructing the sampling distribution from our own data.\n\nThe procedure is very simple. To illustrate, suppose we have a set a data with 100 data points. We generate the bootstrap sampling distribution by drawing the same number of data points (100) with replacement from our data set and compute the parameter estimate — mean, median, whatever — on those points, then we repeat the process many times.\n\nThere are many ways to generate a bootstrap sampling distribution in R. We will use the infer package in this class, which was developed by Hadley Wickham (the tidyverse guy!) and others to simplify aspects of statistical inference in R.\n\nspecify(response=x): choose which variable is the focus of our inference\ngenerate(reps=n, type='bootstrap'): generate n replicates of the data\ncalculate(stat=\"mean\"): statistic to calculate on each sample; what parameter are you trying to estimate?\n\nWe can further use infer to visualize the bootstrap sampling distribution and get a confidence interval around the parameter we estimated.\n\nvisualize(): quick visualization of the distribution\nget_confidence_interval(level=0.95, type=\"percentile\"): computes the confidence interval\nshade_ci(endpoints=c(min, max)): shades the visualization with the computed confidence interval"
  },
  {
    "objectID": "notes/sampling-variability.html#further-reading",
    "href": "notes/sampling-variability.html#further-reading",
    "title": "Sampling variability",
    "section": "6 Further Reading",
    "text": "6 Further Reading"
  },
  {
    "objectID": "notes/data-importing.html",
    "href": "notes/data-importing.html",
    "title": "Data importing",
    "section": "",
    "text": "The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. ~ Tidyverse package docs\n\nThe tidyverse collection of packages includes:\n\nggplot2 - for data visualization\ndplyr - for data wrangling\nreadr - for reading data\ntibble - for modern data frames\nstringr: for string manipulation\nforcats: for dealing with factors\ntidyr: for data tidying\npurrr: for functional programming\n\nWe load the tidyverse like any other package, with library(tidyverse). When we do, we will receive a message with (1) a list packages that were loaded and (2) a warning that there are potential conflicts with base R’s stats functions\n\nWe can resolve conflicts with the :: operator, which allows us to specify which package our intended function belongs to as a prefix: stats::filter() or dplyr::filter()"
  },
  {
    "objectID": "notes/data-importing.html#welcome-to-the-tidyverse",
    "href": "notes/data-importing.html#welcome-to-the-tidyverse",
    "title": "Data importing",
    "section": "",
    "text": "The tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. ~ Tidyverse package docs\n\nThe tidyverse collection of packages includes:\n\nggplot2 - for data visualization\ndplyr - for data wrangling\nreadr - for reading data\ntibble - for modern data frames\nstringr: for string manipulation\nforcats: for dealing with factors\ntidyr: for data tidying\npurrr: for functional programming\n\nWe load the tidyverse like any other package, with library(tidyverse). When we do, we will receive a message with (1) a list packages that were loaded and (2) a warning that there are potential conflicts with base R’s stats functions\n\nWe can resolve conflicts with the :: operator, which allows us to specify which package our intended function belongs to as a prefix: stats::filter() or dplyr::filter()"
  },
  {
    "objectID": "notes/data-importing.html#what-is-tidy-data",
    "href": "notes/data-importing.html#what-is-tidy-data",
    "title": "Data importing",
    "section": "2 What is tidy data?",
    "text": "2 What is tidy data?\nThe same underlying data can be represented in a table in many different ways; some easier to work with than others. The tidyverse makes use of tidy data principles to make datasets easier to work with in R. Tidy data provides a standard way of structuring datasets:\n\neach variable forms a column; each column forms a variable\neach observation forms a row; each row forms an observation\nvalue is a cell; each cell is a single value\n\nWhy is tidy data easier to work with?\n\nBecause consistency and uniformity are very helpful when programming\nVariables as columns works well for vectorized languages (R!)"
  },
  {
    "objectID": "notes/data-importing.html#functional-programming-with-purrr",
    "href": "notes/data-importing.html#functional-programming-with-purrr",
    "title": "Data importing",
    "section": "3 Functional programming with purrr",
    "text": "3 Functional programming with purrr\n\npurrr enhances R’s functional programming (FP) toolkit by providing a complete and consistent set of tools for working with functions and vectors. If you’ve never heard of FP before, the best place to start is the family of map() functions which allow you to replace many for loops with code that is both more succinct and easier to read. ~ purrr docs\n\nLet’s illustrate the joy of the tidyverse with one of its packages: purrr. The docs say that the best place to start is the family of map() functions, so we’ll do that.\nThe map() functions:\n\ntake a vector as input\napply a function to each element\nreturn a new vector\n\nWe say “functions” because there are 5, one for each type of vector:\n\nmap()\nmap_lgl()\nmap_int()\nmap_dbl()\nmap_chr()\n\nTo illustrate, suppose we have a data frame df with 3 columns and we want to compute the mean of each column. We could solve this with copy-and-paste (run mean() 3 different times) or try to use a for loop, but map() can do this with just one line:\nmap_dbl(df, mean)\nNow imagine we have 5 more data frames and we want to compute the mean of each of their columns, too. Again, we could copy and paste the map() function or use it in a for loop. But the map family allows us go up a layer of abstraction. We can use pmap() when we want to apply a function element-wise to corresponding items in multiple lists."
  },
  {
    "objectID": "notes/data-importing.html#modern-data-frames-with-tibble",
    "href": "notes/data-importing.html#modern-data-frames-with-tibble",
    "title": "Data importing",
    "section": "4 Modern data frames with tibble",
    "text": "4 Modern data frames with tibble\n\nA tibble, or tbl_df, is a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not. Tibbles are data.frames that are lazy and surly: they do less and complain more ~ tibble docs\n\nTibbles do less than data frames, in a good way:\n\nnever changes type of input (never converts strings to factors!)\nnever changes the name of variables\nonly recycles vectors of length 1\nnever creates row names\n\nYou can read more in vignette(“tibble”) if you are interested, but understanding these differences is not necessary to be successful in the course. The take-away is that data.frame and tibble sometimes behave differently. The behavior of tibble makes more sense for modern data science, so we should us it instead!\nCreate a tibble with one of the following:\n# (1) coerce an existing object with\nas_tibble(x)\n\n# (2) pass a column of vectors \ntibble(x=1:5, y=1)\n\n# (3) define row-by-row, short for traansposed tibble\ntribble(\n    ~x, ~y, ~z,\n    \"a\", 2, 3.6,\n    \"b\", 1, 8.5\n)\nWe will encounter two main ways tibbles and data frames differ:\n\nprinting - by default, tibbles print the first 10 rows and all columns that fit on screen, making it easier to work with large datasets. Tibbles also report the type of each column (e.g. &lt;dbl&gt;, &lt;chr&gt;)\nsubsetting - tibbles are more strict than data frames, which fixes two quirks we encountered last lecture when subsetting with [[ and $: (1) tibbles never do partial matching, and (2) they always generate a warning if the column you are trying to extract does not exist.\n\nTo test if something is a tibble or a data.frame:\n\nis_tibble(x)\nis.data.frame(x)"
  },
  {
    "objectID": "notes/data-importing.html#reading-data-with-readr",
    "href": "notes/data-importing.html#reading-data-with-readr",
    "title": "Data importing",
    "section": "5 Reading data with readr",
    "text": "5 Reading data with readr\n\nThe goal of readr is to provide a fast and friendly way to read rectangular data from delimited files, such as comma-separated values (CSV) and tab-separated values (TSV). It is designed to parse many types of data found in the wild, while providing an informative problem report when parsing leads to unexpected results. ~ readr docs\n\nOften we want to read in some data we’ve generated or collected outside of R. The most basic and common format is plain-text rectangular files. We will “read” these into R with readr’s read_*() functions.\nThe read_*() functions have two important arguments:\n\nfile path - the path to the file (that reader will try to parse)\ncolumn specification - a description of how each column should be converted from a character vector to a specific data type (col_types)\n\nThere are 7 supported file types, each with their own read_*() function:\n\nread_csv(): comma-separated values (CSV)\nread_tsv(): tab-separated values (TSV)\nread_csv2(): semicolon-separated values\nread_delim(): delimited files (CSV and TSV are important special cases)\nread_fwf(): fixed-width files\nread_table(): whitespace-separated files\nread_log(): web log files\n\nTo read .csv files, include a path and (optionally) a column specification:\n# (1) pass only the path; readr guesses col_types \nread_csv(path='path/to/file.csv')\n\n# (2) include a column specification with col_types\nread_csv(\n    path='path/to/file.csv', \n    col_types = list( x = col_string(), y = col_skip() )\n)\nWith no colum specification, readr uses the the first 1000 rows to guess with a simple heuristic:\n\nif column contains only T/F, logical\nif only numbers, double\nif ISO8601 standard, date or date-time\notherwise string\n\nThere are 11 column types that can be specified:\n\ncol_logical() - reads as boolean TRUE FALSE values\ncol_integer() - reads as integer\ncol_double() - reads as double\ncol_number() - numeric parser that can ignore non-numbers\ncol_character() - reads as strings\ncol_factor(levels, ordered = FALSE) - creates factors\ncol_datetime(format = \"\") - creates date-times\ncol_date(format = \"\") - creates dates\ncol_time(format = \"\") - creates times\ncol_skip() - skips a column\ncol_guess() - tries to guess the column\n\nSome useful additional arguments:\n\nif there is no header, include col_names = FALSE\nto provide a header, include col_names = c(\"x\",\"y\",\"z\")\nto skip some lines, include skip = n, where n is number of lines to skip\nto select which columns to import, include col_select(x, y)\nto guess column types with all rows, include guess_max = Inf\n\nSometimes weird things happen. The most common problems are:\n\nmissing values are not NA - your dataset has missing values, but they are not coded as NA as R expects. Solve by adding na argument (e.g. na=c(\"N/A\"))\ncolumn names have spaces - R cannot include spaces in variable names, so it adds backticks (e.g. `brain size`); we can just refer to them with backticks, but if that gets annoying, see janitor::clean_names() to fix them!\n\nReading more complex file types requires functions outside the tidyverse:\n\nexcel with readxl - see Spreadsheets in R for Data Science\ngoogle sheets with googlesheets4 - see Spreadsheets in R for Data Science\ndatabases with DBI - see Databases in R for Data Science\njson data with jsonlite - see Hierarchical data in R for Data Science"
  },
  {
    "objectID": "notes/data-importing.html#further-reading-and-references",
    "href": "notes/data-importing.html#further-reading-and-references",
    "title": "Data importing",
    "section": "6 Further reading and references",
    "text": "6 Further reading and references\nRecommended further reading:\n\nData tidying in R for Data Science\nTibbles in R for Data Science\nData import in R for Data Science:\nreadr cheatsheet"
  },
  {
    "objectID": "notes/data-visualization.html",
    "href": "notes/data-visualization.html",
    "title": "Data visualization",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/data-visualization.html#why-visualize-data",
    "href": "notes/data-visualization.html#why-visualize-data",
    "title": "Data visualization",
    "section": "1 Why visualize data?",
    "text": "1 Why visualize data?\n\nVisualization is a fundamentally human activity. A good visualization will show you things you did not expect or raise new questions about the data. A good visualization might also hint that you’re asking the wrong question or that you need to collect different data. ~ R for Data Science"
  },
  {
    "objectID": "notes/data-visualization.html#data-visualization-with-ggplot2",
    "href": "notes/data-visualization.html#data-visualization-with-ggplot2",
    "title": "Data visualization",
    "section": "2 Data visualization with ggplot2",
    "text": "2 Data visualization with ggplot2\nthree main parts:\n\ndata- your (tidy) data\naesthetic mappings (aes) - make data visible\ngeometric objects -\n\nother layers:\n\nfacets\nstatistics - statistical transformations of data\nscales - map data values to visual values of aestetic\ncoordinates\nthemes - overall visuals"
  },
  {
    "objectID": "notes/data-visualization.html#aesthetic-mapping",
    "href": "notes/data-visualization.html#aesthetic-mapping",
    "title": "Data visualization",
    "section": "3 Aesthetic mapping",
    "text": "3 Aesthetic mapping\naesthetic mappings (aes), make data visible\n\nx, y: variable on x and y axis\ncolor: outline color of geom\nfill: fill color of geom\ngroup: group geom belongs to\nshape: shape used to plot point (circle, square, etc)\nlinetype: type of line used (solid, dash, et)\nsize"
  },
  {
    "objectID": "notes/data-visualization.html#geometric-objects",
    "href": "notes/data-visualization.html#geometric-objects",
    "title": "Data visualization",
    "section": "4 Geometric objects",
    "text": "4 Geometric objects\ngeometric objects (geoms) - type of plot"
  },
  {
    "objectID": "notes/data-visualization.html#additional-layers",
    "href": "notes/data-visualization.html#additional-layers",
    "title": "Data visualization",
    "section": "5 Additional layers",
    "text": "5 Additional layers\nWe’ll cover some in the class:\n\nfacets\n\nfacet_grid(.~var1)\nfacet_grid(var2~.)\nfacet_grid(var2~var1)\nfacet_wrap(~var1) - wrap facets\n“scales”?\n\ncoordinates\n\ncoord_cartesian(xlim=c(0,5)) - xlim, ylim\ncoord_flip()\n\nlabels\n\nlabs(title=\"plot title\") - x, y, subtitle\nannotate()\n\nstatistics\nscales\nposition adjustments\nthemes\n\ntheme(legend.position=\"bottom\")"
  },
  {
    "objectID": "notes/data-visualization.html#saving-plots",
    "href": "notes/data-visualization.html#saving-plots",
    "title": "Data visualization",
    "section": "6 Saving plots",
    "text": "6 Saving plots\nHelper functions:\n\nlast_plot() - returns the last plot\nggsave(\"plot.png\", width=5, height=5) - saves last plot\n\n\nFurther reading and references\nUseful resources: - ggplot cheat sheet - introduction to palmerpenguins\nRecommended further reading: - https://moderndive.com/2-viz.html\nOther references:\nhttps://r.qcbs.ca/workshop03/book-en/the-basics-of-visualizing-data.html\nhttps://r4ds.hadley.nz/layers"
  },
  {
    "objectID": "notes/hypothesis-testing.html",
    "href": "notes/hypothesis-testing.html",
    "title": "Hypothesis testing",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/hypothesis-testing.html#hypothesis-testing",
    "href": "notes/hypothesis-testing.html#hypothesis-testing",
    "title": "Hypothesis testing",
    "section": "1 Hypothesis testing",
    "text": "1 Hypothesis testing\n\nWe can’t! At least we can’t quantify it. Intead we can use a cool statistical approach to help us: hypothesis test\nHow it works: pos a NULL hypothesis?\n\nwhy null? we need to quantify it\nask if the null were true, how likely is that we’d observe this result?"
  },
  {
    "objectID": "notes/hypothesis-testing.html#p-values",
    "href": "notes/hypothesis-testing.html#p-values",
    "title": "Hypothesis testing",
    "section": "2 p-values",
    "text": "2 p-values\n\nOne way to quantify how likely this is with a ttest.\n\nt-tests are parametric — we figure out the probability theoretically based on an existing distribution\ncomputes a p-value, which quantifies how likely it is we’d observe some result\ndoes the t-test construct the null? I think yes, theoretically, or because properties are known.\n\nInterpretation of p-value\n\nreject the null (p &lt; some threshold)\nfail to reject the null (p &gt; some threshold)"
  },
  {
    "objectID": "notes/hypothesis-testing.html#nonparametric-approaches-to-p-values",
    "href": "notes/hypothesis-testing.html#nonparametric-approaches-to-p-values",
    "title": "Hypothesis testing",
    "section": "3 Nonparametric approaches to p-values",
    "text": "3 Nonparametric approaches to p-values\n\nBut what if we aren’t doing the mean? Or the distribution isn’t normal? We can use the same principle of the sampling distribution. Two approaches to construct the null distribution from the data:\n\nResampling: one appraoch: shuffle around, then get the mean, construct null distribution\nBootstrapping: another approach: sample with replacement, get the mean, construct null distribution\nwhich one should we pick? see k\n\np-value\n\nhow likely is the observed mean? with either of these null distributions we can use the confidence interval\nbeforehand we set an alpha value (usually 0.05) which is the cutoff for what we think is likely/unlikely."
  },
  {
    "objectID": "notes/hypothesis-testing.html#there-is-only-one-test",
    "href": "notes/hypothesis-testing.html#there-is-only-one-test",
    "title": "Hypothesis testing",
    "section": "4 There is only one test",
    "text": "4 There is only one test\n(insert image of there is only one test)\n\nAllows us to appreciate that, though there is a myriad of statistical tests available, there is really only one test:\n\ndescibe the flow chart\n\nWe can use the infer package to perform this kind of hypothesis testing\n\nShow example.\n\n5 Exploring relationships\n\nLast week we explored data in which we measured a single quantity: brain size. We explored this dataset with a histogram and modeled it with a single value (mean). Suppose we have a slightly more complex dataset in which we measure both brain size and body mass (two quantities!). We might want to know whether there is a relationship between brain size and body mass. We can explore the relationship between two quantities visually with a scatter plot.\nIf there is no relationship between the variables, we say they are independent. We can think of independence in the following way: knowing the value of one variable provides no information about the other variable. In our example, knowing an animal’s body size provides no information about their brain size. If there is some relationship between the variables, we can consider two types:\n\nThere may be a linear relationship between the variables. When one goes up the other goes up (positive) or when one goes up the other goes down (negative). In our example, there is a linear relationship between brain size and body mass: as body mass increases, brain size also increases.\nOr a nonlinear relationship. Nonlinear is a very broad category that encompasses all relationships that are not linear (e.g. a U-shaped curve)."
  },
  {
    "objectID": "notes/hypothesis-testing.html#correlation",
    "href": "notes/hypothesis-testing.html#correlation",
    "title": "Hypothesis testing",
    "section": "6 Correlation",
    "text": "6 Correlation\nOne way to quantify linear relationships is with correlation (\\(r\\)). Correlation expresses the linear relationship as a range from -1 to 1, where -1 means the relationship is perfectly negative and 1 means the relationship is perfectly positive.\nCorrelation can be calculated by taking the z-score of each variable (a normalization technique in which we subtract the mean and divide by the standard deviation) and then compute the average product of each variable:\n\nshow equation\nwe can achieve this calculation in R by computing\nadd getting z-score with R?\nCorrelation only describes linear relationships.\n\nIs the correlation we observed significantly different from zero? We can apply the techniques we learned over the past few weeks to find out. Just like the mean — and all other test statistics! — \\(r\\) is subject to sampling variability. We can indicate our uncertainty around the correlation we observe in the same way: construct the sampling distribution of the correlation via bootstrapping, compute a confidence interval, and compute the p-value.\n(demo with infer framework)"
  },
  {
    "objectID": "notes/hypothesis-testing.html#further-reading",
    "href": "notes/hypothesis-testing.html#further-reading",
    "title": "Hypothesis testing",
    "section": "7 Further reading",
    "text": "7 Further reading\n\nThe logic of hypothesis testing - Chapter 13, Statistical Modeling\nHypothesis testing - Chapter 9, Modern Dive"
  },
  {
    "objectID": "notes/r-basics.html",
    "href": "notes/r-basics.html",
    "title": "R basics",
    "section": "",
    "text": "We begin by defining some basic concepts:\n\nExpressions are combinations of values, variables, operators, and functions that can be evaluated to produce a result. Expressions can be as simple as a single value or more complex involving calculations, comparisons, and function calls. They are the fundamental building blocks of programming.\n\n10 - a simple value expression that evaluates to 10.\nx &lt;- 10 - an expression that assigns the value of 10 to x.\nx + 10 - an expression that adds the value of x to 10.\na &lt;- x + 10 - an expression that adds the value of x to 10 and assigns the result to the variable a\n\nOjbects allow us to store various types of data, such as numbers, text, vectors, matrices; and more complex structures like functions and data frames. Objects are created by assigning values to variable names with the assignment operator, &lt;-. For example, in x &lt;- 10, x is an object assigned to the value 10.\nNames that we assign to objects must include only letters, numbers, ., or _. Names must start with a letter (or . if not followed by a number).\nAttributes allow you to attach arbitrary metadata to an object. For example, adding a dim (dimension) attribute to a vector allows it to behave like a matrix or n dimensional array.\nFunctions (or commands) are reusable pieces of code that take some input, preform some task or computation, and return an output. Many functions are built-in to base R (see below!), others can be part of packages or even defined by you. Functions are objects!\nEnvironment is the collection of all the objects (functions, variables etc.) we defined in the current R session.\nPackages are collections of functions, data, and documentation bundled together in R. They enhance R’s capabilities by introducing new functions and specialized data structures. Packages need to be installed and loaded before you can use their functions or data.\nComments are notes you leave to yourself (within code blocks in colab) to document your code; comments are not evaluated.\nMessages are notes R leaves for you, after you run your code. Messages can be simply for-your-information, warnings that something unexpected might happen, or erros if R cannot evaluate your code.\n\nWays to get help when coding in R:\n\nRead package docs - packages usually come with extensive documentation and examples. Reading the docs is one of the best ways to figure things out. Here is an example from the dplyr package.\nRead error messages - read any error messages you receive while coding — they give clues about what is going wrong!\nAsk R - Use R’s built-in functions to get help as you code\nAsk on Ed - ask questions on our class discussion board!\nAsk Google or Stack Overflow - It is a normal and important skill (not cheating) to google things while coding and learning to code! Use keywords and package names to ensure your solutions are course-relevant.\nAsk ChatGPT - You can similarly use ChatGPT or other LLMs as a resource. But keep in mind they may provide a solution that is wrong or not relevant to what we are learning in this course."
  },
  {
    "objectID": "notes/r-basics.html#basics",
    "href": "notes/r-basics.html#basics",
    "title": "R basics",
    "section": "",
    "text": "We begin by defining some basic concepts:\n\nExpressions are combinations of values, variables, operators, and functions that can be evaluated to produce a result. Expressions can be as simple as a single value or more complex involving calculations, comparisons, and function calls. They are the fundamental building blocks of programming.\n\n10 - a simple value expression that evaluates to 10.\nx &lt;- 10 - an expression that assigns the value of 10 to x.\nx + 10 - an expression that adds the value of x to 10.\na &lt;- x + 10 - an expression that adds the value of x to 10 and assigns the result to the variable a\n\nOjbects allow us to store various types of data, such as numbers, text, vectors, matrices; and more complex structures like functions and data frames. Objects are created by assigning values to variable names with the assignment operator, &lt;-. For example, in x &lt;- 10, x is an object assigned to the value 10.\nNames that we assign to objects must include only letters, numbers, ., or _. Names must start with a letter (or . if not followed by a number).\nAttributes allow you to attach arbitrary metadata to an object. For example, adding a dim (dimension) attribute to a vector allows it to behave like a matrix or n dimensional array.\nFunctions (or commands) are reusable pieces of code that take some input, preform some task or computation, and return an output. Many functions are built-in to base R (see below!), others can be part of packages or even defined by you. Functions are objects!\nEnvironment is the collection of all the objects (functions, variables etc.) we defined in the current R session.\nPackages are collections of functions, data, and documentation bundled together in R. They enhance R’s capabilities by introducing new functions and specialized data structures. Packages need to be installed and loaded before you can use their functions or data.\nComments are notes you leave to yourself (within code blocks in colab) to document your code; comments are not evaluated.\nMessages are notes R leaves for you, after you run your code. Messages can be simply for-your-information, warnings that something unexpected might happen, or erros if R cannot evaluate your code.\n\nWays to get help when coding in R:\n\nRead package docs - packages usually come with extensive documentation and examples. Reading the docs is one of the best ways to figure things out. Here is an example from the dplyr package.\nRead error messages - read any error messages you receive while coding — they give clues about what is going wrong!\nAsk R - Use R’s built-in functions to get help as you code\nAsk on Ed - ask questions on our class discussion board!\nAsk Google or Stack Overflow - It is a normal and important skill (not cheating) to google things while coding and learning to code! Use keywords and package names to ensure your solutions are course-relevant.\nAsk ChatGPT - You can similarly use ChatGPT or other LLMs as a resource. But keep in mind they may provide a solution that is wrong or not relevant to what we are learning in this course."
  },
  {
    "objectID": "notes/r-basics.html#important-functions",
    "href": "notes/r-basics.html#important-functions",
    "title": "R basics",
    "section": "2 Important functions",
    "text": "2 Important functions\nFor objects:\n\nstr(x) - returns summary of object’s structure\ntypeof(x) - returns object’s data type\nlength(x) - returns object’s length\nattributes(x) - returns list of object’s attributes\nx - returns object x\nprint(x) - prints object x\n\nFor environment:\n\nls() - list all variables in environment\nrm(x) - remove x variable from environment\nrm(list = ls()) - remove all variables from environment\n\nFor packages:\n\ninstall.packages() to install packages\nlibrary() to load the package into your current R session.\ndata() to load data from package into environment\nsessionInfo() - version information for current R session and packages\n\nFor help:\n\n?mean - get help with a function\nhelp('mean') - search help files for word or phrase\nhelp(package='tidyverse') - find help for a package"
  },
  {
    "objectID": "notes/r-basics.html#vectors",
    "href": "notes/r-basics.html#vectors",
    "title": "R basics",
    "section": "3 Vectors",
    "text": "3 Vectors\nOne of the must fundamental data structures in R is the vector. There are two types:\n\natomic vector - elements of the same data type\nlist - elements refer to any object (even complex objects or other lists)\n\nAtomic vectors can be one of six data types:\n\ndouble - real numbers, written in decimal (0.1234) or scientific notation (1.23e4)\n\nnumbers are double by default (3 is stored as 3.00)\nthree special doubles: Inf, -Inf, and NaN (not a number)\n\ninteger - integers, whole numbers followed by L (3L or 1e3L)\ncharacter - strings with single or double quotes (‘hello world!’ or “hello world!”)\nlogical - boolean, written (TRUE or FALSE) or abbreviated (T or F)\ncomplex - complex numbers, where i is the imaginary number (5 + 3i)\nraw - stores raw bytes\n\nTo create atomic vectors:\n\nc(2,4,6) - c() function for combining elements, returns 2 4 6\n2:4 - : notation to construct a sequence of integers, returns 2 3 4\nseq(from = 2, to = 6, by=2) - seq() function to create an evenly-spaced sequence, returns 2 4 6\n\nTo check an object’s data type:\n\ntypeof(x) - returns the data type of object x\nis.*(x) - test if object x is data type, returns TRUE or FALSE\n\nis.double()\nis.integer()\nis.character()\nis.logical()\n\n\nTo change an object to data type (explicit coercion):\n\nas.*(x) - coerce object to data type\n\nas.double()\nas.integer()\nas.character()\nas.logical()\n\n\nNote that atomic vectors must contain only elements of the same type. If you try to include elements of different types, R will coerce them into the same type with no warning (implicit coercion) according to the heirarchy character &gt; double &gt; integer &gt; logical."
  },
  {
    "objectID": "notes/r-basics.html#operations",
    "href": "notes/r-basics.html#operations",
    "title": "R basics",
    "section": "4 Operations",
    "text": "4 Operations\nArithmetic operators:\n\n+ - add\n- - subtract\n* - multiply\n/ - divide\n^ - exponent\n\nComparison operators return true or false:\n\na == b - equal to\na != b - not equal to\na &gt; b - greater than\na &lt; b - less than\na &gt;= b - greater than or equal to\na &lt;= b - less than or equal to\n\nLogical operators combine multiple true or false statements:\n\n& - and\n| - or\n! - not\nany() - returns true if any element meets condition\nall() - returns true if all elements meet condition\n%in% - returns true if any element is in the following vector\n\nMost math operations (and many functions) are vectorized in R:\n\nthey can work on entire vectors, without the need for explicit loops or iteration.\nthis a powerful feature that allows you to write cleaner, more efficient code\nTo illustrate, suppose x &lt;- c(1, 2, 3):\n\nx + 100 returns c(101, 102, 103)\nx == 1 returns c(TRUE, FALSE, FALSE)"
  },
  {
    "objectID": "notes/r-basics.html#more-complex-structures",
    "href": "notes/r-basics.html#more-complex-structures",
    "title": "R basics",
    "section": "5 More complex structures",
    "text": "5 More complex structures\nSome more complex data structures are built from atomic vectors by adding attributes:\n\nmatrix - a vector with a dim attribute representing 2 dimensions\narray - a vector with a dim attribute representing n dimensions\nfactor - an integer vector with two attributes: class=\"factor\" and levels, which defines the set of allowed values (useful for categorical data)\ndate-time - a double vector where the value is the number of seconds since Jan 01, 1970 and a tzone attribute representing the time zone\ndata.frame - a named list of vectors (of equal length) with attributes for names (column names), row.names, and class=\"data.frame\" (used to represent datasets)\n\nTo create more complex structures:\n\nlist(x=c(1,2,3), y=c('a','b')) - create a list\nmatrix(x, nrow=2, ncol=2) - create a matrix from a vector x with nrow and ncol\narray(x, dim=c(2,3,2)) - create an array from a vector x with dimensions\nfactor(x, levels=unique(x)) - turn a vector x into a factor\ndata.frame(x=c(1,2,3), y=c('a','b','c')) - create a data frame\n\nMissing elements and empty vectors:\n\nNA- used to represent missing or unknown elements in vectors. Note that NA is contageous: expressions including NA usually return NA. Check for NA values with is.na().\nNULL - used to represent an empty or absent vector of arbitrary type. NULL is its own special type and always has length zero and NULL attributes. Check for NULL values with is.null()."
  },
  {
    "objectID": "notes/r-basics.html#subsetting",
    "href": "notes/r-basics.html#subsetting",
    "title": "R basics",
    "section": "6 Subsetting",
    "text": "6 Subsetting\n\nSubsetting is a natural complement to str(). While str() shows you all the pieces of any object (its structure), subsetting allows you to pull out the pieces that you’re interested in. ~ Hadley Wickham, Advanced R\n\nThere are three operators for subsetting objects:\n\n[ - subsets (one or more) elements\n[[ and $ - extracts a single element\n\nThere are six ways to subset multiple elements from vectors with [:\n\nx[c(1,2)] - positive integers select elements at specified indexes\nx[-c(1,2)] - negative integers select all but elements at specified indexes\nx[c(\"name\", \"name2\")] select elements by name, if elements are named\nx[] - nothing returns the original object\nx[0] - zero returns a zero-length vector\nx[c(TRUE, TRUE)] - select elements where corresponding logical value is TRUE\n\nThese also apply when selecting multiple elements from higher dimensional objects (matrix, array, data frame), but note that:\n\nindexes for different dimensions are separated by commas [rows, columns, ...]\nomitted dimensions return all values along that dimension\nthe result is simplified to the lowest possible dimensions by default\ndata frames can also be indexed like a vector (selects columns)\n\nThere are 3 ways to extract a single element from any data structure:\n\n[[2]] - a single positive integer (index)\n[['name']] - a single string\nx$name - the $ operator is a useful shorthand for [['name']]\n\nWhen extracting single elements, note that:\n\n[[ is preferred for atomic vectors for clarity (though[ also works)\n$ does partial matching without warning; use options(warnPartialMatchDollar=TRUE)\nthe behavior for invalid indexes is inconsistent: sometimes you’ll get an error message, and sometimes it will return NULL"
  },
  {
    "objectID": "notes/r-basics.html#built-in-functions",
    "href": "notes/r-basics.html#built-in-functions",
    "title": "R basics",
    "section": "7 Built-in functions",
    "text": "7 Built-in functions\nNote that you do not need to memorize these built-in functions to be successful on quizzes. Use this as a reference.\nFor basic math:\n\nlog(x) - natural log\nexp(x) - exponential\nsqrt(x) - square root\nabs(x) - absolute value\nmax(x) - largest element\nmin(x) - smallest element\nround(x, n) - round to n decimal places\nsignif(x, n) - round to n significant figures\nsum(x) - add all elements\n\nFor stats:\n\nmean(x) - mean\nmedian(x) - median\nsd(x) - standard deviation\nvar(x) - variance\nquantile(x) - percentage quantiles\nrank(x) - rank of elements\ncor(x, y) - correlation\nlm(x ~ y, data=df) - fit a linear model\nglm(x ~ y, data=df) - fit a generalized linear model\nsummary(x) - get more detailed information from a fitted model\naov(x) - analysis of variance\n\nFor vectors:\n\nsort(x) - return sorted vector\ntable(x) - see counts of values in a vector\nrev(x) - return reversed vector\nunique(x) - return unique values in a vector\narray(x, dim) - transform vector into n-dimensional array\n\nFor matrices:\n\nt(m) - transpose matrix\nm %+% n - matrix multiplication\nsolve(m, n) - find x in m * x = n\n\nFor data frames:\n\nview(df) - see the full data frame\nhead(df) - see the first 6 rows of data frame\nnrow(df) - number of rows in a data frame\nncol(df) - number of columns in a data frame\ndim(df) - number of rows and columns in a data frame\ncbind(df1, df2) - bind dataframe columns\nrbind(df1, df2) - bind dataframe rows\n\nFor strings:\n\npaste(x, y, sep=' ') - join vectors together element-wise\ntoupper(x) - convert to uppercase\ntolower(x) - convert to lowercase\nnchar(x) - number of characters in a string\n\nFor simple plotting:\n\nplot(x) values of x in order\nplot(x, y) - values of x against y\nhist(x) - histogram of x"
  },
  {
    "objectID": "notes/r-basics.html#programming-in-r",
    "href": "notes/r-basics.html#programming-in-r",
    "title": "R basics",
    "section": "8 Programming in R",
    "text": "8 Programming in R\nWriting functions and handling control flow are important aspects of learning to program in any language. For our purposes, some general conceptual knowledge on these topics is sufficient (see below). Those interested to learn more might enjoy the book Hands-On Programming with R.\n\nFunctions are reusable pieces of code that take some input, perform some task or computation, and return an output.\nfunction(inputs){\n    # do something\n    return(output)\n}\nControl flow refers to managing the order in which expressions are executed in a program:\n\nif…else - if something is true, do this; otherwise do that\nfor loops - repeat code a specific number of times\nwhile loops - repeat code as long as certain conditions are true\nbreak - exit a loop early\nnext - skip to next iteration in a loop"
  },
  {
    "objectID": "notes/r-basics.html#further-reading-and-references",
    "href": "notes/r-basics.html#further-reading-and-references",
    "title": "R basics",
    "section": "9 Further reading and references",
    "text": "9 Further reading and references\nSuggested further reading:\n\nBase R Cheat Sheet\nR Nuts and Bolts in R Programming for Data Science by Roger Peng\n\nOther references:\n\nVectors in Advanced R by Hadley Wickham\nSubsetting in Advanced R by Hadley Wickham\nA field guide to base R in R for Data Science by Hadley Wickham"
  },
  {
    "objectID": "notes/model-reliability.html",
    "href": "notes/model-reliability.html",
    "title": "Model reliability",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/model-specification.html",
    "href": "notes/model-specification.html",
    "title": "Model specification",
    "section": "",
    "text": "Under Construction"
  },
  {
    "objectID": "notes/model-specification.html#correlation-as-model-building",
    "href": "notes/model-specification.html#correlation-as-model-building",
    "title": "Model specification",
    "section": "1 Correlation as model building",
    "text": "1 Correlation as model building\nCorrelation is actually a simple case of model building in which we use one value (\\(x\\)) to predict another (\\(y\\)). Specifically, we are fitting the linear model \\(y = ax + b\\), where \\(a\\) and \\(b\\) are free parameters. Here, \\(y\\) is known as the response variable (the value we are trying to predict) and \\(x\\) is the explanatory variable (the one that we are attempting to explain the response variable with).\n\nAfter z-scoring our variables, the correlation between \\(x\\) and \\(y\\) is equal to the slope of the line that best predicts \\(y\\) from \\(x\\). In our example data set, the correlation between brain size and body mass is NUM and the slope of the line that best describes the relationship between these z-scored variables is also NUM."
  },
  {
    "objectID": "notes/model-specification.html#model-building-overview",
    "href": "notes/model-specification.html#model-building-overview",
    "title": "Model specification",
    "section": "2 Model building overview",
    "text": "2 Model building overview\nIdeally we want to understand statistical modeling beyond the simple case of correlation. What if we have more than one explanatory variable? What if the relationship between variables is not linear? To address model building more broadly, it is helpful to think of building any model as a four-step process. We’ll treat each of these separately over the coming weeks. The goal for today is to get a big picture overview of the model building process and the types of models we might encounter in our research.\n\nModel specification (this week): what is the form?\nModel fitting (week 6); you have the form, how do you guess the free parameters?\nModel accuracy (week 7): you’ve estimated the parameters, how well does that model describe your data?\nModel reliability (week 8): when you estimate the parameters, there is some uncertainty on them"
  },
  {
    "objectID": "notes/model-specification.html#types-of-models",
    "href": "notes/model-specification.html#types-of-models",
    "title": "Model specification",
    "section": "3 Types of models",
    "text": "3 Types of models\nModel specification involves deciding which type of model we’d like to apply. We will mostly apply linear models in this class — for good reasons we will talk about next time! — but it’s useful to first have a conceptual overview of the types of models we could apply.\n\n\nSupervised vs unsupervised\nAs a starting point, we can divide statistical models into two types of learning (so called because we are trying to “learn” something about the data):\n\nIn supervised learning, we want to predict an output (or response) variable based on one or more input (or explanatory) variables. We call this supervised learning because both the input and output variables are known (sometimes this is called “labeled” data), and we are trying to learn the relationship between them. Linear regression is an example of a supervised learning model.\nIn unsupervised learning, there is no specific output variable that we are trying to predict. Instead, the model’s objective is to discover the underlying structure or patterns in the data. We call this unsupervised learning because only the input data is available (sometimes this is called “unlabeled” data); the model is trying to identify relationships in the data without being “supervised” by an outcome variable. PCA and cluster analysis are examples of unsupervised learning.\nThere are other machine learning approaches beyond these, like semi-supervised learning (combining both labeled and unlabeled data) and reinforcement learning (learning through trial and error based on rewards or penalties). But in this course we will focus on supervised learning models.\n\n\n\nRegression v classification\nRegression and classification are both types of supervised learning models — using one or more input variables to predict an output variable. The only difference between them is in type of output variable:\n\nRegression is used when we want to predict a continuous output, meaning it is a number that can take on any value within a range (e.g. height, weight, response time)\nClassification is used when we want to predict a categorical output, meaning it falls into specific classes or categories (e.g. true/false, yes/no, male/female/nonbinary). We cover this during advanced model building.\n\n\n\nLinear v nonlinear regression\nThere are many types of regression models, but we can simplify by dividing them into two main types of models:\n\nIn linear regression, the relationship between the explanatory variable(s) and response variable is represented by a linear equation (a straight line graphed on a two-dimensional plane).\nNonlinear regression is useful when the data does not follow a linear pattern, and the relationship between the variables is better captured by more complex functions (e.g. a curve or any other nonlinear shape). Nonlinear regression models can be further divided into two types:\n\nWe can linearize a nonlinear model by applying a mathematical transformation to make it look like a linear equation (e.g. log, square root, etc). We can fit a linearized model just like a linear model, but the prediction of the model is not linear with respect to x.\nSometimes, no matter what transformation you apply, you cannot acheive a linear form. These types of models are referred to as nonlinearizable nonlinear models and are beyond the scope of this class!"
  },
  {
    "objectID": "notes/model-specification.html#model-specification",
    "href": "notes/model-specification.html#model-specification",
    "title": "Model specification",
    "section": "4 Model specification",
    "text": "4 Model specification\nRecall from last week that model specification is one aspect of the model building process. It involves selecting the functional form of the model (the type of model) and choosing which variables to include. When specifying a model, you’ll need to make the following decisions:\n\nResponse variable (\\(y\\)): Choose the variable you want to predict or explain (output).\nExplanatory variables(\\(x_n\\)): Choose the variables that may explain the variation in the response variable (inputs).\nFunctional form: Specify the functional relationship between the response and explanatory variables. For linear models, the relationship is linear, and we use the linear model equation as our functional form!\nModel terms: Choose which model terms to include, which is another way of saying that you need to decide how to include your explanatory variables in the model (since they can be included in more than one way).\n\nIn addition to the decisions above, the following issues can also be considered part of the model specification process. But we will consider these in future weeks.\n\nModel assumptions: Check any assumptions underlying the model you selected (e.g. does the model assume the relationship is linear?).\nModel complexity: Simple models are easier to interpret but may not capture all complexities in the data. Complex models may suffer from overfitting the data or being difficult to interpret.\n\nA well-specified model should be based on a clear understanding of the data, the underlying relationships, and the research question."
  },
  {
    "objectID": "notes/model-specification.html#response-variable-y",
    "href": "notes/model-specification.html#response-variable-y",
    "title": "Model specification",
    "section": "5 Response variable, \\(y\\)",
    "text": "5 Response variable, \\(y\\)\nChoosing the response variable is usually straightforward once you’ve clearly defined your research question: what is the thing you are trying to understand? You also need to make sure whatever you’ve selected is something you can measure (or has already been measured!)\n\nIn the swim records example, we are trying to explain variation in record times, so we choose record time as our \\(y\\). In the brain size example we are trying to explain variation in brain sizes, so we choose brain size as our \\(y\\).\nRemember from last time that we can use regression for continuous response variables (numbers) but we need to use classification if the response variable is categorical (categories or levels)."
  },
  {
    "objectID": "notes/model-specification.html#explanatory-variables-x_n",
    "href": "notes/model-specification.html#explanatory-variables-x_n",
    "title": "Model specification",
    "section": "6 Explanatory variables, \\(x_n\\)",
    "text": "6 Explanatory variables, \\(x_n\\)\nChoosing which explanatory variables to include requires a bit more careful consideration. It’s one part using your knowledge about the domain you are studying and one part exploratory data analysis!\n\nOne extreme would be to include just one explanatory variable: the obvious one based on your research question. In the swim records example, we want to understand how swim records change over time, so we should definitely include time as an explanatory variable. But this model is underspecified. We need to consider other variables that have the potential to explain variation in our response variable, even if they are not of direct interest. For example, some variation in swim record times can likely be explained by the swimmer’s gender, so we should include gender as an explanatory variable in our model.\nImportantly, we do not include every explanatory variable we can think of! We want to explain the variation in our response variable without building too complex a model or overfitting the data (overspecifying). We’ll go into more detail about this in future lectures. For now, just remember you’re Goldilocks: you want the explanatory variables in the model to explain just the right amount of variation."
  },
  {
    "objectID": "notes/model-specification.html#functional-form",
    "href": "notes/model-specification.html#functional-form",
    "title": "Model specification",
    "section": "7 Functional form",
    "text": "7 Functional form\nLast lecture we introduced the types of models that we could select when specifying a model. We also mentioned that we will focus on linear models in this class (which are a type of regression model, which are themselves types of supervised learning models!)\nWhen specifying the functional form of a model, we’re literally specifying the mathematical formula we’re going to use to represent the relationship between our response and explanatory variables. Linear models are models in which the response variable (output) is a weighted sum of the explanatory variables (inputs). In other words, there is a linear relationship between the response variable and the explanatory variables. The linear model equation can be expressed in many ways (which can be confusing!). Here are four different ways of representing the formula for the linear model, to emphasize that they are all the same thing.\n\nIn high school algebra, the linear model equation is represented as the equation of a straight line. \\(y\\) is the response variable, \\(x\\) is the explanatory variable, \\(a\\) is the slope of the line (the relationship between \\(x\\) and \\(y\\)) and \\(b\\) is the y-intercept (the value of \\(y\\) when \\(x\\) is zero). You have (hopefully!) already encountered this equation.\n\n\n\\(y=ax+b\\).\n\n\n\nIn machine learning, the linear model equation is usually represented as a weighted sum of input variables. Note that the only changes are that we refer to the free parameters as weights (\\(w_n\\)) instead of \\(a\\) and \\(b\\) (to emphasize these are the weights the model learns) and the ability to add more than one input (\\(x_1, x_2, ...x_n\\) instead of just \\(x\\)):\n\n\n\\(y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\\)\n\n\nIn statistics, the linear model equation is also represented as a weighted sum of input variables, except we call the weights “regression coefficients” (\\(\\beta_n\\)) and we add an error term to account for unexplained variability (\\(\\varepsilon\\)):\n\n\n\\(y = β_0 + β_1x_1 + β_2x_2 + ... + β_nx_n + ε\\)\n\n\nIn matrix notation, the linear model equation is represented as a dot product of vectors. This is just a more compact representation of the statistics (or machine learning) way, often used in linear algebra and statistics. \\(X\\) is the matrix containing the values of the explanatory variables, \\(β\\) is the vector of regression coefficients, and \\(\\varepsilon\\) is the vector of error terms.\n\n\n\\(y = Xβ + ε\\)"
  },
  {
    "objectID": "notes/model-specification.html#model-terms",
    "href": "notes/model-specification.html#model-terms",
    "title": "Model specification",
    "section": "8 Model terms",
    "text": "8 Model terms\nWe’ve specified our response and explanatory variables and the functional form of our model. Now we need to specify the model terms. Model terms describe how to include the explanatory variables in our model (they can be included in more than one way!) There are four kinds of terms: (1) intercept, (2) main, (3) interaction, and (4) transformation.\n\nThe intercept term, \\(\\beta_0\\), is a constant (not variable) capturing the typical value of the response variable when all explanatory variables are zero. It allows the model to have an offset from the origin, so it is also called the “offset” or “gain” parameter in some fields. Unless it makes sense for our response variable to be zero when all other variables are zero (it rarely does!) we should include the intercept term.\n\n\nin R: y ~ 1\nin eq: \\(y=\\beta_0 + \\varepsilon\\)\n\n\nMain terms (AKA main effects) represent the effect of each explanatory variable on the response variable directly. In other words, how does the response variable change as a result of changes in a given explanatory variable, when all other explanatory variables are zero? Each main term corresponds to one explanatory variable and is included in the model as a single term (\\(\\beta_nx_n\\)). We can add as many explanatory variables as we like to the model:\n\n\nin R: y ~ 1 + year + gender\nin eq: \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\)\n\nNote that can include categorical explanatory variables like gender, we just need to find a way to represent the same information numerically, since linear models require numerical inputs.\nInteraction terms allow us to express that the effect of one explanatory variable on the response variable is different at different values of another explanatory variable. For example, in the swim records data, the effect of gender on record times changes over year (or said another way, the effect of year on record times is different for men and women). We are still describing how variation in the response variable is explained by one or more explanatory variables, we’re just describing how two (or more) variables combine to influence the response. In the linear model equation, we add a term to the model in which we multiply the values of the interacting variables.\n\nin R: y ~ 1 + year + gender + year:gender\n\nor the short way: y ~ 1 + year * gender\n\nin eq: \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2\\)\n\nTransformation terms allow us to modify the explanatory variables to accommodate nonlinear relationships with the response variable. Some of the most common transformations are \\(x^2\\), \\(\\sqrt{x}\\), and \\(log(x)\\). Note that \\(x\\) must be a quantitative variable: we can’t transform categorical variables. In the swim records example, squaring the year term (\\(x_1^2\\)) allowed our model to have a curve shape. But notice that this makes it seem like record times are slowing down after 1990. This is obviously not the case — records inherently only get faster! — but models lack common sense, and there is no easy math way to tell our model to “be curvy, but also never slope upward”. - in R: y ~ sq(year) * gender - eq: \\(y = \\beta_0 + \\beta_1x_1^2 + \\beta_2x_2 + \\beta_3x_1^2x_2\\)\n\nFurther reading\n\nCh 6: Language of models in Statistical Modeling"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Data Science for Studying Language & the Mind! The Fall 2023 course information and syllabus are below. Course materials for previous semesters are archived here."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "Course description",
    "text": "Course description\nData Sci for Lang & Mind is an entry-level course designed to teach basic principles of statistics and data science to students with little or no background in statistics or computer science. Students will learn to identify patterns in data using visualizations and descriptive statistics; make predictions from data using machine learning and optimization; and quantify the certainty of their predictions using statistical models. This course aims to help students build a foundation of critical thinking and computational skills that will allow them to work with data in all fields related to the study of the mind (e.g. linguistics, psychology, philosophy, cognitive science, neuroscience).\nThere are no prerequisites beyond high school algebra. No prior programming or statistics experience is necessary, though you will still enjoy this course if you already have a little. Students who have taken several computer science or statistics classes should look for a more advanced course."
  },
  {
    "objectID": "syllabus.html#people",
    "href": "syllabus.html#people",
    "title": "Syllabus",
    "section": "People",
    "text": "People\n\nInstructor: Dr. Katie Schuler\nTAs: June Choe, Avinash Goss, Ravi Arya"
  },
  {
    "objectID": "syllabus.html#lectures",
    "href": "syllabus.html#lectures",
    "title": "Syllabus",
    "section": "Lectures",
    "text": "Lectures\nTuesdays and Thursdays at 10:15am in COHN 402."
  },
  {
    "objectID": "syllabus.html#labs",
    "href": "syllabus.html#labs",
    "title": "Syllabus",
    "section": "Labs",
    "text": "Labs\nHands-on practice, quiz prep, and problem set work guided by TAs.\n\n402: Thu at 1:45p in WILL 4 with June\n403: Thu at 3:30p in TOWN 305 with June\n404: Fri at 10:15a in WILL 24 with Ravi\n405: Fri at 12:00p in TOWN 307 with Avinash"
  },
  {
    "objectID": "syllabus.html#office-hours",
    "href": "syllabus.html#office-hours",
    "title": "Syllabus",
    "section": "Office hours",
    "text": "Office hours\nThe linguistics department is on the 3rd floor of the C-wing at 3401 Walnut street, between Franklin’s Table and Modern Eye.\n\nKatie: Tue 12:30-1:30p in 314C ling dept\nJune: Thu 12:30-1:30p in 325C ling dept\nAvinash: TBD\nRavi: TBD"
  },
  {
    "objectID": "syllabus.html#quizzes",
    "href": "syllabus.html#quizzes",
    "title": "Syllabus",
    "section": "Quizzes",
    "text": "Quizzes\nThere are 4 quizzes, taken in class on Tuesdays. Missed quizzes cannot be made up except in cases of genuine conflict or emergency (documentation and a Course Action Notice are required). Instead, you will be invited to submit a missed quiz for half credit (50%)."
  },
  {
    "objectID": "syllabus.html#problem-sets",
    "href": "syllabus.html#problem-sets",
    "title": "Syllabus",
    "section": "Problem sets",
    "text": "Problem sets\nThere are 6 problem sets, due on Sundays to Gradescope by 11:59pm. Students may request an extension of up to 3 days. Extensions beyond this are not permitted, because delaying the release of solutions would negatively impact other students. After solutions are posted, late problem sets can be submitted for half credit (50%)."
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\n\n60% problem sets (equally weighted, lowest dropped)\n40% quizzes (equally weighted)\nLetter grade minimums: 97% A+, 93% A, 90% A-, 87% B+, 84% B, 80% B-, 77% C+, 74% C, 70% C-, 67% D+, 64% D, 61% D-, else F\nAll problems will be graded according to this rubric."
  },
  {
    "objectID": "syllabus.html#collaborations",
    "href": "syllabus.html#collaborations",
    "title": "Syllabus",
    "section": "Collaborations",
    "text": "Collaborations\nCollaboration on problem sets is highly encouraged! If you collaborate, you need to write your own code/solutions, name your collaborators, and cite any outside sources you consulted (you don’t need to cite the course material)."
  },
  {
    "objectID": "syllabus.html#accomodations",
    "href": "syllabus.html#accomodations",
    "title": "Syllabus",
    "section": "Accomodations",
    "text": "Accomodations\nWe will support any accommodations arranged through Disability Services via the Weingarten Center."
  },
  {
    "objectID": "syllabus.html#extra-credit",
    "href": "syllabus.html#extra-credit",
    "title": "Syllabus",
    "section": "Extra credit",
    "text": "Extra credit\nThere is no extra credit in the course. However, students can submit any missed problem set or quiz by the end of the semester for half credit (50%). To ensure fair treatment across all students, all students will receive a 1% “bonus” to their final course grade: 92.54% will become 93.54%."
  },
  {
    "objectID": "syllabus.html#regrade-requests",
    "href": "syllabus.html#regrade-requests",
    "title": "Syllabus",
    "section": "Regrade requests",
    "text": "Regrade requests\nRegrade requests should be submitted through Gradescope within one week of receiving your graded assignment. Please explain why you believe there was a grading mistake, given the posted solutions and rubric."
  },
  {
    "objectID": "syllabus.html#resources",
    "href": "syllabus.html#resources",
    "title": "Syllabus",
    "section": "Resources",
    "text": "Resources\nIn addition to the course website, we will use the following:\n\ngoogle colab (r kernel) - for computing\ncanvas - for posting grades\ngradescope - for submitting problem sets\ned discussion - for announcements and questions\n\nPlease consider using these Penn resources this semester:\n\nWeingarten Center for academic support and tutoring.\nWellness at Penn for health and wellbeing."
  },
  {
    "objectID": "rubric.html",
    "href": "rubric.html",
    "title": "Rubric",
    "section": "",
    "text": "All quiz and problem set questions are graded with the following 4-point rubric. By “understand” we mean you understand the lecture concepts and can apply the technical skills you learned in the course. Solutions that include packages or functions not covered in this course will recieve a highest possible score of 2."
  },
  {
    "objectID": "rubric.html#overall-grade",
    "href": "rubric.html#overall-grade",
    "title": "Rubric",
    "section": "Overall grade",
    "text": "Overall grade\nTo compute your overall grade on a problem set or quiz, we take your average score on all questions, add a constant of 6, and divide by 10. For example, given a problem set with 3 questions:\n\nscores of 4-3-3 would result in a 93.3% (A): (4+3+3)/3 + 6 = 9.33/10\nscores of 3-2-2 would result in a 83.3% (B-): (3+2+2)/3 + 6 = 8.33/10\nscores of 1-1-1 would result in a 70% (C-): (1+1+1)/3 + 6 = 7/10\n\nWe chose this rubric because all reasonable attempts receive a passing grade, but A+ is reserved for students with advanced understanding of the material."
  },
  {
    "objectID": "rubric.html#missed-problem-sets-and-quizzes",
    "href": "rubric.html#missed-problem-sets-and-quizzes",
    "title": "Rubric",
    "section": "Missed problem sets and quizzes",
    "text": "Missed problem sets and quizzes\nMissed problem sets or quizzes will receive an overall score of zero. However, students can submit any missed problem set or quiz by the end of the semester for half credit (50%)."
  }
]