---
title: "Sampling Distributions"
subtitle: "Data Science for Studying Language and the Mind"
author: Katie Schuler
date: 09-21-2023
echo: true
format: 
    revealjs:
        theme: dark
        slide-number: true
        incremental: true 
        footer: "[https://kathrynschuler.com/datasci](https://kathrynschuler.com/datasci/)"
---

```{r}
#| echo: false
#| message: false

library(tidyverse)
library(tidymodels)

theme_set(theme_classic(base_size = 20))

```

## You are `here` {.smaller} 

:::: {.columns}

::: {.column width="33%"}

##### Data science with R 
::: {.nonincremental}
- Hello, world!
- R basics
- Data importing
- Data visualization
- Data wrangling 
:::
:::

::: {.column width="33%"}

##### Stats & Model buidling
::: {.nonincremental}
- `Probability distributions`
- `Sampling variability`
- Hypothesis testing
- Model specification
- Model fitting 
- Model accuracy
- Model reliability
:::
:::

::: {.column width="33%"}

##### More advanced 
::: {.nonincremental}

- Classification
- Feature engineering (preprocessing) 
- Inference for regression
- Mixed-effect models
::: 
:::

::::

## Attribution

- Inspired by a MATLAB course Katie took by Kendrick Kay 
- Data simulated from Ritchie et al 2018:

. . . 

> Sex Differences in the Adult Human Brain: Evidence from 5216 UK Biobank Participants

# Explore a simple dataset 

## Dataset {.smaller}

Suppose we measure a single quantity: `brain volume of human adults` 

```{r}
#| echo: false 

data <- read_csv("http://kathrynschuler.com/datasets/brain_volume.csv") %>% select(volume)
glimpse(data)

ggplot(data, aes(x = volume)) +
    geom_rug() +
    labs(x = "brain volume")

```



## Visualize the distribution {.smaller}

Visualize the distribution of the data with a `histogram`

```{r}
#| echo: false 

ggplot(data, aes(x = volume)) +
    geom_rug() +
  geom_histogram(color = "black", fill = "gray", bins = 16, alpha = 0.5) +
    labs(x = "brain volume")

```




## Measure of central tendency {.smaller}

Summarize the data with a single value: `mean`, a measure of where a central or typical value might fall

```{r}
#| output-location: column

sum_stats <- data %>% summarise(
    n = n(), 
    mean = mean(volume))
sum_stats
```

. . . 

```{r}
#| echo: false 

ggplot(data, aes(x = volume)) +
    geom_rug() +
  geom_histogram(color = "black", fill = "gray", bins = 16, alpha = 0.5) +
  geom_vline(xintercept = sum_stats$mean, y = 1100, size = 2) +
  coord_cartesian(ylim = c(0, 1200)) +
    labs(x = "brain volume")

```



## Measure of variability {.smaller}

Summarize the spread of the data with `standard deviation`

```{r}
#| output-location: column

sum_stats <- data %>% summarise(
    n = n(), 
    mean = mean(volume),
    sd = sd(volume))
sum_stats
```

. . . 

```{r}
#| echo: false 


ggplot(data, aes(x = volume)) +
    geom_rug() +
  geom_histogram(color = "black", fill = "gray", bins = 16, alpha = 0.5) +
    geom_vline(xintercept = sum_stats$mean, y = 1100, size = 2) +

  geom_vline(xintercept = sum_stats$mean-sum_stats$sd, linetype = "dashed", size = 2) +
  geom_vline(xintercept = sum_stats$mean+sum_stats$sd, linetype = "dashed", size = 2) +
  coord_cartesian(ylim = c(0, 1200)) +
    labs(x = "brain volume")

```


## Parametric statistics

Mean and sd are `parametric` summary statistics. They are given by the following equations:

:::: {.columns}
::: {.column width=50%}
$mean(x) = \bar{x} = \frac{\sum_{i=i}^{n} x_{i}}{n}$
:::
::: {.column width=50%}
sd($x$) = $\sqrt{\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}}$
:::
::::

```{r}
#| echo: false 

ggplot(data, aes(x = volume)) +
    geom_rug() +
  geom_histogram(color = "black", fill = "gray", bins = 16, alpha = 0.5) +
    geom_vline(xintercept = sum_stats$mean, y = 1100, size = 2) +

  geom_vline(xintercept = sum_stats$mean-sum_stats$sd, linetype = "dashed", size = 2) +
  geom_vline(xintercept = sum_stats$mean+sum_stats$sd, linetype = "dashed", size = 2) +
  coord_cartesian(ylim = c(0, 1200)) +
    labs(x = "brain volume")

```

- Mean and sd are a good summary of the data when the distribution is `normal` (**gaussian**)

## Nonparametric statistics

But suppose our distribution is not normal.

```{r}
#| echo: false
not_normal <- tibble(
    y = c(rep(5, times = 55 ), rep(15, times = 25 ), rep(25, times = 15), rep(32, 5), 42, 55, 62:65, 72:75, 83 
))

sum_stats_not <- not_normal %>%
    summarise(n = n(), mean = mean(y), sd = sd(y))

ggplot(not_normal, aes(x = y)) +
  geom_histogram(color = "black", fill = "gray", bins = 9, alpha = 0.5) +
  coord_cartesian(ylim = c(0, 60)) +
    labs(x = "y")

```


## Nonparametric statistics

mean and sd are not a good summary anymore.

```{r}
#| echo: false

ggplot(not_normal, aes(x = y)) +
  geom_histogram(color = "black", fill = "gray", bins = 9, alpha = 0.5) +
    geom_vline(xintercept = sum_stats_not$mean, y = 1100, size = 2) +

  geom_vline(xintercept = sum_stats_not$mean-sum_stats_not$sd, linetype = "dashed", size = 2) +
  geom_vline(xintercept = sum_stats_not$mean+sum_stats_not$sd, linetype = "dashed", size = 2) +
  coord_cartesian(ylim = c(0, 60)) +
    labs(x = "y")

```

## Median {.smaller}

Instead we can use the median as our measure of central tendency.

```{r}
#| output-location: column

np_sum_stats <- not_normal %>% summarise(
    n = n(), 
    median = median(y))
np_sum_stats
```

. . . 

```{r}
#| echo: false 

ggplot(not_normal, aes(x = y)) +
  geom_histogram(color = "black", fill = "gray", bins = 9, alpha = 0.5) +
    geom_vline(xintercept = np_sum_stats$median, y = 1100, size = 2) +

#   geom_vline(xintercept = np_sum_stats$lower, linetype = "dashed", size = 2) +
#   geom_vline(xintercept = np_sum_stats$upper, linetype = "dashed", size = 2) +
  coord_cartesian(ylim = c(0, 60)) +
    labs(x = "brain volume")

```


## IQR {.smaller}

And the interquartile range (`IQR`) as a measure of the spread in our data.

```{r}
#| output-location: column

np_sum_stats <- not_normal %>% summarise(
    n = n(), 
    median = median(y),
    lower = quantile(y, 0.25),
    upper = quantile(y, 0.75) )
np_sum_stats
```

. . . 

```{r}
#| echo: false 

ggplot(not_normal, aes(x = y)) +
  geom_histogram(color = "black", fill = "gray", bins = 9, alpha = 0.5) +
    geom_vline(xintercept = np_sum_stats$median, y = 1100, size = 2) +

  geom_vline(xintercept = np_sum_stats$lower, linetype = "dashed", size = 2) +
  geom_vline(xintercept = np_sum_stats$upper, linetype = "dashed", size = 2) +
  coord_cartesian(ylim = c(0, 60)) +
    labs(x = "brain volume")

```

# Probability distributions  {.smaller}

A mathematical function that describes the probability of observing different possible values of a variable

## Uniform probability distribution  {.smaller}

```{r}
#| echo: false

uniform_sample <- tibble(
  y = rep(1:10, each=100)
)

# plot uniform with histogram 
ggplot(uniform_sample, aes(x = y)) +
  geom_histogram(color = "black", fill = "gray", bins = 10, alpha = 0.5) 
```

## Uniform probability distirubtion {.smaller}

All possible values are equally likely

:::: {.columns}
::: {.column width=50%}
```{r}
#| echo: false

uniform_sample <- tibble(
  y = rep(1:10, each=100)
)

# plot uniform with histogram 
ggplot(uniform_sample, aes(x = y)) +
  geom_histogram(color = "black", fill = "gray", bins = 10, alpha = 0.5) 
```

:::

::: {.column width=50%}


```{r}
uniform_sample %>% summarise(
    min = min(y), 
    max = max(y), 
    prob = 1/(max - min))
```



height of prob density func
```{r}
dunif(4, min = 1, max = 10)
```


prob less than given value
```{r}
punif(4, min = 1, max = 10)
```

:::
::::

$p(x) = \frac{1}{max-min}$



## Gaussian (normal) probability distribution {.smaller}

```{r}
#| echo: false

normal_sample <- tibble(
  y = rnorm(1000, mean = 0, sd = 1)
)

# plot uniform with histogram 
ggplot(normal_sample, aes(x = y)) +
  geom_histogram(color = "black", fill = "gray", bins = 12, alpha = 0.5) 
```


## Gaussian (normal) probability distribution {.smaller}

:::: {.columns}
::: {.column width=50%}
```{r}
#| echo: false

# plot uniform with density
ggplot(normal_sample, aes(x = y)) +
  geom_histogram(aes(y = after_stat(density)), color = "black", fill = "gray", bins = 9, alpha = 0.5)  +
  geom_density()

```

:::

::: {.column width=50%}


height of prob density func
```{r}
#dunif(4, min = 1, max = 10)
dnorm(4, mean=0, sd=1)
```


prob less than given value
```{r}
#punif(4, min = 1, max = 10)
pnorm(4, mean=0, sd=1)
```

:::
::::

$p(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^{2}\right)$

# Sampling variability

## The population {.smaller}

We actually want to know something about the `population`: the mean brain volume of Penn undergrads (the **parameter**)


```{r}
#| echo: false
set.seed(45)
population <- tibble(
  volume = rnorm(11250, mean = 1219, sd = 161),
  sex = NA)

  p_stats <- population %>% summarise(n = n(), mean = mean(volume), sd = sd(volume))


population %>%
  ggplot(aes(x = volume)) +
  geom_histogram( color = "black", fill = "white", bins = 24, alpha = 0.5) +
        geom_vline(xintercept = p_stats$mean, color = "gold", linewidth = 2)  +
  annotate(
    geom = "point",
    color = "gold",
    shape = 8,
    size = 10,
    x = p_stats$mean,
    y = 1600
    ) +
      annotate(geom = "text", x = p_stats$mean +200, y = 1600, label = "← parameter", size = 6) +
      labs( y = "count", x = "brain volume", caption = "n = 11,250")


```


## The sample  {.smaller}

But we only have a small `sample` of the  population: maybe we can measure the brain volume of 100 students

```{r}
#| echo: false
sample <- population %>% 
  specify(response = volume) %>% 
  rep_slice_sample(n = 100, reps = 1000) 

s_stats <- sample %>%
  group_by(replicate) %>%
  summarise(mean = mean(volume))

use <- s_stats %>% summarise(
  min = min(mean), rep_min = which.min(mean), max = max(mean), rep_max = which.max(mean))

   population %>%
  ggplot(aes(x = volume)) +
  geom_histogram( color = "black", fill = "white", bins = 24, alpha = 0.5) +
  geom_rug(data = filter(sample, replicate == use$rep_min), color = "navy") +
  geom_vline(xintercept = p_stats$mean, color = "gold", linewidth = 2)  +

  annotate(
    geom = "point",
    color = "gold",
    shape = 8,
    size = 10,
    x = p_stats$mean,
    y = 1600
    ) +      annotate(geom = "text", x = p_stats$mean +200, y = 1600, label = "← parameter", size = 6) +
    labs(y = "count", x = "brain volume", caption = "sample n = 100")

 

```

## Sampling variability {.smaller}

Any statistic we compute from a random sample we've collected (**parameter estimate**) will be subject to `sampling variability` and will differ from that statistics computed on the entire population (**parameter**)

```{r}
#| echo: false

 population %>%
  ggplot(aes(x = volume)) +
  #coord_cartesian(xlim = c(800, 1600)) +
  geom_histogram( color = "black", fill = "white", bins = 24, alpha = 0.5) +
  geom_rug(data = filter(sample, replicate == use$rep_min), color = "navy") +
  geom_vline(xintercept = p_stats$mean, color = "gold", linewidth = 2)  +
  geom_vline(xintercept = use$min, color = "navy", linewidth = 2)  +

  annotate(
    geom = "point",
    color = "gold",
    shape = 8,
    size = 10,
    x = p_stats$mean,
    y = 1600
    ) +
         annotate(geom = "text", x = p_stats$mean +200, y = 1600, label = "← parameter", size = 6) +
    annotate(geom = "text", x = use$min - 250, y = 1600, label = "parameter estimate →", color = "navy", size = 6) +
    labs(y = "count", x = "brain volume", caption = "sample n = 100")
 

```


## Sampling variability {.smaller}

If we took another sample of 100 students, our parameter estimate would be different. 

```{r}
#| echo: false

 population %>%
  ggplot(aes(x = volume)) +
  #coord_cartesian(xlim = c(800, 1600)) +
  geom_histogram( color = "black", fill = "white", bins = 24, alpha = 0.5) +
  geom_rug(data = filter(sample, replicate == use$rep_max), color = "darkred") +
  geom_vline(xintercept = p_stats$mean, color = "gold", linewidth = 2)  +
  geom_vline(xintercept = use$max, color = "darkred", linewidth = 2)  +

  annotate(
    geom = "point",
    color = "gold",
    shape = 8,
    size = 10,
    x = p_stats$mean,
    y = 1600
    ) +
         annotate(geom = "text", x = p_stats$mean - 200, y = 1600, label = "parameter →", size = 6) +
    annotate(geom = "text", x = use$max + 250, y = 1600, label = "← parameter estimate", color = "darkred", size = 6) +
    labs(y = "count", x = "brain volume", caption = "sample #2 n = 100")

```


## Sampling distribution {.smaller}

The `sampling distribution` is the probability distribution of values our parameter estimate can take on. Constructed by taking a random sample, computing stat of interest, and repeating many times.

```{r}
#| echo: false

ggplot(s_stats, aes(x = mean)) +
    geom_histogram(bins = 18, color = "gray") +
     geom_vline(xintercept = use$max, color = "darkred", linewidth = 2)  +
       geom_vline(xintercept = use$min, color = "navy", linewidth = 2)  +

    labs(title = "Sampling distribution of mean brain volume", x = "parameter estimate (mean)") 

```


## Quantifying sampling variability {.smaller}

The `spread` of the sampling distribution indicates how the parameter estimate will vary from different random samples. We can quantify the spread (express our uncertainty on our parameter estimate) in two ways


```{r}
#| echo: false

ggplot(s_stats, aes(x = mean)) +
    geom_rug() +
    geom_histogram(bins = 18, color = "gray")  
```

## Quantifying sampling variability with `standard error` {.smaller}

One way is to compute the standard deviation of the sampling distribution: the `standard error`

```{r}
#| echo: false

se <- s_stats %>% summarise(samp_mean = mean(mean), samp_sd = sd(mean), se = samp_sd/sqrt(length(mean)))

theoretical_ci <- se %>% mutate(lower = samp_mean-samp_sd, upper = samp_mean + samp_sd) %>% select(lower, upper)

ggplot(s_stats, aes(x = mean)) +
    geom_rug() +
    geom_histogram(bins = 18, color = "gray")  +
    shade_ci(theoretical_ci) +
    labs(y = "count")

```

## Quantifying sampling variability with a `confidence interval`{.smaller}

Another way is to construct a `confidence interval`

```{r}
#| echo: false

ci <- s_stats %>%
  summarise(lower = quantile(mean, 0.157), upper = quantile(mean, .843))


ggplot(s_stats, aes(x = mean)) +
    geom_rug() +
    geom_histogram(bins = 18, color = "gray")  +
    shade_ci(ci) +
    labs(y = "count")

```

## Practical considerations 

- We don't have access to the entire population
- We can (usually) only do our experiment once







# Bootstrapping

To construct the sampling distribution

## Bootstrapping 

Instead of assuming a parametric probability distributon, we use the data themselves to approximate the underlying distribution: we `sample our sample`!


# Bootsrapping with `infer` 
## `infer` is part of `tidymodels` 

> The tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.


```r
install.packages("tidymodels")`
```

```{r}
#| echo: false
# get a sample to work with as our "data"
sample1 <- filter(sample, replicate == 1)

```

## Generate the sampling distribution {.smaller}

Generate the sampling distribution with `specify()`, `generate()`, and `calculate()`

```{r}
#| output-location: column

bootstrap_distribution <- sample1  %>% 
  specify(response = volume) %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "mean")

bootstrap_distribution
```

## Visualize the bootstrap distribution {.smaller}

Visualize the bootstrap distribution you generated with `visualize()`

```{r}
#| output-location: column
bootstrap_distribution %>% 
  visualize()
```

## Quantify the spread with `se` {.smaller}

Quantify the spread of the sampling distributon with `get_confidence_interval()`, using **standard error**

```{r}
#| output-location: column
se_bootstrap <- bootstrap_distribution %>% 
  get_confidence_interval(
    type = "se",
    point_estimate = mean(sample1$volume)
  )

se_bootstrap

```

. . . 

```{r}
#| output-location: column
bootstrap_distribution %>% 
  visualize() +
  shade_confidence_interval(
    endpoints = se_bootstrap
  )
```

## Quantify the spread with `ci` {.smaller}

Quantify the spread of the sampling distributon with `get_confidence_interval`, using a **confidence interval**

```{r}
#| output-location: column
ci_bootstrap <- bootstrap_distribution %>% 
  get_confidence_interval(
    type  ="percentile", 
    level = 0.95
  )

ci_bootstrap
```

. . . 

```{r}
#| output-location: column

bootstrap_distribution %>% 
  visualize() +
  shade_confidence_interval(
    endpoints = ci_bootstrap
  )
```